{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdead5bb",
   "metadata": {
    "papermill": {
     "duration": 0.008378,
     "end_time": "2025-06-09T20:28:17.728651",
     "exception": false,
     "start_time": "2025-06-09T20:28:17.720273",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# VLM Benchmark for Object Property Abstraction\n",
    "\n",
    "This notebook implements a benchmark for evaluating Vision Language Models (VLMs) on object property abstraction and visual question answering (VQA) tasks. The benchmark includes three types of questions:\n",
    "\n",
    "1. Direct Recognition\n",
    "2. Property Inference\n",
    "3. Counterfactual Reasoning\n",
    "\n",
    "And three types of images:\n",
    "- REAL\n",
    "- ANIMATED\n",
    "- AI GENERATED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2054ebe9",
   "metadata": {
    "papermill": {
     "duration": 0.003898,
     "end_time": "2025-06-09T20:28:17.737077",
     "exception": false,
     "start_time": "2025-06-09T20:28:17.733179",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccf8a7c",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-09T20:28:17.759324Z",
     "iopub.status.busy": "2025-06-09T20:28:17.759004Z",
     "iopub.status.idle": "2025-06-09T20:28:19.490155Z",
     "shell.execute_reply": "2025-06-09T20:28:19.489193Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 1.737039,
     "end_time": "2025-06-09T20:28:19.491619",
     "exception": false,
     "start_time": "2025-06-09T20:28:17.754580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install qwen-vl-utils flash-attn #--no-build-isolation\n",
    "%pip install -q -U google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d2356f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T20:28:19.502850Z",
     "iopub.status.busy": "2025-06-09T20:28:19.502575Z",
     "iopub.status.idle": "2025-06-09T20:28:25.674870Z",
     "shell.execute_reply": "2025-06-09T20:28:25.673757Z"
    },
    "papermill": {
     "duration": 6.179241,
     "end_time": "2025-06-09T20:28:25.676579",
     "exception": false,
     "start_time": "2025-06-09T20:28:19.497338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import gc\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Any\n",
    "from google import genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15391e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get API key from environment\n",
    "api_key = os.getenv('GOOGLE_API_KEY')\n",
    "if not api_key:\n",
    "    raise ValueError(\"GOOGLE_API_KEY not found in environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ca2b9a",
   "metadata": {
    "papermill": {
     "duration": 0.004194,
     "end_time": "2025-06-09T20:28:25.686407",
     "exception": false,
     "start_time": "2025-06-09T20:28:25.682213",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Benchmark Tester Class\n",
    "\n",
    "This class handles the evaluation of models against our benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28935166",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T20:28:25.696680Z",
     "iopub.status.busy": "2025-06-09T20:28:25.696171Z",
     "iopub.status.idle": "2025-06-09T20:28:25.720893Z",
     "shell.execute_reply": "2025-06-09T20:28:25.720152Z"
    },
    "papermill": {
     "duration": 0.031348,
     "end_time": "2025-06-09T20:28:25.722049",
     "exception": false,
     "start_time": "2025-06-09T20:28:25.690701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BenchmarkTester:\n",
    "    def __init__(self, benchmark_path=\"/var/scratch/ave303/OP_bench/benchmark.json\", data_dir=\"/var/scratch/ave303/OP_bench/\"):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        with open(benchmark_path, 'r') as f:\n",
    "            self.benchmark = json.load(f)\n",
    "        self.data_dir = data_dir\n",
    "        self.api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "    def clean_answer(self, answer):\n",
    "        \"\"\"Extract number and reasoning from the model's answer.\"\"\"\n",
    "        # Try to extract number and reasoning using regex\n",
    "        import re\n",
    "        pattern = r'(\\d+)\\s*\\[(.*?)\\]'\n",
    "        match = re.search(pattern, answer)\n",
    "        \n",
    "        if match:\n",
    "            number = match.group(1)\n",
    "            objects = [obj.strip() for obj in match.group(2).split(',')]\n",
    "            return {\n",
    "                \"count\": number,\n",
    "                \"reasoning\": objects\n",
    "            }\n",
    "        else:\n",
    "            # Fallback if format isn't matched\n",
    "            numbers = re.findall(r'\\d+', answer)\n",
    "            return {\n",
    "                \"count\": numbers[0] if numbers else \"0\",\n",
    "                \"reasoning\": []\n",
    "            }\n",
    "    \n",
    "    def evaluate_model(self, model_name, save_path, start_idx=0, batch_size=5):\n",
    "        results = []\n",
    "        print(f\"\\nEvaluating {model_name}...\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # client = genai.Client(api_key=self.api_key)\n",
    "\n",
    "        # Force garbage collection before starting\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        try:\n",
    "            images = self.benchmark['benchmark']['images'][start_idx:start_idx + batch_size]\n",
    "            total_images = len(images)\n",
    "            \n",
    "            for idx, image_data in enumerate(tqdm(images, desc=\"Processing images\")):\n",
    "                try:\n",
    "                    client = genai.Client(api_key=self.api_key)\n",
    "                    print(f\"\\nProcessing image {idx+1}/{total_images}: {image_data['image_id']}\")\n",
    "                    image_path = Path(self.data_dir)/image_data['path']\n",
    "                    if not image_path.exists():\n",
    "                        print(f\"Warning: Image not found at {image_path}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Load and preprocess image\n",
    "                    # image = Image.open(image_path).convert(\"RGB\")\n",
    "                    \n",
    "                    image = client.files.upload(file=image_path)\n",
    "                    image_results = []  # Store results for current image\n",
    "                    \n",
    "                    for question in image_data['questions']:\n",
    "                        try:\n",
    "                            print(f\"Question: {question['question']}\")\n",
    "\n",
    "                            response = client.models.generate_content(\n",
    "                                model=model_name,\n",
    "                                contents=[image, f\"{question['question']} Your response MUST be in the following format and nothing else:\\n <NUMBER> [<OBJECT1>, <OBJECT2>, <OBJECT3>, ...]\"],\n",
    "                            )\n",
    "                            # print(response.text)\n",
    "                            \n",
    "                            # Clear cache before processing each question\n",
    "                            torch.cuda.empty_cache()\n",
    "                            \n",
    "                            cleaned_answer = self.clean_answer(response.text)\n",
    "                            \n",
    "                            image_results.append({\n",
    "                                \"image_id\": image_data[\"image_id\"],\n",
    "                                \"image_type\": image_data[\"image_type\"],\n",
    "                                \"question_id\": question[\"id\"],\n",
    "                                \"question\": question[\"question\"],\n",
    "                                \"ground_truth\": question[\"answer\"],\n",
    "                                \"model_answer\": cleaned_answer[\"count\"],\n",
    "                                \"model_reasoning\": cleaned_answer[\"reasoning\"],\n",
    "                                \"raw_answer\": response.text,  # Keep raw answer for debugging\n",
    "                                \"property_category\": question[\"property_category\"]\n",
    "                            })\n",
    "                            \n",
    "                            # # Clear memory\n",
    "                            # del outputs, inputs\n",
    "                            # torch.cuda.empty_cache()\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing question: {str(e)}\")\n",
    "                            continue\n",
    "                    \n",
    "                    # Add results from this image\n",
    "                    results.extend(image_results)\n",
    "\n",
    "                    client.files.delete(name=image.name)\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing image {image_data['image_id']}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            # Save final results\n",
    "            if results:\n",
    "                with open(save_path, 'w') as f:\n",
    "                    json.dump(results, f, indent=4)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during evaluation: {str(e)}\")\n",
    "            if results:\n",
    "                with open(f\"{save_path}_error_state.json\", 'w') as f:\n",
    "                    json.dump(results, f, indent=4)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494c1b67",
   "metadata": {},
   "source": [
    "## Test Gemini 2.0 Flash\n",
    "\n",
    "Let's evaluate Gemini-2.0-flash model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2f9bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_Gemini2_0Flash():\n",
    "    tester = BenchmarkTester()\n",
    "    Gemini2_0Flash_results = tester.evaluate_model(\n",
    "        \"gemini-2.0-flash\",\n",
    "        \"Gemini2_0Flash_results.json\",\n",
    "        start_idx=45,\n",
    "        batch_size=5\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00f4a8a",
   "metadata": {
    "papermill": {
     "duration": 0.004177,
     "end_time": "2025-06-09T20:28:25.769210",
     "exception": false,
     "start_time": "2025-06-09T20:28:25.765033",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Run Evaluation\n",
    "\n",
    "Now we can run our evaluation. Let's start with the Gemini-2.0-flash model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42818981",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Gemini2_0Flash()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7163706,
     "sourceId": 11436696,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python ((op_bench)",
   "language": "python",
   "name": "op_bench"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 160.663536,
   "end_time": "2025-06-09T20:30:56.721727",
   "environment_variables": {},
   "exception": null,
   "input_path": "/var/scratch/ave303/OP_bench/opa-benchmark-gemma3-27b-it.ipynb",
   "output_path": "gemma3-27b-it_output.ipynb",
   "parameters": {},
   "start_time": "2025-06-09T20:28:16.058191",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
