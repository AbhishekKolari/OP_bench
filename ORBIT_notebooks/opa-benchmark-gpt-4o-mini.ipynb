{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-VULwmhnFWi"
      },
      "source": [
        "## Install and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8DvQar-nIWA",
        "outputId": "413bb9c9-77da-4bc0-ff2a-726e024b8b1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai==0.28.1\n",
            "  Downloading openai-0.28.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai==0.28.1) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai==0.28.1) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai==0.28.1) (3.12.14)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28.1) (2025.7.14)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.1) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.1) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.1) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.1) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.1) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.1) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28.1) (1.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from aiosignal>=1.4.0->aiohttp->openai==0.28.1) (4.14.1)\n",
            "Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m616.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.97.1\n",
            "    Uninstalling openai-1.97.1:\n",
            "      Successfully uninstalled openai-1.97.1\n",
            "Successfully installed openai-0.28.1\n"
          ]
        }
      ],
      "source": [
        "!pip install openai==0.28.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAALH5qEnOYG"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import nltk\n",
        "import requests\n",
        "import random\n",
        "import openai\n",
        "import pickle\n",
        "import base64\n",
        "import requests\n",
        "import os\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "sys.path.append(os.path.abspath('../UtilsYF'))\n",
        "from normal_utils import *\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9SmtBkloGEO"
      },
      "source": [
        "# Run GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EPUVFfCoQfx"
      },
      "outputs": [],
      "source": [
        "def encode_image(image_path):\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        encoded_string = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
        "    return encoded_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJQ85VjsoQ2C"
      },
      "outputs": [],
      "source": [
        "def query_gpt_v(image_path,prompt,model=\"gpt-4o-mini-2024-07-18\"):\n",
        "    # Encode image as Base64\n",
        "    encoded_image = encode_image(image_path)\n",
        "    # Construct the messages for the GPT-4o API\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encoded_image}\"}},\n",
        "                {\"type\": \"text\", \"text\": prompt}\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,  # Adjust the model if needed\n",
        "        messages=messages,\n",
        "        max_tokens=2000,\n",
        "        temperature = 0.0\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHXz4dshnhXW"
      },
      "outputs": [],
      "source": [
        "benchmark_path=\"OP_bench/benchmark.json\"\n",
        "benchmark_path_VG=\"ORBIT_VG_output.json\"\n",
        "\n",
        "with open(benchmark_path_VG, 'r') as f:\n",
        "  benchmark = json.load(f)\n",
        "\n",
        "output_file = \"4o-mini_temp0_results.json\"\n",
        "output_file_VG = \"4o-mini_temp0_VG_results.json\"\n",
        "model_name = \"gpt-4o-mini\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xD12_14oLXY"
      },
      "outputs": [],
      "source": [
        "def clean_answer(answer):\n",
        "  pattern = r'(\\d+)\\s*\\[(.*?)\\]'\n",
        "  match = re.search(pattern, answer)\n",
        "\n",
        "  if match:\n",
        "      number = match.group(1)\n",
        "      objects = [obj.strip() for obj in match.group(2).split(',')]\n",
        "      return {\n",
        "          \"count\": number,\n",
        "          \"reasoning\": objects\n",
        "      }\n",
        "  else:\n",
        "      # Fallback if format isn't matched\n",
        "      numbers = re.findall(r'\\d+', answer)\n",
        "      return {\n",
        "          \"count\": numbers[0] if numbers else \"0\",\n",
        "          \"reasoning\": []\n",
        "      }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_w8E2eFoNSV",
        "outputId": "2817c93c-3449-43ef-ade3-a449986298a9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing images: 100%|██████████| 44/44 [01:37<00:00,  2.21s/it]\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "# images = benchmark['benchmark']['images']\n",
        "images = benchmark['images']\n",
        "for idx, image_data in enumerate(tqdm(images, desc=\"Processing images\")):\n",
        "  # image_path = \"OP_bench/\" + image_data['path']\n",
        "  image_path = image_data['path']\n",
        "  image_results = []\n",
        "  for question in image_data['questions']:\n",
        "    prompt = f\"{question['question']} Your response MUST be in the following format and nothing else:\\n <NUMBER> [<OBJECT1>, <OBJECT2>, <OBJECT3>, ...]\"\n",
        "    answer = query_gpt_v(image_path, prompt, \"gpt-4o-mini-2024-07-18\")\n",
        "    cleaned_answer = clean_answer(answer)\n",
        "\n",
        "\n",
        "    image_results.append({\n",
        "                        \"image_id\": image_data[\"image_id\"],\n",
        "                        \"image_type\": image_data[\"image_type\"],\n",
        "                        \"question_id\": question[\"id\"],\n",
        "                        \"question\": question[\"question\"],\n",
        "                        \"ground_truth\": question[\"answer\"],\n",
        "                        \"model_answer\": cleaned_answer[\"count\"],\n",
        "                        \"model_reasoning\": cleaned_answer[\"reasoning\"],\n",
        "                        \"raw_answer\": answer,  # Keep raw answer for debugging\n",
        "                        \"property_category\": question[\"property_category\"]\n",
        "                    })\n",
        "  results.extend(image_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R28sTYOjoYr5"
      },
      "outputs": [],
      "source": [
        "with open(output_file_VG, 'w') as f:\n",
        "  json.dump(results, f, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcJbYu-byjSD"
      },
      "source": [
        "# Analyse Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7tUaTnfx9ze"
      },
      "outputs": [],
      "source": [
        "def load_results(file_path):\n",
        "  with open(file_path, 'r') as f:\n",
        "      return json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3XTJ8xyzOk_"
      },
      "outputs": [],
      "source": [
        "def calculate_accuracy(predictions, ground_truths):\n",
        "  \"\"\"Calculate accuracy between predictions and ground truths.\"\"\"\n",
        "  # Convert Series to lists to avoid pandas Series ambiguity\n",
        "  if hasattr(predictions, 'tolist'):\n",
        "      predictions = predictions.tolist()\n",
        "  if hasattr(ground_truths, 'tolist'):\n",
        "      ground_truths = ground_truths.tolist()\n",
        "\n",
        "  # Calculate accuracy\n",
        "  correct = sum(1 for p, g in zip(predictions, ground_truths) if str(p) == str(g))\n",
        "  return correct / len(predictions) if predictions else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRaJb6PDzUFd",
        "outputId": "517fd74d-3865-409c-adee-354da8d4d4b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4o-mini Overall Accuracy: 30.37%\n"
          ]
        }
      ],
      "source": [
        "results = load_results(output_file)\n",
        "df = pd.DataFrame(results)\n",
        "overall_accuracy = calculate_accuracy(df['model_answer'], df['ground_truth'])\n",
        "print(f\"4o-mini Overall Accuracy: {overall_accuracy:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lx-6r6MN1Ukq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJ3pCl5I1UnU",
        "outputId": "aa287dd8-e272-4548-cc56-762c7c8a8631"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4o-mini Overall Accuracy for orbit-vg: 36.36%\n"
          ]
        }
      ],
      "source": [
        "results = load_results(output_file_VG)\n",
        "df = pd.DataFrame(results)\n",
        "overall_accuracy = calculate_accuracy(df['model_answer'], df['ground_truth'])\n",
        "print(f\"4o-mini Overall Accuracy for orbit-vg: {overall_accuracy:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdgfdRrLUvCp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a50XmfUnUvFH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3ekoPuaUwnG"
      },
      "outputs": [],
      "source": [
        "def calculate_error_metrics(predictions, ground_truths, category=None):\n",
        "    \"\"\"\n",
        "    Calculate MAE, MSE, and RMSE between predictions and ground truths.\n",
        "\n",
        "    Args:\n",
        "        predictions: List of predicted counts\n",
        "        ground_truths: List of ground truth counts\n",
        "        category: Optional category name for the analysis\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing MAE, MSE, and RMSE\n",
        "    \"\"\"\n",
        "    # Convert to numeric values if they're strings\n",
        "    preds = [int(str(p)) for p in predictions]\n",
        "    truths = [int(str(g)) for g in ground_truths]\n",
        "\n",
        "    # Calculate differences\n",
        "    differences = [abs(p - t) for p, t in zip(preds, truths)]\n",
        "\n",
        "    # Calculate metrics\n",
        "    mae = sum(differences) / len(differences)\n",
        "    mse = sum(d * d for d in differences) / len(differences)\n",
        "    rmse = mse ** 0.5\n",
        "\n",
        "    result = {\n",
        "        'MAE': mae,\n",
        "        'MSE': mse,\n",
        "        'RMSE': rmse,\n",
        "        'sample_size': len(differences)\n",
        "    }\n",
        "\n",
        "    if category:\n",
        "        result['category'] = category\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdUp3hJ2bznR"
      },
      "outputs": [],
      "source": [
        "def calculate_off_by_n_accuracy(predictions, ground_truths, n=1):\n",
        "    \"\"\"\n",
        "    Calculate accuracy within n counts of the ground truth.\n",
        "\n",
        "    Args:\n",
        "        predictions: List of predicted counts\n",
        "        ground_truths: List of ground truth counts\n",
        "        n: Maximum allowed difference (default=1)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing accuracy for each tolerance level up to n\n",
        "    \"\"\"\n",
        "    # Convert to numeric values if they're strings\n",
        "    preds = [int(str(p)) for p in predictions]\n",
        "    truths = [int(str(g)) for g in ground_truths]\n",
        "\n",
        "    results = {}\n",
        "    # Cumulative accuracies\n",
        "    for tolerance in range(n + 1):\n",
        "        correct = sum(1 for p, t in zip(preds, truths) if abs(p - t) <= tolerance)\n",
        "        accuracy = correct / len(preds)\n",
        "        results[f'off_by_{tolerance}'] = accuracy\n",
        "\n",
        "    # Individual accuracies\n",
        "    for tolerance in range(n + 1):\n",
        "        if tolerance == 0:\n",
        "            results[f'exactly_{tolerance}'] = results[f'off_by_{tolerance}']\n",
        "        else:\n",
        "            # Count only predictions that are exactly off by tolerance\n",
        "            exact_count = sum(1 for p, t in zip(preds, truths) if abs(p - t) == tolerance)\n",
        "            results[f'exactly_{tolerance}'] = exact_count / len(preds)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDwB-LPJcRZr"
      },
      "outputs": [],
      "source": [
        "def analyze_error_distribution(predictions, ground_truths):\n",
        "    \"\"\"\n",
        "    Analyze the distribution of counting errors.\n",
        "\n",
        "    Args:\n",
        "        predictions: List of predicted counts\n",
        "        ground_truths: List of ground truth counts\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing error distribution statistics\n",
        "    \"\"\"\n",
        "    # Convert to numeric values if they're strings\n",
        "    preds = [int(str(p)) for p in predictions]\n",
        "    truths = [int(str(g)) for g in ground_truths]\n",
        "\n",
        "    # Calculate errors\n",
        "    errors = [p - t for p, t in zip(preds, truths)]\n",
        "\n",
        "    # Calculate statistics\n",
        "    mean_error = sum(errors) / len(errors)\n",
        "    median_error = sorted(errors)[len(errors) // 2]\n",
        "\n",
        "    # Count over/under predictions\n",
        "    over_count = sum(1 for e in errors if e > 0)\n",
        "    under_count = sum(1 for e in errors if e < 0)\n",
        "    exact_count = sum(1 for e in errors if e == 0)\n",
        "\n",
        "    return {\n",
        "        'mean_error': mean_error,\n",
        "        'median_error': median_error,\n",
        "        'over_count': over_count,\n",
        "        'under_count': under_count,\n",
        "        'exact_count': exact_count,\n",
        "        'error_distribution': errors\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0To0PqkJUvHm"
      },
      "outputs": [],
      "source": [
        "def analyze_results(results_file, model_name):\n",
        "    \"\"\"Analyze results across different dimensions with additional metrics.\"\"\"\n",
        "    results = load_results(results_file)\n",
        "\n",
        "    # Convert results to DataFrame for easier analysis\n",
        "    df = pd.DataFrame(results)\n",
        "\n",
        "    # 1. Overall Accuracy\n",
        "    overall_accuracy = calculate_accuracy(df['model_answer'], df['ground_truth'])\n",
        "    print(f\"\\n{model_name} Overall Accuracy: {overall_accuracy:.2%}\")\n",
        "\n",
        "    # 2. Overall Error Metrics\n",
        "    overall_error_metrics = calculate_error_metrics(df['model_answer'], df['ground_truth'], 'Overall')\n",
        "    print(\"\\nOverall Error Metrics:\")\n",
        "    for metric, value in overall_error_metrics.items():\n",
        "        if metric != 'category' and metric != 'sample_size':\n",
        "            print(f\"{metric}: {value:.2f}\")\n",
        "\n",
        "    # 3. Off-by-N Accuracy (Overall)\n",
        "    off_by_n = calculate_off_by_n_accuracy(df['model_answer'], df['ground_truth'], n=2)\n",
        "    print(\"\\nOverall Off-by-N Accuracy:\")\n",
        "    for n, acc in off_by_n.items():\n",
        "        print(f\"{n}: {acc:.2%}\")\n",
        "\n",
        "    # 4. Error Distribution (Overall)\n",
        "    error_dist = analyze_error_distribution(df['model_answer'], df['ground_truth'])\n",
        "    print(\"\\nOverall Error Distribution:\")\n",
        "    print(f\"Mean Error: {error_dist['mean_error']:.2f}\")\n",
        "    print(f\"Median Error: {error_dist['median_error']:.2f}\")\n",
        "    print(f\"Over-counts: {error_dist['over_count']}\")\n",
        "    print(f\"Under-counts: {error_dist['under_count']}\")\n",
        "    print(f\"Exact counts: {error_dist['exact_count']}\")\n",
        "\n",
        "    # 5. Question-type Analysis\n",
        "    df['question_type'] = df.apply(lambda x: int(x['question_id'].replace('Q', '')), axis=1)\n",
        "    df['question_category'] = df['question_type'].map({\n",
        "        1: 'Direct Recognition',\n",
        "        2: 'Property Inference',\n",
        "        3: 'Counterfactual'\n",
        "    })\n",
        "\n",
        "    question_accuracies = df.groupby('question_category').apply(\n",
        "        lambda x: calculate_accuracy(x['model_answer'], x['ground_truth'])\n",
        "    )\n",
        "    print(\"\\nAccuracy by Question Type:\")\n",
        "    for q_type, acc in question_accuracies.items():\n",
        "        print(f\"{q_type}: {acc:.2%}\")\n",
        "\n",
        "    # 6. Image Type Analysis\n",
        "    image_type_accuracies = df.groupby('image_type').apply(\n",
        "        lambda x: calculate_accuracy(x['model_answer'], x['ground_truth'])\n",
        "    )\n",
        "    print(\"\\nAccuracy by Image Type:\")\n",
        "    for img_type, acc in image_type_accuracies.items():\n",
        "        print(f\"{img_type}: {acc:.2%}\")\n",
        "\n",
        "    # 7. Property Category Analysis\n",
        "    property_accuracies = df.groupby('property_category').apply(\n",
        "        lambda x: calculate_accuracy(x['model_answer'], x['ground_truth'])\n",
        "    )\n",
        "    print(\"\\nAccuracy by Property Category:\")\n",
        "    for prop, acc in property_accuracies.items():\n",
        "        print(f\"{prop}: {acc:.2%}\")\n",
        "\n",
        "\n",
        "    # Calculate error metrics for each question category\n",
        "    question_error_metrics = {}\n",
        "    for q_type in df['question_category'].unique():\n",
        "        q_df = df[df['question_category'] == q_type]\n",
        "        question_error_metrics[q_type] = calculate_error_metrics(\n",
        "            q_df['model_answer'],\n",
        "            q_df['ground_truth'],\n",
        "            q_type\n",
        "        )\n",
        "\n",
        "    print(\"\\nError Metrics by Question Type:\")\n",
        "    for q_type, metrics in question_error_metrics.items():\n",
        "        print(f\"\\n{q_type}:\")\n",
        "        for metric, value in metrics.items():\n",
        "            if metric != 'category' and metric != 'sample_size':\n",
        "                print(f\"{metric}: {value:.2f}\")\n",
        "\n",
        "    # Similar analysis for image types and property categories\n",
        "    image_error_metrics = {}\n",
        "    for i_type in df['image_type'].unique():\n",
        "        i_df = df[df['image_type'] == i_type]\n",
        "        image_error_metrics[i_type] = calculate_error_metrics(\n",
        "            i_df['model_answer'],\n",
        "            i_df['ground_truth'],\n",
        "            i_type\n",
        "        )\n",
        "\n",
        "    print(\"\\nError Metrics by Image Type:\")\n",
        "    for i_type, metrics in image_error_metrics.items():\n",
        "        print(f\"\\n{i_type}:\")\n",
        "        for metric, value in metrics.items():\n",
        "            if metric != 'category' and metric != 'sample_size':\n",
        "                print(f\"{metric}: {value:.2f}\")\n",
        "\n",
        "\n",
        "    property_error_metrics = {}\n",
        "    for p_type in df['property_category'].unique():\n",
        "        p_df = df[df['property_category'] == p_type]\n",
        "        property_error_metrics[p_type] = calculate_error_metrics(\n",
        "            p_df['model_answer'],\n",
        "            p_df['ground_truth'],\n",
        "            p_type\n",
        "        )\n",
        "\n",
        "    print(\"\\nError Metrics by Property caterogry:\")\n",
        "    for p_type, metrics in property_error_metrics.items():\n",
        "        print(f\"\\n{p_type}:\")\n",
        "        for metric, value in metrics.items():\n",
        "            if metric != 'category' and metric != 'sample_size':\n",
        "                print(f\"{metric}: {value:.2f}\")\n",
        "\n",
        "    return {\n",
        "        'overall': overall_accuracy,\n",
        "        'overall_error_metrics': overall_error_metrics,\n",
        "        'question_error_metrics': question_error_metrics,\n",
        "        'off_by_n': off_by_n,\n",
        "        'error_distribution': error_dist,\n",
        "        'question_type': question_accuracies,\n",
        "        'image_type': image_type_accuracies,\n",
        "        'property': property_accuracies,\n",
        "        'df': df\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rdul2qVUvKi",
        "outputId": "419709d2-f657-4d72-b9a5-8975ecc4067e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "gpt-4o-mini Overall Accuracy: 30.37%\n",
            "\n",
            "Overall Error Metrics:\n",
            "MAE: 1.94\n",
            "MSE: 230.88\n",
            "RMSE: 15.19\n",
            "\n",
            "Overall Off-by-N Accuracy:\n",
            "off_by_0: 30.37%\n",
            "off_by_1: 64.17%\n",
            "off_by_2: 80.46%\n",
            "exactly_0: 30.37%\n",
            "exactly_1: 33.80%\n",
            "exactly_2: 16.30%\n",
            "\n",
            "Overall Error Distribution:\n",
            "Mean Error: -0.36\n",
            "Median Error: 0.00\n",
            "Over-counts: 230\n",
            "Under-counts: 522\n",
            "Exact counts: 328\n",
            "\n",
            "Accuracy by Question Type:\n",
            "Counterfactual: 28.61%\n",
            "Direct Recognition: 28.89%\n",
            "Property Inference: 33.61%\n",
            "\n",
            "Accuracy by Image Type:\n",
            "AI_GENERATED: 27.78%\n",
            "ANIMATED: 38.89%\n",
            "REAL: 24.44%\n",
            "\n",
            "Accuracy by Property Category:\n",
            "functional: 25.00%\n",
            "physical: 21.31%\n",
            "relational: 37.81%\n",
            "taxonomic: 34.23%\n",
            "\n",
            "Error Metrics by Question Type:\n",
            "\n",
            "Direct Recognition:\n",
            "MAE: 2.96\n",
            "MSE: 683.59\n",
            "RMSE: 26.15\n",
            "\n",
            "Property Inference:\n",
            "MAE: 1.30\n",
            "MSE: 3.92\n",
            "RMSE: 1.98\n",
            "\n",
            "Counterfactual:\n",
            "MAE: 1.56\n",
            "MSE: 5.11\n",
            "RMSE: 2.26\n",
            "\n",
            "Error Metrics by Image Type:\n",
            "\n",
            "REAL:\n",
            "MAE: 1.83\n",
            "MSE: 6.96\n",
            "RMSE: 2.64\n",
            "\n",
            "ANIMATED:\n",
            "MAE: 2.53\n",
            "MSE: 681.17\n",
            "RMSE: 26.10\n",
            "\n",
            "AI_GENERATED:\n",
            "MAE: 1.47\n",
            "MSE: 4.49\n",
            "RMSE: 2.12\n",
            "\n",
            "Error Metrics by Property caterogry:\n",
            "\n",
            "physical:\n",
            "MAE: 3.77\n",
            "MSE: 1005.98\n",
            "RMSE: 31.72\n",
            "\n",
            "functional:\n",
            "MAE: 1.44\n",
            "MSE: 4.17\n",
            "RMSE: 2.04\n",
            "\n",
            "relational:\n",
            "MAE: 1.26\n",
            "MSE: 3.89\n",
            "RMSE: 1.97\n",
            "\n",
            "taxonomic:\n",
            "MAE: 1.51\n",
            "MSE: 5.61\n",
            "RMSE: 2.37\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-11-2790841813.py:42: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  question_accuracies = df.groupby('question_category').apply(\n",
            "/tmp/ipython-input-11-2790841813.py:50: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  image_type_accuracies = df.groupby('image_type').apply(\n",
            "/tmp/ipython-input-11-2790841813.py:58: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  property_accuracies = df.groupby('property_category').apply(\n"
          ]
        }
      ],
      "source": [
        "results = analyze_results(output_file, model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3AWqfwQ0Cd1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
