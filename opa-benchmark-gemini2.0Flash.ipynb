{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdead5bb",
   "metadata": {
    "papermill": {
     "duration": 0.008378,
     "end_time": "2025-06-09T20:28:17.728651",
     "exception": false,
     "start_time": "2025-06-09T20:28:17.720273",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# VLM Benchmark for Object Property Abstraction\n",
    "\n",
    "This notebook implements a benchmark for evaluating Vision Language Models (VLMs) on object property abstraction and visual question answering (VQA) tasks. The benchmark includes three types of questions:\n",
    "\n",
    "1. Direct Recognition\n",
    "2. Property Inference\n",
    "3. Counterfactual Reasoning\n",
    "\n",
    "And three types of images:\n",
    "- REAL\n",
    "- ANIMATED\n",
    "- AI GENERATED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2054ebe9",
   "metadata": {
    "papermill": {
     "duration": 0.003898,
     "end_time": "2025-06-09T20:28:17.737077",
     "exception": false,
     "start_time": "2025-06-09T20:28:17.733179",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5157a60",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-09T20:28:17.745702Z",
     "iopub.status.busy": "2025-06-09T20:28:17.745431Z",
     "iopub.status.idle": "2025-06-09T20:28:17.749619Z",
     "shell.execute_reply": "2025-06-09T20:28:17.748977Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 0.009882,
     "end_time": "2025-06-09T20:28:17.750766",
     "exception": false,
     "start_time": "2025-06-09T20:28:17.740884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# %pip install transformers torch Pillow tqdm bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccf8a7c",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-09T20:28:17.759324Z",
     "iopub.status.busy": "2025-06-09T20:28:17.759004Z",
     "iopub.status.idle": "2025-06-09T20:28:19.490155Z",
     "shell.execute_reply": "2025-06-09T20:28:19.489193Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 1.737039,
     "end_time": "2025-06-09T20:28:19.491619",
     "exception": false,
     "start_time": "2025-06-09T20:28:17.754580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install qwen-vl-utils flash-attn #--no-build-isolation\n",
    "%pip install -q -U google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d2356f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T20:28:19.502850Z",
     "iopub.status.busy": "2025-06-09T20:28:19.502575Z",
     "iopub.status.idle": "2025-06-09T20:28:25.674870Z",
     "shell.execute_reply": "2025-06-09T20:28:25.673757Z"
    },
    "papermill": {
     "duration": 6.179241,
     "end_time": "2025-06-09T20:28:25.676579",
     "exception": false,
     "start_time": "2025-06-09T20:28:19.497338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import gc\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Any\n",
    "from google import genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15391e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get API key from environment\n",
    "api_key = os.getenv('GOOGLE_API_KEY')\n",
    "if not api_key:\n",
    "    raise ValueError(\"GOOGLE_API_KEY not found in environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ca2b9a",
   "metadata": {
    "papermill": {
     "duration": 0.004194,
     "end_time": "2025-06-09T20:28:25.686407",
     "exception": false,
     "start_time": "2025-06-09T20:28:25.682213",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Benchmark Tester Class\n",
    "\n",
    "This class handles the evaluation of models against our benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28935166",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T20:28:25.696680Z",
     "iopub.status.busy": "2025-06-09T20:28:25.696171Z",
     "iopub.status.idle": "2025-06-09T20:28:25.720893Z",
     "shell.execute_reply": "2025-06-09T20:28:25.720152Z"
    },
    "papermill": {
     "duration": 0.031348,
     "end_time": "2025-06-09T20:28:25.722049",
     "exception": false,
     "start_time": "2025-06-09T20:28:25.690701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BenchmarkTester:\n",
    "    def __init__(self, benchmark_path=\"/var/scratch/ave303/OP_bench/benchmark.json\", data_dir=\"/var/scratch/ave303/OP_bench/\"):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        with open(benchmark_path, 'r') as f:\n",
    "            self.benchmark = json.load(f)\n",
    "        self.data_dir = data_dir\n",
    "        self.api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "    def clean_answer(self, answer):\n",
    "        \"\"\"Clean the model output to extract just the number.\"\"\"\n",
    "        # Remove any text that's not a number\n",
    "        # import re\n",
    "        # numbers = re.findall(r'\\d+', answer)\n",
    "        # if numbers:\n",
    "        #     return numbers[0]  # Return the first number found\n",
    "        # return answer\n",
    "        \"\"\"Extract number and reasoning from the model's answer.\"\"\"\n",
    "        # Try to extract number and reasoning using regex\n",
    "        import re\n",
    "        pattern = r'(\\d+)\\s*\\[(.*?)\\]'\n",
    "        match = re.search(pattern, answer)\n",
    "        \n",
    "        if match:\n",
    "            number = match.group(1)\n",
    "            objects = [obj.strip() for obj in match.group(2).split(',')]\n",
    "            return {\n",
    "                \"count\": number,\n",
    "                \"reasoning\": objects\n",
    "            }\n",
    "        else:\n",
    "            # Fallback if format isn't matched\n",
    "            numbers = re.findall(r'\\d+', answer)\n",
    "            return {\n",
    "                \"count\": numbers[0] if numbers else \"0\",\n",
    "                \"reasoning\": []\n",
    "            }\n",
    "\n",
    "    # def model_generation(self, model_name, model, inputs, processor):\n",
    "    #     \"\"\"Generate answer and decode.\"\"\"\n",
    "    #     outputs = None  # Initialize outputs to None\n",
    "    #     input_len = inputs[\"input_ids\"].shape[-1]\n",
    "        \n",
    "    #     if model_name==\"Gemma3\":\n",
    "    #         outputs = model.generate(**inputs, max_new_tokens=200, do_sample=False)\n",
    "    #         outputs = outputs[0][input_len:]\n",
    "    #         answer = processor.decode(outputs, skip_special_tokens=True)\n",
    "    #         # outputs = model.generate(**inputs, do_sample=False, max_new_tokens=100)\n",
    "    #         # answer = processor.batch_decode(\n",
    "    #         #     outputs,\n",
    "    #         #     skip_special_tokens=True,\n",
    "    #         # )[0]\n",
    "    #     else:\n",
    "    #         print(f\"Warning: Unknown model name '{model_name}' in model_generation.\")\n",
    "    #         answer = \"\"  # Return an empty string\n",
    "\n",
    "    #     return answer, outputs\n",
    "    \n",
    "    def evaluate_model(self, model_name, save_path, start_idx=0, batch_size=5):\n",
    "        results = []\n",
    "        print(f\"\\nEvaluating {model_name}...\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # client = genai.Client(api_key=self.api_key)\n",
    "\n",
    "        # Force garbage collection before starting\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        try:\n",
    "            images = self.benchmark['benchmark']['images'][start_idx:start_idx + batch_size]\n",
    "            total_images = len(images)\n",
    "            \n",
    "            for idx, image_data in enumerate(tqdm(images, desc=\"Processing images\")):\n",
    "                try:\n",
    "                    client = genai.Client(api_key=self.api_key)\n",
    "                    print(f\"\\nProcessing image {idx+1}/{total_images}: {image_data['image_id']}\")\n",
    "                    image_path = Path(self.data_dir)/image_data['path']\n",
    "                    if not image_path.exists():\n",
    "                        print(f\"Warning: Image not found at {image_path}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Load and preprocess image\n",
    "                    # image = Image.open(image_path).convert(\"RGB\")\n",
    "                    \n",
    "                    image = client.files.upload(file=image_path)\n",
    "                    image_results = []  # Store results for current image\n",
    "                    \n",
    "                    for question in image_data['questions']:\n",
    "                        try:\n",
    "                            # prompt = self.format_question(question, model_name)\n",
    "                            print(f\"Question: {question['question']}\")\n",
    "\n",
    "                            # messages = [\n",
    "                            #     {\n",
    "                            #         \"role\": \"user\",\n",
    "                            #         \"content\": [\n",
    "                            #             {\"type\": \"image\", \"image\": image},\n",
    "                            #             # {\"type\": \"text\", \"text\": f\"{question['question']} Answer format: total number(numerical) objects(within square brackets)\"},\n",
    "                            #             # {\"type\": \"text\", \"text\": f\"{question['question']} Provide just the total count and the list of objects in the given format \\n Format: number [objects]\"},\n",
    "                            #             # {\"type\": \"text\", \"text\": f\"{question['question']} Answer Format: number [objects]\"},\n",
    "                            #             {\"type\": \"text\", \"text\": f\"{question[\"question\"]} Your response MUST be in the following format and nothing else:\\n <NUMBER> [<OBJECT1>, <OBJECT2>, <OBJECT3>, ...]\"}\n",
    "                            #         ]\n",
    "                            #     },\n",
    "                            # ]\n",
    "                            # messages = [\n",
    "                            #     {\n",
    "                            #         \"role\": \"system\",\n",
    "                            #         \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n",
    "                            #     },\n",
    "                            #     {\n",
    "                            #         \"role\": \"user\",\n",
    "                            #         \"content\": [\n",
    "                            #             {\"type\": \"image\", \"image\": image},\n",
    "                            #             {\"type\": \"text\", \"text\": f\"{question['question']} Your response MUST be in the following format and nothing else:\\n <NUMBER> [<OBJECT1>, <OBJECT2>, <OBJECT3>, ...]\"}\n",
    "                            #         ]\n",
    "                            #     }\n",
    "                            # ]\n",
    "\n",
    "                            response = client.models.generate_content(\n",
    "                                model=model_name,\n",
    "                                contents=[image, f\"{question['question']} Your response MUST be in the following format and nothing else:\\n <NUMBER> [<OBJECT1>, <OBJECT2>, <OBJECT3>, ...]\"],\n",
    "                            )\n",
    "                            # print(response.text)\n",
    "                            \n",
    "                            # Clear cache before processing each question\n",
    "                            torch.cuda.empty_cache()\n",
    "                            \n",
    "                            # Process image and text\n",
    "                            # inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(self.device)\n",
    "                            # if model_name==\"smolVLM2\":\n",
    "                            # inputs = processor.apply_chat_template(\n",
    "                            #     messages,\n",
    "                            #     add_generation_prompt=True,\n",
    "                            #     tokenize=True,\n",
    "                            #     return_dict=True,\n",
    "                            #     return_tensors=\"pt\",\n",
    "                            # ).to(model.device, dtype=torch.bfloat16)\n",
    "                           \n",
    "                            # with torch.no_grad():\n",
    "                            #     answer, outputs = self.model_generation(model_name, model, inputs, processor)    #call for model.generate\n",
    "        \n",
    "                            cleaned_answer = self.clean_answer(response.text)\n",
    "                            \n",
    "                            image_results.append({\n",
    "                                \"image_id\": image_data[\"image_id\"],\n",
    "                                \"image_type\": image_data[\"image_type\"],\n",
    "                                \"question_id\": question[\"id\"],\n",
    "                                \"question\": question[\"question\"],\n",
    "                                \"ground_truth\": question[\"answer\"],\n",
    "                                \"model_answer\": cleaned_answer[\"count\"],\n",
    "                                \"model_reasoning\": cleaned_answer[\"reasoning\"],\n",
    "                                \"raw_answer\": response.text,  # Keep raw answer for debugging\n",
    "                                \"property_category\": question[\"property_category\"]\n",
    "                            })\n",
    "                            \n",
    "                            # # Clear memory\n",
    "                            # del outputs, inputs\n",
    "                            # torch.cuda.empty_cache()\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing question: {str(e)}\")\n",
    "                            continue\n",
    "                    \n",
    "                    # Add results from this image\n",
    "                    results.extend(image_results)\n",
    "                    \n",
    "                    # # Save intermediate results only every 2 images or if it's the last image\n",
    "                    # if (idx + 1) % 2 == 0 or idx == total_images - 1:\n",
    "                    #     with open(f\"{save_path}_checkpoint.json\", 'w') as f:\n",
    "                    #         json.dump(results, f, indent=4)\n",
    "\n",
    "                    client.files.delete(name=image.name)\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing image {image_data['image_id']}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            # Save final results\n",
    "            if results:\n",
    "                with open(save_path, 'w') as f:\n",
    "                    json.dump(results, f, indent=4)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during evaluation: {str(e)}\")\n",
    "            if results:\n",
    "                with open(f\"{save_path}_error_state.json\", 'w') as f:\n",
    "                    json.dump(results, f, indent=4)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494c1b67",
   "metadata": {},
   "source": [
    "## Test Gemini 2.0 Flash\n",
    "\n",
    "Let's evaluate Gemini-2.0-flash model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2f9bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_Gemini2_0Flash():\n",
    "    tester = BenchmarkTester()\n",
    "    Gemini2_0Flash_results = tester.evaluate_model(\n",
    "        \"gemini-2.0-flash\",\n",
    "        \"Gemini2_0Flash_results.json\",\n",
    "        start_idx=45,\n",
    "        batch_size=5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808accb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_Gemini2_0Flash_loop():\n",
    "#     for i in range(0, 45, 5):\n",
    "#         tester = BenchmarkTester()\n",
    "#         Gemini2_0Flash_results = tester.evaluate_model(\n",
    "#             \"gemini-2.0-flash\",\n",
    "#             f\"Gemini2_0Flash_results{i}.json\",\n",
    "#             start_idx=i,\n",
    "#             batch_size=5\n",
    "#         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00f4a8a",
   "metadata": {
    "papermill": {
     "duration": 0.004177,
     "end_time": "2025-06-09T20:28:25.769210",
     "exception": false,
     "start_time": "2025-06-09T20:28:25.765033",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Run Evaluation\n",
    "\n",
    "Now we can run our evaluation. Let's start with the SmolVLM2 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986c501a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T20:28:25.778400Z",
     "iopub.status.busy": "2025-06-09T20:28:25.778165Z",
     "iopub.status.idle": "2025-06-09T20:30:53.367049Z",
     "shell.execute_reply": "2025-06-09T20:30:53.366368Z"
    },
    "papermill": {
     "duration": 147.594937,
     "end_time": "2025-06-09T20:30:53.368263",
     "exception": false,
     "start_time": "2025-06-09T20:28:25.773326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_Gemma3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42818981",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Gemini2_0Flash()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d4768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import os\n",
    "\n",
    "# def merge_json_results(base_filename=\"Gemini2_0Flash_results\", output_filename=\"Gemini2_0Flash_results1.json\"):\n",
    "#     \"\"\"\n",
    "#     Merges multiple JSON files generated by the Gemini2_0Flash_loop into a single file.\n",
    "#     Assumes each JSON file contains a list of dictionaries.\n",
    "\n",
    "#     Args:\n",
    "#         base_filename (str): The base name of the individual JSON files (e.g., \"Gemini2_0Flash_results\").\n",
    "#         output_filename (str): The name of the consolidated output JSON file.\n",
    "#     \"\"\"\n",
    "#     all_merged_results = [] # Initialize as an empty LIST, not a dictionary\n",
    "\n",
    "#     # Iterate with the same `i` values as your original loop (0, 5, ..., 45)\n",
    "#     for j in range(0, 50, 5):\n",
    "#         filename = f\"{base_filename}{j}.json\" # Constructs filename like Gemini2_0Flash_results0.json, etc.\n",
    "\n",
    "#         if os.path.exists(filename):\n",
    "#             with open(filename, 'r') as f:\n",
    "#                 try:\n",
    "#                     batch_results = json.load(f)\n",
    "#                     # Check if the loaded data is indeed a list, if not, handle error\n",
    "#                     if isinstance(batch_results, list):\n",
    "#                         all_merged_results.extend(batch_results) # Extend the master list\n",
    "#                     else:\n",
    "#                         print(f\"Warning: {filename} does not contain a JSON list. Skipping.\")\n",
    "#                 except json.JSONDecodeError:\n",
    "#                     print(f\"Warning: Could not decode JSON from {filename}. Skipping.\")\n",
    "#         else:\n",
    "#             print(f\"Warning: File {filename} not found. Skipping.\")\n",
    "\n",
    "#     # Save the merged results\n",
    "#     with open(output_filename, 'w') as f:\n",
    "#         json.dump(all_merged_results, f, indent=4) # indent for pretty printing\n",
    "\n",
    "#     print(f\"Successfully merged {len(all_merged_results)} results into {output_filename}\")\n",
    "\n",
    "\n",
    "# # After your `test_Gemini2_0Flash_loop()` function finishes running:\n",
    "# # (Make sure your test_Gemini2_0Flash_loop() function runs first to create the files)\n",
    "# # For example:\n",
    "# # test_Gemini2_0Flash_loop()\n",
    "# #\n",
    "# # Then call the merge function:\n",
    "# merge_json_results(output_filename=\"Gemini2_0Flash_results1.json\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7163706,
     "sourceId": 11436696,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python ((op_bench)",
   "language": "python",
   "name": "op_bench"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 160.663536,
   "end_time": "2025-06-09T20:30:56.721727",
   "environment_variables": {},
   "exception": null,
   "input_path": "/var/scratch/ave303/OP_bench/opa-benchmark-gemma3-27b-it.ipynb",
   "output_path": "gemma3-27b-it_output.ipynb",
   "parameters": {},
   "start_time": "2025-06-09T20:28:16.058191",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
