{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fef7e23",
   "metadata": {
    "papermill": {
     "duration": 0.005878,
     "end_time": "2025-07-28T14:03:38.908550",
     "exception": false,
     "start_time": "2025-07-28T14:03:38.902672",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# VLM Benchmark for Object Property Abstraction\n",
    "\n",
    "This notebook implements a benchmark for evaluating Vision Language Models (VLMs) on object property abstraction and visual question answering (VQA) tasks. The benchmark includes three types of questions:\n",
    "\n",
    "1. Direct Recognition\n",
    "2. Property Inference\n",
    "3. Counterfactual Reasoning\n",
    "\n",
    "And three types of images:\n",
    "- REAL\n",
    "- ANIMATED\n",
    "- AI GENERATED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492240cd",
   "metadata": {
    "papermill": {
     "duration": 0.004939,
     "end_time": "2025-07-28T14:03:38.918956",
     "exception": false,
     "start_time": "2025-07-28T14:03:38.914017",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "310ebe2d",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-07-28T14:03:38.930404Z",
     "iopub.status.busy": "2025-07-28T14:03:38.929687Z",
     "iopub.status.idle": "2025-07-28T14:03:38.934069Z",
     "shell.execute_reply": "2025-07-28T14:03:38.933307Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 0.011242,
     "end_time": "2025-07-28T14:03:38.935250",
     "exception": false,
     "start_time": "2025-07-28T14:03:38.924008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# %pip install transformers torch Pillow tqdm bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4551413",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-07-28T14:03:38.946218Z",
     "iopub.status.busy": "2025-07-28T14:03:38.945902Z",
     "iopub.status.idle": "2025-07-28T14:03:40.684215Z",
     "shell.execute_reply": "2025-07-28T14:03:40.683255Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 1.745268,
     "end_time": "2025-07-28T14:03:40.685602",
     "exception": false,
     "start_time": "2025-07-28T14:03:38.940334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: qwen-vl-utils in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (0.0.11)\r\n",
      "Requirement already satisfied: flash-attn in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (2.7.4.post1)\r\n",
      "Requirement already satisfied: av in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from qwen-vl-utils) (14.3.0)\r\n",
      "Requirement already satisfied: packaging in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from qwen-vl-utils) (25.0)\r\n",
      "Requirement already satisfied: pillow in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from qwen-vl-utils) (10.3.0)\r\n",
      "Requirement already satisfied: requests in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from qwen-vl-utils) (2.32.3)\r\n",
      "Requirement already satisfied: torch in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from flash-attn) (2.2.1)\r\n",
      "Requirement already satisfied: einops in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from flash-attn) (0.8.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from requests->qwen-vl-utils) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from requests->qwen-vl-utils) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from requests->qwen-vl-utils) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from requests->qwen-vl-utils) (2025.4.26)\r\n",
      "Requirement already satisfied: filelock in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (3.18.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (4.13.2)\r\n",
      "Requirement already satisfied: sympy in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (1.14.0)\r\n",
      "Requirement already satisfied: networkx in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (2025.3.2)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (8.9.2.26)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (10.3.2.106)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (2.19.3)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from torch->flash-attn) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash-attn) (12.8.93)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: MarkupSafe>=2.0 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from jinja2->torch->flash-attn) (3.0.2)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages (from sympy->torch->flash-attn) (1.3.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install qwen-vl-utils flash-attn #--no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de455c0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T14:03:40.698488Z",
     "iopub.status.busy": "2025-07-28T14:03:40.698034Z",
     "iopub.status.idle": "2025-07-28T14:03:46.430672Z",
     "shell.execute_reply": "2025-07-28T14:03:46.429730Z"
    },
    "papermill": {
     "duration": 5.740236,
     "end_time": "2025-07-28T14:03:46.432018",
     "exception": false,
     "start_time": "2025-07-28T14:03:40.691782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/scratch/ave303/anaconda3/envs/op_bench/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import gc\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Any\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import time\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7baf9ef",
   "metadata": {
    "papermill": {
     "duration": 0.005387,
     "end_time": "2025-07-28T14:03:46.443737",
     "exception": false,
     "start_time": "2025-07-28T14:03:46.438350",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Benchmark Tester Class\n",
    "\n",
    "This class handles the evaluation of models against our benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3c1615b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T14:03:46.456565Z",
     "iopub.status.busy": "2025-07-28T14:03:46.456165Z",
     "iopub.status.idle": "2025-07-28T14:03:46.467224Z",
     "shell.execute_reply": "2025-07-28T14:03:46.466480Z"
    },
    "papermill": {
     "duration": 0.019196,
     "end_time": "2025-07-28T14:03:46.468371",
     "exception": false,
     "start_time": "2025-07-28T14:03:46.449175",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class BenchmarkTester:\n",
    "#     def __init__(self, benchmark_path=\"/var/scratch/ave303/OP_bench/benchmark.json\", data_dir=\"/var/scratch/ave303/OP_bench/\"):\n",
    "#         self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#         with open(benchmark_path, 'r') as f:\n",
    "#             self.benchmark = json.load(f)\n",
    "#         self.data_dir = data_dir\n",
    "    \n",
    "#     def format_question(self, question, model_name):\n",
    "#         \"\"\"Format a question for the model.\"\"\"\n",
    "\n",
    "#         if model_name==\"blip2\":\n",
    "#             return f\"Question: {question['question']} Answer:\"\n",
    "#         else:\n",
    "#             return f\"Question: {question['question']} Answer with a number and list of objects. Answer:\"\n",
    "\n",
    "#     def clean_answer(self, answer):\n",
    "#         \"\"\"Clean the model output to extract just the number.\"\"\"\n",
    "#         # Remove any text that's not a number\n",
    "#         # import re\n",
    "#         # numbers = re.findall(r'\\d+', answer)\n",
    "#         # if numbers:\n",
    "#         #     return numbers[0]  # Return the first number found\n",
    "#         # return answer\n",
    "#         \"\"\"Extract number and reasoning from the model's answer.\"\"\"\n",
    "#         # Try to extract number and reasoning using regex\n",
    "#         import re\n",
    "#         pattern = r'(\\d+)\\s*\\[(.*?)\\]'\n",
    "#         match = re.search(pattern, answer)\n",
    "        \n",
    "#         if match:\n",
    "#             number = match.group(1)\n",
    "#             objects = [obj.strip() for obj in match.group(2).split(',')]\n",
    "#             return {\n",
    "#                 \"count\": number,\n",
    "#                 \"reasoning\": objects\n",
    "#             }\n",
    "#         else:\n",
    "#             # Fallback if format isn't matched\n",
    "#             numbers = re.findall(r'\\d+', answer)\n",
    "#             return {\n",
    "#                 \"count\": numbers[0] if numbers else \"0\",\n",
    "#                 \"reasoning\": []\n",
    "#             }\n",
    "\n",
    "#     def model_generation(self, model_name, model, inputs, processor):\n",
    "#         \"\"\"Generate answer and decode.\"\"\"\n",
    "#         outputs = None  # Initialize outputs to None\n",
    "        \n",
    "#         if model_name==\"smolVLM2\":\n",
    "#             outputs = model.generate(**inputs, do_sample=False, max_new_tokens=64)\n",
    "#             answer = processor.batch_decode(\n",
    "#                 outputs,\n",
    "#                 skip_special_tokens=True,\n",
    "#             )[0]\n",
    "#         elif model_name==\"Qwen2.5-VL\":\n",
    "#             outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "#             outputs = [\n",
    "#                 out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, outputs)\n",
    "#             ]\n",
    "#             answer = processor.batch_decode(\n",
    "#                 outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "#             )[0]\n",
    "#         else:\n",
    "#             print(f\"Warning: Unknown model name '{model_name}' in model_generation.\")\n",
    "#             answer = \"\"  # Return an empty string\n",
    "\n",
    "#         return answer, outputs\n",
    "    \n",
    "#     def evaluate_model(self, model_name, model, processor, save_path, start_idx=0, batch_size=5):\n",
    "#         results = []\n",
    "#         print(f\"\\nEvaluating {model_name}...\")\n",
    "#         print(f\"Using device: {self.device}\")\n",
    "        \n",
    "#         # Force garbage collection before starting\n",
    "#         gc.collect()\n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "#         try:\n",
    "#             images = self.benchmark['benchmark']['images'][start_idx:start_idx + batch_size]\n",
    "#             total_images = len(images)\n",
    "            \n",
    "#             for idx, image_data in enumerate(tqdm(images, desc=\"Processing images\")):\n",
    "#                 try:\n",
    "#                     print(f\"\\nProcessing image {idx+1}/{total_images}: {image_data['image_id']}\")\n",
    "#                     image_path = Path(self.data_dir)/image_data['path']\n",
    "#                     if not image_path.exists():\n",
    "#                         print(f\"Warning: Image not found at {image_path}\")\n",
    "#                         continue\n",
    "                    \n",
    "#                     # Load and preprocess image\n",
    "#                     image = Image.open(image_path).convert(\"RGB\")\n",
    "#                     image_results = []  # Store results for current image\n",
    "                    \n",
    "#                     for question in image_data['questions']:\n",
    "#                         try:\n",
    "#                             # prompt = self.format_question(question, model_name)\n",
    "#                             print(f\"Question: {question['question']}\")\n",
    "\n",
    "#                             messages = [\n",
    "#                                 {\n",
    "#                                     \"role\": \"user\",\n",
    "#                                     \"content\": [\n",
    "#                                         {\"type\": \"image\", \"image\": image},\n",
    "#                                         # {\"type\": \"text\", \"text\": f\"{question['question']} Answer format: total number(numerical) objects(within square brackets)\"},\n",
    "#                                         # {\"type\": \"text\", \"text\": f\"{question['question']} Provide just the total count and the list of objects in the given format \\n Format: number [objects]\"},\n",
    "#                                         # {\"type\": \"text\", \"text\": f\"{question['question']} Answer Format: number [objects]\"},\n",
    "#                                         {\"type\": \"text\", \"text\": f\"{question[\"question\"]} Your response MUST be in the following format and nothing else:\\n <NUMBER> [<OBJECT1>, <OBJECT2>, <OBJECT3>, ...]\"}\n",
    "#                                     ]\n",
    "#                                 },\n",
    "#                             ]\n",
    "                            \n",
    "#                             # Clear cache before processing each question\n",
    "#                             torch.cuda.empty_cache()\n",
    "                            \n",
    "#                             # Process image and text\n",
    "#                             # inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(self.device)\n",
    "#                             if model_name==\"smolVLM2\":\n",
    "#                                 inputs = processor.apply_chat_template(\n",
    "#                                     messages,\n",
    "#                                     add_generation_prompt=True,\n",
    "#                                     tokenize=True,\n",
    "#                                     return_dict=True,\n",
    "#                                     return_tensors=\"pt\",\n",
    "#                                 ).to(model.device, dtype=torch.float16)\n",
    "#                             else:\n",
    "                                \n",
    "#                                 text = processor.apply_chat_template(\n",
    "#                                     messages, tokenize=False, add_generation_prompt=True\n",
    "#                                 )\n",
    "#                                 # image_inputs, video_inputs = process_vision_info(messages)\n",
    "#                                 inputs = processor(\n",
    "#                                     text=text,\n",
    "#                                     images=image,\n",
    "#                                     videos=None,\n",
    "#                                     padding=True,\n",
    "#                                     return_tensors=\"pt\",\n",
    "#                                 ).to(\"cuda\")\n",
    "                            \n",
    "#                             # Generate answer with better settings\n",
    "#                             with torch.no_grad():\n",
    "#                                 answer, outputs = self.model_generation(model_name, model, inputs, processor)    #call for model.generate\n",
    "        \n",
    "#                             cleaned_answer = self.clean_answer(answer)\n",
    "                            \n",
    "#                             image_results.append({\n",
    "#                                 \"image_id\": image_data[\"image_id\"],\n",
    "#                                 \"image_type\": image_data[\"image_type\"],\n",
    "#                                 \"question_id\": question[\"id\"],\n",
    "#                                 \"question\": question[\"question\"],\n",
    "#                                 \"ground_truth\": question[\"answer\"],\n",
    "#                                 \"model_answer\": cleaned_answer[\"count\"],\n",
    "#                                 \"model_reasoning\": cleaned_answer[\"reasoning\"],\n",
    "#                                 \"raw_answer\": answer,  # Keep raw answer for debugging\n",
    "#                                 \"property_category\": question[\"property_category\"]\n",
    "#                             })\n",
    "                            \n",
    "#                             # Clear memory\n",
    "#                             del outputs, inputs\n",
    "#                             torch.cuda.empty_cache()\n",
    "                            \n",
    "#                         except Exception as e:\n",
    "#                             print(f\"Error processing question: {str(e)}\")\n",
    "#                             continue\n",
    "                    \n",
    "#                     # Add results from this image\n",
    "#                     results.extend(image_results)\n",
    "                    \n",
    "#                     # Save intermediate results only every 2 images or if it's the last image\n",
    "#                     if (idx + 1) % 2 == 0 or idx == total_images - 1:\n",
    "#                         with open(f\"{save_path}_checkpoint.json\", 'w') as f:\n",
    "#                             json.dump(results, f, indent=4)\n",
    "                            \n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error processing image {image_data['image_id']}: {str(e)}\")\n",
    "#                     continue\n",
    "            \n",
    "#             # Save final results\n",
    "#             if results:\n",
    "#                 with open(save_path, 'w') as f:\n",
    "#                     json.dump(results, f, indent=4)\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"An error occurred during evaluation: {str(e)}\")\n",
    "#             if results:\n",
    "#                 with open(f\"{save_path}_error_state.json\", 'w') as f:\n",
    "#                     json.dump(results, f, indent=4)\n",
    "        \n",
    "#         return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67377d93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T14:03:46.480782Z",
     "iopub.status.busy": "2025-07-28T14:03:46.480543Z",
     "iopub.status.idle": "2025-07-28T14:03:46.514918Z",
     "shell.execute_reply": "2025-07-28T14:03:46.514115Z"
    },
    "papermill": {
     "duration": 0.042134,
     "end_time": "2025-07-28T14:03:46.516138",
     "exception": false,
     "start_time": "2025-07-28T14:03:46.474004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BenchmarkTester:\n",
    "    def __init__(self, benchmark_path=\"/var/scratch/ave303/OP_bench/benchmark.json\", data_dir=\"/var/scratch/ave303/OP_bench/\"):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        with open(benchmark_path, 'r') as f:\n",
    "            self.benchmark = json.load(f)\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "    def clean_answer(self, answer):\n",
    "        \"\"\"Extract number and reasoning from the model's answer.\"\"\"\n",
    "        # Try to extract number and reasoning using regex\n",
    "        import re\n",
    "        pattern = r'(\\d+)\\s*\\[(.*?)\\]'\n",
    "        match = re.search(pattern, answer)\n",
    "        \n",
    "        if match:\n",
    "            number = match.group(1)\n",
    "            objects = [obj.strip() for obj in match.group(2).split(',')]\n",
    "            return {\n",
    "                \"count\": number,\n",
    "                \"reasoning\": objects\n",
    "            }\n",
    "        else:\n",
    "            # Fallback if format isn't matched\n",
    "            numbers = re.findall(r'\\d+', answer)\n",
    "            return {\n",
    "                \"count\": numbers[0] if numbers else \"0\",\n",
    "                \"reasoning\": []\n",
    "            }\n",
    "\n",
    "    def model_generation(self, model_name, model, inputs, processor):\n",
    "        \"\"\"Generate answer and decode with greedy decoding.\"\"\"\n",
    "        outputs = None  # Initialize outputs to None\n",
    "        \n",
    "        if model_name == \"Qwen2.5-VL\":\n",
    "            # Explicit greedy decoding parameters\n",
    "            outputs = model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=200,\n",
    "                do_sample=False,          # Disable sampling for greedy decoding\n",
    "                temperature=None,         # Not used in greedy decoding\n",
    "                top_p=None,              # Not used in greedy decoding  \n",
    "                top_k=None,              # Not used in greedy decoding\n",
    "                num_beams=1,             # Single beam for greedy decoding\n",
    "                early_stopping=False,    # Let it generate until max_tokens or EOS\n",
    "                pad_token_id=processor.tokenizer.pad_token_id,\n",
    "                eos_token_id=processor.tokenizer.eos_token_id\n",
    "            )\n",
    "            outputs = [\n",
    "                out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, outputs)\n",
    "            ]\n",
    "            answer = processor.batch_decode(\n",
    "                outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "            )[0]\n",
    "        else:\n",
    "            print(f\"Warning: Unknown model name '{model_name}' in model_generation.\")\n",
    "            answer = \"\"  # Return an empty string\n",
    "\n",
    "        return answer, outputs\n",
    "    \n",
    "    def evaluate_model(self, model_name, model, processor, save_path, start_idx=0, batch_size=5):\n",
    "        results = []\n",
    "        \n",
    "        # Initialize tracking variables\n",
    "        successful_images = []\n",
    "        failed_images = []\n",
    "        total_questions_processed = 0\n",
    "        total_questions_failed = 0\n",
    "        \n",
    "        print(f\"\\nEvaluating {model_name}...\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Force garbage collection before starting\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        try:\n",
    "            images = self.benchmark['benchmark']['images'][start_idx:start_idx + batch_size]\n",
    "            total_images = len(images)\n",
    "            \n",
    "            for idx, image_data in enumerate(tqdm(images, desc=\"Processing images\")):\n",
    "                image_start_time = time.time()\n",
    "                current_image_questions_failed = 0\n",
    "                current_image_questions_total = 0\n",
    "                \n",
    "                try:\n",
    "                    image_path = Path(self.data_dir)/image_data['path']\n",
    "                    if not image_path.exists():\n",
    "                        failed_images.append({\n",
    "                            'image_id': image_data['image_id'],\n",
    "                            'image_type': image_data.get('image_type', 'unknown'),\n",
    "                            'error_type': 'file_not_found',\n",
    "                            'error_message': f'Image not found at {image_path}'\n",
    "                        })\n",
    "                        continue\n",
    "                    \n",
    "                    # Load and preprocess image\n",
    "                    image = Image.open(image_path).convert(\"RGB\")\n",
    "                    image_results = []  # Store results for current image\n",
    "                    \n",
    "                    for question_idx, question in enumerate(image_data['questions']):\n",
    "                        current_image_questions_total += 1\n",
    "                        total_questions_processed += 1\n",
    "                        \n",
    "                        try:\n",
    "                            messages = [\n",
    "                                {\n",
    "                                    \"role\": \"user\",\n",
    "                                    \"content\": [\n",
    "                                        {\"type\": \"image\", \"image\": image},\n",
    "                                        {\"type\": \"text\", \"text\": f\"{question['question']} Your response MUST be in the following format and nothing else:\\n <NUMBER> [<OBJECT1>, <OBJECT2>, <OBJECT3>, ...]\"}\n",
    "                                    ]\n",
    "                                },\n",
    "                            ]\n",
    "                            \n",
    "                            # Clear cache before processing each question\n",
    "                            torch.cuda.empty_cache()\n",
    "                            \n",
    "                            # Process image and text for Qwen2.5-VL\n",
    "                            text = processor.apply_chat_template(\n",
    "                                messages, tokenize=False, add_generation_prompt=True\n",
    "                            )\n",
    "                            inputs = processor(\n",
    "                                text=text,\n",
    "                                images=image,\n",
    "                                videos=None,\n",
    "                                padding=True,\n",
    "                                return_tensors=\"pt\",\n",
    "                            ).to(\"cuda\")\n",
    "                            \n",
    "                            # Generate answer with greedy decoding\n",
    "                            with torch.no_grad():\n",
    "                                answer, outputs = self.model_generation(model_name, model, inputs, processor)\n",
    "        \n",
    "                            cleaned_answer = self.clean_answer(answer)\n",
    "                            \n",
    "                            image_results.append({\n",
    "                                \"image_id\": image_data[\"image_id\"],\n",
    "                                \"image_type\": image_data.get(\"image_type\", \"unknown\"),\n",
    "                                \"question_id\": question[\"id\"],\n",
    "                                \"question\": question[\"question\"],\n",
    "                                \"ground_truth\": question[\"answer\"],\n",
    "                                \"model_answer\": cleaned_answer[\"count\"],\n",
    "                                \"model_reasoning\": cleaned_answer[\"reasoning\"],\n",
    "                                \"raw_answer\": answer,  # Keep raw answer for debugging\n",
    "                                \"property_category\": question[\"property_category\"]\n",
    "                            })\n",
    "                            \n",
    "                            # Clear memory\n",
    "                            del outputs, inputs\n",
    "                            torch.cuda.empty_cache()\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            current_image_questions_failed += 1\n",
    "                            total_questions_failed += 1\n",
    "                            continue\n",
    "                    \n",
    "                    # Add results from this image\n",
    "                    results.extend(image_results)\n",
    "                    \n",
    "                    # Calculate success rate for this image\n",
    "                    questions_succeeded = current_image_questions_total - current_image_questions_failed\n",
    "                    \n",
    "                    if current_image_questions_failed == 0:\n",
    "                        # All questions succeeded\n",
    "                        successful_images.append({\n",
    "                            'image_id': image_data['image_id'],\n",
    "                            'image_type': image_data.get('image_type', 'unknown'),\n",
    "                            'questions_total': current_image_questions_total,\n",
    "                            'questions_succeeded': questions_succeeded,\n",
    "                            'processing_time': time.time() - image_start_time\n",
    "                        })\n",
    "                    else:\n",
    "                        # Some questions failed\n",
    "                        image_success_rate = (questions_succeeded / current_image_questions_total * 100) if current_image_questions_total > 0 else 0\n",
    "                        failed_images.append({\n",
    "                            'image_id': image_data['image_id'],\n",
    "                            'image_type': image_data.get('image_type', 'unknown'),\n",
    "                            'error_type': 'partial_failure',\n",
    "                            'questions_total': current_image_questions_total,\n",
    "                            'questions_failed': current_image_questions_failed,\n",
    "                            'questions_succeeded': questions_succeeded,\n",
    "                            'success_rate': f\"{image_success_rate:.1f}%\"\n",
    "                        })\n",
    "                    \n",
    "                    # Save intermediate results only every 2 images or if it's the last image\n",
    "                    if (idx + 1) % 2 == 0 or idx == total_images - 1:\n",
    "                        checkpoint_path = f\"{save_path}_checkpoint.json\"\n",
    "                        with open(checkpoint_path, 'w') as f:\n",
    "                            json.dump(results, f, indent=4)\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    failed_images.append({\n",
    "                        'image_id': image_data['image_id'],\n",
    "                        'image_type': image_data.get('image_type', 'unknown'),\n",
    "                        'error_type': 'complete_failure',\n",
    "                        'error_message': str(e)\n",
    "                    })\n",
    "                    continue\n",
    "            \n",
    "            # Save final results\n",
    "            if results:\n",
    "                with open(save_path, 'w') as f:\n",
    "                    json.dump(results, f, indent=4)\n",
    "            \n",
    "        except Exception as e:\n",
    "            if results:\n",
    "                error_save_path = f\"{save_path}_error_state.json\"\n",
    "                with open(error_save_path, 'w') as f:\n",
    "                    json.dump(results, f, indent=4)\n",
    "        \n",
    "        # Print comprehensive summary\n",
    "        self._print_evaluation_summary(\n",
    "            model_name, total_images, successful_images, failed_images, \n",
    "            total_questions_processed, total_questions_failed, len(results)\n",
    "        )\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _print_evaluation_summary(self, model_name, total_images, successful_images, \n",
    "                                failed_images, total_questions_processed, total_questions_failed, total_results):\n",
    "        \"\"\"Print a comprehensive evaluation summary.\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"EVALUATION SUMMARY FOR {model_name.upper()}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Image-level statistics\n",
    "        num_successful = len(successful_images)\n",
    "        num_failed = len(failed_images)\n",
    "        \n",
    "        print(f\"📊 IMAGE PROCESSING SUMMARY:\")\n",
    "        print(f\"   Total images attempted: {total_images}\")\n",
    "        print(f\"   Successfully processed: {num_successful} ({num_successful/total_images*100:.1f}%)\")\n",
    "        print(f\"   Failed images: {num_failed} ({num_failed/total_images*100:.1f}%)\")\n",
    "        \n",
    "        # Question-level statistics\n",
    "        questions_succeeded = total_questions_processed - total_questions_failed\n",
    "        print(f\"\\n📝 QUESTION PROCESSING SUMMARY:\")\n",
    "        print(f\"   Total questions attempted: {total_questions_processed}\")\n",
    "        print(f\"   Successfully processed: {questions_succeeded} ({questions_succeeded/total_questions_processed*100:.1f}%)\")\n",
    "        print(f\"   Failed questions: {total_questions_failed} ({total_questions_failed/total_questions_processed*100:.1f}%)\")\n",
    "        print(f\"   Results saved: {total_results}\")\n",
    "        \n",
    "        # Successful images details\n",
    "        if successful_images:\n",
    "            print(f\"\\n✅ SUCCESSFUL IMAGES ({len(successful_images)}):\")\n",
    "            for img in successful_images:\n",
    "                print(f\"   • {img['image_id']} (Type: {img['image_type']}, \"\n",
    "                      f\"Questions: {img['questions_succeeded']}/{img['questions_total']}, \"\n",
    "                      f\"Time: {img['processing_time']:.1f}s)\")\n",
    "        \n",
    "        # Failed images details\n",
    "        if failed_images:\n",
    "            print(f\"\\n❌ FAILED/PROBLEMATIC IMAGES ({len(failed_images)}):\")\n",
    "            for img in failed_images:\n",
    "                if img['error_type'] == 'complete_failure':\n",
    "                    print(f\"   • {img['image_id']} (Type: {img['image_type']}) - \"\n",
    "                          f\"COMPLETE FAILURE: {img.get('error_message', 'Unknown error')}\")\n",
    "                elif img['error_type'] == 'partial_failure':\n",
    "                    print(f\"   • {img['image_id']} (Type: {img['image_type']}) - \"\n",
    "                          f\"PARTIAL: {img['questions_failed']}/{img['questions_total']} failed \"\n",
    "                          f\"({img['success_rate']} success)\")\n",
    "                elif img['error_type'] == 'file_not_found':\n",
    "                    print(f\"   • {img['image_id']} (Type: {img['image_type']}) - \"\n",
    "                          f\"FILE NOT FOUND: {img['error_message']}\")\n",
    "        \n",
    "        # Group failed images by type\n",
    "        if failed_images:\n",
    "            print(f\"\\n📈 FAILURE ANALYSIS BY IMAGE TYPE:\")\n",
    "            from collections import defaultdict\n",
    "            failures_by_type = defaultdict(list)\n",
    "            for img in failed_images:\n",
    "                failures_by_type[img['image_type']].append(img)\n",
    "            \n",
    "            for img_type, failures in failures_by_type.items():\n",
    "                print(f\"   • {img_type}: {len(failures)} failed images\")\n",
    "                for failure in failures:\n",
    "                    print(f\"     - {failure['image_id']} ({failure['error_type']})\")\n",
    "        \n",
    "        print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74831b4",
   "metadata": {
    "papermill": {
     "duration": 0.005386,
     "end_time": "2025-07-28T14:03:46.527093",
     "exception": false,
     "start_time": "2025-07-28T14:03:46.521707",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test SmolVLM Model\n",
    "\n",
    "Let's evaluate the SmolVLM2-2.2B-Instruct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7328a56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T14:03:46.539018Z",
     "iopub.status.busy": "2025-07-28T14:03:46.538778Z",
     "iopub.status.idle": "2025-07-28T14:03:46.543194Z",
     "shell.execute_reply": "2025-07-28T14:03:46.542441Z"
    },
    "papermill": {
     "duration": 0.011674,
     "end_time": "2025-07-28T14:03:46.544321",
     "exception": false,
     "start_time": "2025-07-28T14:03:46.532647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def test_smolVLM2():\n",
    "#     from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "\n",
    "#     print(\"Loading smolVLM model...\")\n",
    "    \n",
    "#     model = AutoModelForImageTextToText.from_pretrained(\n",
    "#         \"HuggingFaceTB/SmolVLM2-2.2B-Instruct\",\n",
    "#         torch_dtype=torch.float16,\n",
    "#         attn_implementation=\"flash_attention_2\",\n",
    "#         low_cpu_mem_usage=True,\n",
    "#         trust_remote_code=True\n",
    "#     ).to(\"cuda\")\n",
    "\n",
    "#     processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM2-2.2B-Instruct\")\n",
    "\n",
    "#     ## A bit slow without the flash_attention2 requires ampere gpu's. Better performance in some cases\n",
    "\n",
    "#     # Optional: Enable memory efficient attention\n",
    "#     if hasattr(model.config, 'use_memory_efficient_attention'):\n",
    "#         model.config.use_memory_efficient_attention = True\n",
    "\n",
    "#     tester = BenchmarkTester()\n",
    "#     smolVLM_results = tester.evaluate_model(\n",
    "#         \"smolVLM2\",\n",
    "#         model, \n",
    "#         processor, \n",
    "#         \"smolVLM2_results_1.json\", \n",
    "#         batch_size=25\n",
    "#     )\n",
    "\n",
    "#     # Clean up\n",
    "#     del model, processor\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e0ecbe",
   "metadata": {
    "papermill": {
     "duration": 0.005322,
     "end_time": "2025-07-28T14:03:46.555302",
     "exception": false,
     "start_time": "2025-07-28T14:03:46.549980",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test Qwen2.5-VL\n",
    "\n",
    "Lets evaluate the Qwen2.5-VL-7B-Instruct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35cb639d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T14:03:46.567333Z",
     "iopub.status.busy": "2025-07-28T14:03:46.567014Z",
     "iopub.status.idle": "2025-07-28T14:03:46.572763Z",
     "shell.execute_reply": "2025-07-28T14:03:46.572116Z"
    },
    "papermill": {
     "duration": 0.013072,
     "end_time": "2025-07-28T14:03:46.573924",
     "exception": false,
     "start_time": "2025-07-28T14:03:46.560852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_Qwen2_5VL():\n",
    "    from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "    \n",
    "    # default: Load the model on the available device(s)\n",
    "    # model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    #     \"Qwen/Qwen2.5-VL-3B-Instruct\", \n",
    "    #     load_in_8bit=True, # throws error when .to() is added\n",
    "    #     torch_dtype=torch.bfloat16, \n",
    "    #     device_map=\"auto\",\n",
    "    #     # attn_implementation=\"flash_attention_2\",\n",
    "    #     low_cpu_mem_usage=True\n",
    "    # )\n",
    "    \n",
    "    # We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "        \"/var/scratch/ave303/models/qwen2-5-vl-32b\",\n",
    "        torch_dtype=torch.float16,\n",
    "        # attn_implementation=\"flash_attention_2\",\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # default processer\n",
    "    processor = AutoProcessor.from_pretrained(\"/var/scratch/ave303/models/qwen2-5-vl-32b\")\n",
    "\n",
    "    ### Qwen2.5-VL-7B-Instruct --> goes out of CUDA memory\n",
    "    ### Qwen2.5-VL-3B-Instruct --> can handle only 2 images before going out of memory but decent performance\n",
    "\n",
    "    # Optional: Enable memory efficient attention\n",
    "    if hasattr(model.config, 'use_memory_efficient_attention'):\n",
    "        model.config.use_memory_efficient_attention = True\n",
    "\n",
    "    tester = BenchmarkTester()\n",
    "    Qwen2_5VL_results = tester.evaluate_model(\n",
    "        \"Qwen2.5-VL\",\n",
    "        model, \n",
    "        processor, \n",
    "        \"Qwen2.5-VL_32b_results.json\",\n",
    "        # start_idx=2,\n",
    "        batch_size=360\n",
    "    )\n",
    "\n",
    "    # Clean up\n",
    "    del model, processor\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dce02ce",
   "metadata": {
    "papermill": {
     "duration": 0.005366,
     "end_time": "2025-07-28T14:03:46.584903",
     "exception": false,
     "start_time": "2025-07-28T14:03:46.579537",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Run Evaluation\n",
    "\n",
    "Now we can run our evaluation. Let's start with the SmolVLM2 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ffdade4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T14:03:46.596891Z",
     "iopub.status.busy": "2025-07-28T14:03:46.596656Z",
     "iopub.status.idle": "2025-07-28T14:03:46.600160Z",
     "shell.execute_reply": "2025-07-28T14:03:46.599430Z"
    },
    "papermill": {
     "duration": 0.010811,
     "end_time": "2025-07-28T14:03:46.601346",
     "exception": false,
     "start_time": "2025-07-28T14:03:46.590535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_smolVLM2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba954a45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T14:03:46.613441Z",
     "iopub.status.busy": "2025-07-28T14:03:46.613130Z",
     "iopub.status.idle": "2025-07-28T20:21:14.585021Z",
     "shell.execute_reply": "2025-07-28T20:21:14.584142Z"
    },
    "papermill": {
     "duration": 22647.979257,
     "end_time": "2025-07-28T20:21:14.586371",
     "exception": false,
     "start_time": "2025-07-28T14:03:46.607114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:   3%|▎         | 1/30 [00:01<00:56,  1.94s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:   7%|▋         | 2/30 [00:04<01:03,  2.26s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  10%|█         | 3/30 [00:07<01:05,  2.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  13%|█▎        | 4/30 [00:09<01:03,  2.46s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  17%|█▋        | 5/30 [00:12<01:01,  2.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  20%|██        | 6/30 [00:14<01:01,  2.58s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  23%|██▎       | 7/30 [00:17<01:00,  2.63s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  27%|██▋       | 8/30 [00:20<00:57,  2.63s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  30%|███       | 9/30 [00:22<00:55,  2.64s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  33%|███▎      | 10/30 [00:25<00:52,  2.62s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  37%|███▋      | 11/30 [00:27<00:49,  2.58s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  40%|████      | 12/30 [00:30<00:47,  2.65s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  43%|████▎     | 13/30 [00:33<00:45,  2.71s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  47%|████▋     | 14/30 [00:36<00:44,  2.78s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  50%|█████     | 15/30 [00:39<00:42,  2.82s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  53%|█████▎    | 16/30 [00:42<00:39,  2.82s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  57%|█████▋    | 17/30 [00:44<00:35,  2.76s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  60%|██████    | 18/30 [00:47<00:34,  2.84s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  63%|██████▎   | 19/30 [00:50<00:31,  2.83s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  67%|██████▋   | 20/30 [00:53<00:27,  2.75s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  70%|███████   | 21/30 [00:55<00:23,  2.64s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  73%|███████▎  | 22/30 [00:58<00:20,  2.57s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  77%|███████▋  | 23/30 [01:00<00:17,  2.52s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  80%|████████  | 24/30 [01:03<00:15,  2.62s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  83%|████████▎ | 25/30 [01:05<00:13,  2.62s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  87%|████████▋ | 26/30 [01:08<00:10,  2.63s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  90%|█████████ | 27/30 [01:11<00:08,  2.72s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  93%|█████████▎| 28/30 [01:14<00:05,  2.75s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  97%|█████████▋| 29/30 [01:17<00:02,  2.82s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [01:19<00:00,  2.67s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [01:19<00:00,  2.66s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Qwen2.5-VL...\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   0%|          | 0/360 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   0%|          | 1/360 [01:06<6:39:28, 66.76s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   1%|          | 2/360 [02:29<7:34:45, 76.22s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   1%|          | 3/360 [03:41<7:20:40, 74.06s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   1%|          | 4/360 [05:05<7:43:54, 78.19s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   1%|▏         | 5/360 [06:31<7:58:51, 80.93s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   2%|▏         | 6/360 [08:23<8:59:13, 91.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   2%|▏         | 7/360 [09:07<7:27:13, 76.02s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   2%|▏         | 8/360 [09:39<6:03:30, 61.96s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   2%|▎         | 9/360 [10:57<6:32:25, 67.08s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   3%|▎         | 10/360 [12:48<7:49:54, 80.55s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   3%|▎         | 11/360 [13:38<6:53:54, 71.16s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   3%|▎         | 12/360 [15:28<8:01:43, 83.05s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   4%|▎         | 13/360 [16:32<7:26:03, 77.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   4%|▍         | 14/360 [17:01<6:01:21, 62.66s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   4%|▍         | 15/360 [18:29<6:45:11, 70.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   4%|▍         | 16/360 [20:03<7:24:53, 77.60s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   5%|▍         | 17/360 [21:00<6:47:36, 71.30s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   5%|▌         | 18/360 [22:06<6:37:05, 69.66s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   5%|▌         | 19/360 [23:33<7:05:06, 74.80s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   6%|▌         | 20/360 [24:31<6:35:20, 69.77s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   6%|▌         | 21/360 [26:01<7:09:28, 76.01s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   6%|▌         | 22/360 [26:06<5:07:23, 54.57s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   6%|▋         | 23/360 [26:11<3:42:56, 39.69s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   7%|▋         | 24/360 [27:28<4:45:28, 50.98s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   7%|▋         | 25/360 [28:23<4:51:30, 52.21s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   7%|▋         | 26/360 [29:56<5:58:51, 64.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   8%|▊         | 27/360 [30:45<5:31:31, 59.73s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   8%|▊         | 28/360 [31:51<5:40:29, 61.54s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   8%|▊         | 29/360 [33:23<6:29:34, 70.62s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   8%|▊         | 30/360 [34:28<6:20:36, 69.20s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   9%|▊         | 31/360 [35:52<6:43:12, 73.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   9%|▉         | 32/360 [37:11<6:51:01, 75.19s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   9%|▉         | 33/360 [38:04<6:13:43, 68.57s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:   9%|▉         | 34/360 [39:08<6:03:57, 66.99s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  10%|▉         | 35/360 [41:55<8:45:24, 97.00s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  10%|█         | 36/360 [44:10<9:46:12, 108.56s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  10%|█         | 37/360 [45:31<8:59:41, 100.25s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  11%|█         | 38/360 [46:30<7:51:20, 87.83s/it] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  11%|█         | 39/360 [47:34<7:12:22, 80.82s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  11%|█         | 40/360 [49:00<7:18:45, 82.27s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  11%|█▏        | 41/360 [49:44<6:16:16, 70.77s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  12%|█▏        | 42/360 [50:26<5:29:50, 62.23s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  12%|█▏        | 43/360 [51:22<5:18:53, 60.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  12%|█▏        | 44/360 [52:35<5:37:53, 64.16s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  12%|█▎        | 45/360 [54:34<7:02:32, 80.48s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  13%|█▎        | 46/360 [56:40<8:13:10, 94.24s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  13%|█▎        | 47/360 [57:37<7:13:34, 83.11s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  13%|█▎        | 48/360 [58:25<6:16:33, 72.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  14%|█▎        | 49/360 [59:32<6:07:33, 70.91s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  14%|█▍        | 50/360 [1:00:43<6:06:52, 71.01s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  14%|█▍        | 51/360 [1:02:37<7:11:49, 83.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  14%|█▍        | 52/360 [1:03:30<6:22:10, 74.45s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  15%|█▍        | 53/360 [1:04:38<6:11:38, 72.64s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  15%|█▌        | 54/360 [1:04:42<4:25:03, 51.97s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  15%|█▌        | 55/360 [1:05:35<4:26:02, 52.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  16%|█▌        | 56/360 [1:05:39<3:11:54, 37.88s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  16%|█▌        | 57/360 [1:07:25<4:54:21, 58.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  16%|█▌        | 58/360 [1:08:26<4:56:37, 58.93s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  16%|█▋        | 59/360 [1:09:22<4:51:33, 58.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  17%|█▋        | 60/360 [1:11:05<5:57:41, 71.54s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  17%|█▋        | 61/360 [1:12:21<6:03:46, 73.00s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  17%|█▋        | 62/360 [1:13:07<5:22:03, 64.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  18%|█▊        | 63/360 [1:14:35<5:55:20, 71.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  18%|█▊        | 64/360 [1:14:50<4:29:38, 54.66s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  18%|█▊        | 65/360 [1:15:54<4:43:07, 57.59s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  18%|█▊        | 66/360 [1:16:42<4:27:40, 54.63s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  19%|█▊        | 67/360 [1:17:59<4:59:19, 61.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  19%|█▉        | 68/360 [1:19:17<5:23:18, 66.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  19%|█▉        | 69/360 [1:20:58<6:13:00, 76.91s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  19%|█▉        | 70/360 [1:21:59<5:47:51, 71.97s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  20%|█▉        | 71/360 [1:22:13<4:23:44, 54.75s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  20%|██        | 72/360 [1:23:14<4:31:00, 56.46s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  20%|██        | 73/360 [1:23:54<4:07:15, 51.69s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  21%|██        | 74/360 [1:25:04<4:31:38, 56.99s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  21%|██        | 75/360 [1:25:08<3:14:58, 41.05s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  21%|██        | 76/360 [1:26:10<3:44:35, 47.45s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  21%|██▏       | 77/360 [1:26:49<3:31:37, 44.87s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  22%|██▏       | 78/360 [1:28:33<4:54:01, 62.56s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  22%|██▏       | 79/360 [1:30:27<6:05:51, 78.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  22%|██▏       | 80/360 [1:31:25<5:36:32, 72.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  22%|██▎       | 81/360 [1:32:07<4:53:34, 63.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  23%|██▎       | 82/360 [1:33:14<4:57:27, 64.20s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  23%|██▎       | 83/360 [1:34:12<4:48:12, 62.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  23%|██▎       | 84/360 [1:34:15<3:25:23, 44.65s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  24%|██▎       | 85/360 [1:34:33<2:46:43, 36.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  24%|██▍       | 86/360 [1:35:19<3:00:13, 39.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  24%|██▍       | 87/360 [1:36:41<3:57:42, 52.24s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  24%|██▍       | 88/360 [1:37:55<4:25:43, 58.61s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  25%|██▍       | 89/360 [1:38:09<3:25:05, 45.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  25%|██▌       | 90/360 [1:38:48<3:14:51, 43.30s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  25%|██▌       | 91/360 [1:40:34<4:39:07, 62.26s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  26%|██▌       | 92/360 [1:40:38<3:19:51, 44.74s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  26%|██▌       | 93/360 [1:41:37<3:37:31, 48.88s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  26%|██▌       | 94/360 [1:42:41<3:56:52, 53.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  26%|██▋       | 95/360 [1:43:36<3:57:58, 53.88s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  27%|██▋       | 96/360 [1:45:02<4:40:22, 63.72s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  27%|██▋       | 97/360 [1:45:17<3:34:45, 48.99s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  27%|██▋       | 98/360 [1:46:32<4:07:40, 56.72s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  28%|██▊       | 99/360 [1:46:35<2:56:45, 40.63s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  28%|██▊       | 100/360 [1:48:11<4:08:55, 57.45s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  28%|██▊       | 101/360 [1:50:02<5:16:50, 73.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  28%|██▊       | 102/360 [1:51:19<5:20:46, 74.60s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  29%|██▊       | 103/360 [1:52:45<5:33:40, 77.90s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  29%|██▉       | 104/360 [1:54:00<5:28:26, 76.98s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  29%|██▉       | 105/360 [1:55:27<5:40:39, 80.15s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  29%|██▉       | 106/360 [1:57:07<6:04:32, 86.11s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  30%|██▉       | 107/360 [1:58:31<6:00:12, 85.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  30%|███       | 108/360 [1:58:46<4:29:34, 64.18s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  30%|███       | 109/360 [1:58:48<3:10:49, 45.62s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  31%|███       | 110/360 [1:58:51<2:15:57, 32.63s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  31%|███       | 111/360 [1:58:53<1:38:25, 23.72s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  31%|███       | 112/360 [2:00:40<3:20:56, 48.61s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  31%|███▏      | 113/360 [2:02:53<5:04:22, 73.94s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  32%|███▏      | 114/360 [2:04:53<5:59:14, 87.62s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  32%|███▏      | 115/360 [2:06:07<5:41:56, 83.74s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  32%|███▏      | 116/360 [2:06:22<4:16:16, 63.02s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  32%|███▎      | 117/360 [2:07:31<4:21:58, 64.69s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  33%|███▎      | 118/360 [2:09:31<5:27:39, 81.24s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  33%|███▎      | 119/360 [2:10:46<5:19:41, 79.59s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  33%|███▎      | 120/360 [2:12:44<6:04:15, 91.06s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  34%|███▎      | 121/360 [2:13:52<5:35:04, 84.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  34%|███▍      | 122/360 [2:14:54<5:07:55, 77.63s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  34%|███▍      | 123/360 [2:15:37<4:25:21, 67.18s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  34%|███▍      | 124/360 [2:16:52<4:33:16, 69.48s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  35%|███▍      | 125/360 [2:18:02<4:32:04, 69.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  35%|███▌      | 126/360 [2:18:57<4:14:30, 65.26s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  35%|███▌      | 127/360 [2:19:36<3:43:08, 57.46s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  36%|███▌      | 128/360 [2:20:32<3:40:15, 56.96s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  36%|███▌      | 129/360 [2:22:03<4:18:04, 67.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  36%|███▌      | 130/360 [2:22:07<3:04:40, 48.17s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  36%|███▋      | 131/360 [2:23:44<4:00:34, 63.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  37%|███▋      | 132/360 [2:26:12<5:35:59, 88.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  37%|███▋      | 133/360 [2:26:51<4:38:00, 73.48s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  37%|███▋      | 134/360 [2:28:05<4:37:34, 73.69s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  38%|███▊      | 135/360 [2:28:07<3:16:17, 52.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  38%|███▊      | 136/360 [2:28:10<2:19:41, 37.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  38%|███▊      | 137/360 [2:29:26<3:01:41, 48.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  38%|███▊      | 138/360 [2:30:27<3:15:06, 52.73s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  39%|███▊      | 139/360 [2:32:19<4:19:04, 70.34s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  39%|███▉      | 140/360 [2:33:28<4:16:48, 70.04s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  39%|███▉      | 141/360 [2:33:32<3:03:10, 50.18s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  39%|███▉      | 142/360 [2:35:06<3:49:41, 63.22s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  40%|███▉      | 143/360 [2:35:10<2:44:29, 45.48s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  40%|████      | 144/360 [2:36:03<2:51:34, 47.66s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  40%|████      | 145/360 [2:37:00<3:00:50, 50.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  41%|████      | 146/360 [2:38:24<3:36:04, 60.58s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  41%|████      | 147/360 [2:39:43<3:54:29, 66.05s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  41%|████      | 148/360 [2:41:37<4:44:52, 80.62s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  41%|████▏     | 149/360 [2:42:39<4:24:08, 75.11s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  42%|████▏     | 150/360 [2:43:23<3:49:30, 65.57s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  42%|████▏     | 151/360 [2:44:43<4:04:04, 70.07s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  42%|████▏     | 152/360 [2:45:13<3:20:51, 57.94s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  42%|████▎     | 153/360 [2:45:48<2:56:24, 51.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  43%|████▎     | 154/360 [2:46:29<2:44:48, 48.00s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  43%|████▎     | 155/360 [2:47:40<3:08:05, 55.05s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  43%|████▎     | 156/360 [2:48:43<3:14:38, 57.25s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  44%|████▎     | 157/360 [2:49:50<3:24:14, 60.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  44%|████▍     | 158/360 [2:51:08<3:40:43, 65.56s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  44%|████▍     | 159/360 [2:52:21<3:46:57, 67.75s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  44%|████▍     | 160/360 [2:53:12<3:28:59, 62.70s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  45%|████▍     | 161/360 [2:53:16<2:29:20, 45.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  45%|████▌     | 162/360 [2:53:18<1:46:39, 32.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  45%|████▌     | 163/360 [2:53:22<1:18:06, 23.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  46%|████▌     | 164/360 [2:53:27<59:33, 18.23s/it]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  46%|████▌     | 165/360 [2:53:31<45:20, 13.95s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  46%|████▌     | 166/360 [2:53:36<35:39, 11.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  46%|████▋     | 167/360 [2:53:38<27:28,  8.54s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  47%|████▋     | 168/360 [2:53:41<21:29,  6.71s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  47%|████▋     | 169/360 [2:53:44<17:42,  5.56s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  47%|████▋     | 170/360 [2:55:19<1:42:51, 32.48s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  48%|████▊     | 171/360 [2:55:23<1:15:23, 23.93s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  48%|████▊     | 172/360 [2:55:27<56:18, 17.97s/it]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  48%|████▊     | 173/360 [2:55:31<42:52, 13.76s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  48%|████▊     | 174/360 [2:56:35<1:28:59, 28.71s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  49%|████▊     | 175/360 [2:57:12<1:36:28, 31.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  49%|████▉     | 176/360 [2:57:44<1:36:36, 31.50s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  49%|████▉     | 177/360 [2:58:16<1:36:35, 31.67s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  49%|████▉     | 178/360 [2:58:20<1:10:46, 23.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  50%|████▉     | 179/360 [2:58:24<52:43, 17.48s/it]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  50%|█████     | 180/360 [2:59:06<1:15:12, 25.07s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  50%|█████     | 181/360 [2:59:40<1:22:28, 27.64s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  51%|█████     | 182/360 [3:01:17<2:23:28, 48.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  51%|█████     | 183/360 [3:03:05<3:15:40, 66.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  51%|█████     | 184/360 [3:03:49<2:55:10, 59.72s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  51%|█████▏    | 185/360 [3:04:48<2:53:43, 59.56s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  52%|█████▏    | 186/360 [3:05:32<2:39:01, 54.84s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  52%|█████▏    | 187/360 [3:06:23<2:34:43, 53.66s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  52%|█████▏    | 188/360 [3:07:30<2:44:50, 57.50s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  52%|█████▎    | 189/360 [3:08:45<2:58:57, 62.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  53%|█████▎    | 190/360 [3:09:27<2:40:07, 56.51s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  53%|█████▎    | 191/360 [3:10:12<2:29:35, 53.11s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  53%|█████▎    | 192/360 [3:11:35<2:53:33, 61.98s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  54%|█████▎    | 193/360 [3:12:48<3:01:57, 65.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  54%|█████▍    | 194/360 [3:13:38<2:48:06, 60.76s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  54%|█████▍    | 195/360 [3:14:35<2:44:12, 59.71s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  54%|█████▍    | 196/360 [3:16:06<3:08:36, 69.00s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  55%|█████▍    | 197/360 [3:17:44<3:31:36, 77.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  55%|█████▌    | 198/360 [3:18:54<3:23:18, 75.30s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  55%|█████▌    | 199/360 [3:19:49<3:05:41, 69.20s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  56%|█████▌    | 200/360 [3:21:35<3:34:23, 80.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  56%|█████▌    | 201/360 [3:22:54<3:31:41, 79.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  56%|█████▌    | 202/360 [3:23:49<3:10:41, 72.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  56%|█████▋    | 203/360 [3:24:40<2:52:52, 66.07s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  57%|█████▋    | 204/360 [3:24:44<2:03:32, 47.52s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  57%|█████▋    | 205/360 [3:25:20<1:53:55, 44.10s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  57%|█████▋    | 206/360 [3:26:17<2:03:06, 47.97s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  57%|█████▊    | 207/360 [3:27:13<2:08:24, 50.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  58%|█████▊    | 208/360 [3:28:05<2:08:52, 50.87s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  58%|█████▊    | 209/360 [3:28:55<2:07:10, 50.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  58%|█████▊    | 210/360 [3:29:27<1:52:36, 45.04s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  59%|█████▊    | 211/360 [3:30:27<2:02:23, 49.28s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  59%|█████▉    | 212/360 [3:31:38<2:18:10, 56.02s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  59%|█████▉    | 213/360 [3:32:57<2:34:04, 62.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  59%|█████▉    | 214/360 [3:33:53<2:27:40, 60.69s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  60%|█████▉    | 215/360 [3:34:32<2:11:14, 54.30s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  60%|██████    | 216/360 [3:35:27<2:10:19, 54.30s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  60%|██████    | 217/360 [3:36:13<2:03:38, 51.88s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  61%|██████    | 218/360 [3:36:57<1:57:32, 49.67s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  61%|██████    | 219/360 [3:38:03<2:08:14, 54.57s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  61%|██████    | 220/360 [3:38:50<2:01:40, 52.15s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  61%|██████▏   | 221/360 [3:39:26<1:49:38, 47.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  62%|██████▏   | 222/360 [3:40:47<2:12:29, 57.61s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  62%|██████▏   | 223/360 [3:42:21<2:36:25, 68.51s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  62%|██████▏   | 224/360 [3:43:13<2:23:55, 63.50s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  62%|██████▎   | 225/360 [3:44:11<2:18:43, 61.65s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  63%|██████▎   | 226/360 [3:45:49<2:42:06, 72.59s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  63%|██████▎   | 227/360 [3:46:31<2:20:56, 63.58s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  63%|██████▎   | 228/360 [3:46:58<1:55:18, 52.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  64%|██████▎   | 229/360 [3:47:59<2:00:20, 55.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  64%|██████▍   | 230/360 [3:48:03<1:26:03, 39.72s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  64%|██████▍   | 231/360 [3:48:07<1:02:19, 28.99s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  64%|██████▍   | 232/360 [3:48:11<46:20, 21.73s/it]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  65%|██████▍   | 233/360 [3:48:15<34:41, 16.39s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  65%|██████▌   | 234/360 [3:48:18<25:38, 12.21s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  65%|██████▌   | 235/360 [3:48:22<20:16,  9.73s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  66%|██████▌   | 236/360 [3:48:26<16:37,  8.04s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  66%|██████▌   | 237/360 [3:48:30<13:54,  6.79s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  66%|██████▌   | 238/360 [3:48:34<12:03,  5.93s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  66%|██████▋   | 239/360 [3:48:38<10:44,  5.33s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  67%|██████▋   | 240/360 [3:48:41<09:46,  4.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  67%|██████▋   | 241/360 [3:49:41<42:14, 21.30s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  67%|██████▋   | 242/360 [3:50:40<1:04:06, 32.60s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  68%|██████▊   | 243/360 [3:51:50<1:25:19, 43.76s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  68%|██████▊   | 244/360 [3:53:13<1:47:38, 55.68s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  68%|██████▊   | 245/360 [3:54:31<1:59:12, 62.19s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  68%|██████▊   | 246/360 [3:55:43<2:03:46, 65.15s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  69%|██████▊   | 247/360 [3:56:39<1:57:40, 62.48s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  69%|██████▉   | 248/360 [3:58:33<2:25:20, 77.86s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  69%|██████▉   | 249/360 [3:59:29<2:11:51, 71.28s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  69%|██████▉   | 250/360 [4:00:55<2:19:08, 75.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  70%|██████▉   | 251/360 [4:02:36<2:31:27, 83.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  70%|███████   | 252/360 [4:03:21<2:08:59, 71.66s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  70%|███████   | 253/360 [4:04:17<1:59:32, 67.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  71%|███████   | 254/360 [4:05:05<1:48:24, 61.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  71%|███████   | 255/360 [4:06:35<2:02:26, 69.96s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  71%|███████   | 256/360 [4:07:47<2:02:21, 70.59s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  71%|███████▏  | 257/360 [4:08:39<1:51:42, 65.07s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  72%|███████▏  | 258/360 [4:10:45<2:21:47, 83.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  72%|███████▏  | 259/360 [4:12:04<2:18:06, 82.04s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  72%|███████▏  | 260/360 [4:13:05<2:06:01, 75.61s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  72%|███████▎  | 261/360 [4:13:46<1:47:36, 65.22s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  73%|███████▎  | 262/360 [4:14:59<1:50:30, 67.66s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  73%|███████▎  | 263/360 [4:16:09<1:50:36, 68.42s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  73%|███████▎  | 264/360 [4:17:23<1:51:51, 69.92s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  74%|███████▎  | 265/360 [4:18:13<1:41:08, 63.88s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  74%|███████▍  | 266/360 [4:19:21<1:42:26, 65.39s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  74%|███████▍  | 267/360 [4:20:20<1:38:01, 63.24s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  74%|███████▍  | 268/360 [4:21:08<1:29:58, 58.68s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  75%|███████▍  | 269/360 [4:21:44<1:18:42, 51.90s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  75%|███████▌  | 270/360 [4:22:47<1:22:51, 55.23s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  75%|███████▌  | 271/360 [4:23:59<1:29:25, 60.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  76%|███████▌  | 272/360 [4:24:51<1:24:51, 57.86s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  76%|███████▌  | 273/360 [4:25:57<1:27:31, 60.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  76%|███████▌  | 274/360 [4:27:36<1:42:55, 71.81s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  76%|███████▋  | 275/360 [4:28:48<1:42:02, 72.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  77%|███████▋  | 276/360 [4:30:02<1:41:24, 72.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  77%|███████▋  | 277/360 [4:30:46<1:28:37, 64.07s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  77%|███████▋  | 278/360 [4:31:45<1:25:28, 62.54s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  78%|███████▊  | 279/360 [4:32:38<1:20:33, 59.67s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  78%|███████▊  | 280/360 [4:33:41<1:20:38, 60.48s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  78%|███████▊  | 281/360 [4:34:43<1:20:28, 61.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  78%|███████▊  | 282/360 [4:35:57<1:24:15, 64.81s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  79%|███████▊  | 283/360 [4:37:08<1:25:30, 66.63s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  79%|███████▉  | 284/360 [4:38:27<1:29:25, 70.60s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  79%|███████▉  | 285/360 [4:39:23<1:22:31, 66.01s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  79%|███████▉  | 286/360 [4:41:16<1:38:50, 80.14s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  80%|███████▉  | 287/360 [4:42:18<1:31:04, 74.85s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  80%|████████  | 288/360 [4:43:44<1:33:51, 78.21s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  80%|████████  | 289/360 [4:45:04<1:33:03, 78.64s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  81%|████████  | 290/360 [4:46:51<1:41:50, 87.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  81%|████████  | 291/360 [4:48:33<1:45:08, 91.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  81%|████████  | 292/360 [4:50:42<1:56:38, 102.92s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  81%|████████▏ | 293/360 [4:52:22<1:53:47, 101.90s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  82%|████████▏ | 294/360 [4:54:45<2:05:53, 114.44s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  82%|████████▏ | 295/360 [4:56:14<1:55:34, 106.68s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  82%|████████▏ | 296/360 [4:57:07<1:36:30, 90.47s/it] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  82%|████████▎ | 297/360 [4:58:10<1:26:33, 82.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  83%|████████▎ | 298/360 [5:00:26<1:41:47, 98.50s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  83%|████████▎ | 299/360 [5:01:33<1:30:27, 88.97s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  83%|████████▎ | 300/360 [5:02:54<1:26:31, 86.53s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  84%|████████▎ | 301/360 [5:04:34<1:29:13, 90.73s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  84%|████████▍ | 302/360 [5:06:51<1:40:59, 104.47s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  84%|████████▍ | 303/360 [5:08:23<1:35:43, 100.77s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  84%|████████▍ | 304/360 [5:09:32<1:25:06, 91.19s/it] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  85%|████████▍ | 305/360 [5:10:37<1:16:29, 83.45s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  85%|████████▌ | 306/360 [5:12:01<1:15:10, 83.52s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  85%|████████▌ | 307/360 [5:13:19<1:12:22, 81.94s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  86%|████████▌ | 308/360 [5:14:32<1:08:34, 79.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  86%|████████▌ | 309/360 [5:16:14<1:13:04, 85.97s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  86%|████████▌ | 310/360 [5:18:21<1:21:51, 98.23s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  86%|████████▋ | 311/360 [5:19:02<1:06:10, 81.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  87%|████████▋ | 312/360 [5:19:48<56:30, 70.63s/it]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  87%|████████▋ | 313/360 [5:21:32<1:03:09, 80.64s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  87%|████████▋ | 314/360 [5:22:20<54:18, 70.84s/it]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  88%|████████▊ | 315/360 [5:23:04<47:11, 62.93s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  88%|████████▊ | 316/360 [5:23:43<40:54, 55.78s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  88%|████████▊ | 317/360 [5:25:38<52:30, 73.26s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  88%|████████▊ | 318/360 [5:27:00<53:08, 75.92s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  89%|████████▊ | 319/360 [5:28:07<50:05, 73.31s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  89%|████████▉ | 320/360 [5:29:50<54:51, 82.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  89%|████████▉ | 321/360 [5:30:54<49:59, 76.91s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  89%|████████▉ | 322/360 [5:31:55<45:34, 71.97s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  90%|████████▉ | 323/360 [5:33:10<44:52, 72.78s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  90%|█████████ | 324/360 [5:34:41<46:58, 78.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  90%|█████████ | 325/360 [5:35:17<38:21, 65.75s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  91%|█████████ | 326/360 [5:36:24<37:30, 66.18s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  91%|█████████ | 327/360 [5:37:33<36:52, 67.03s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  91%|█████████ | 328/360 [5:38:25<33:16, 62.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  91%|█████████▏| 329/360 [5:39:11<29:40, 57.45s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  92%|█████████▏| 330/360 [5:40:29<31:52, 63.76s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  92%|█████████▏| 331/360 [5:41:40<31:47, 65.78s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  92%|█████████▏| 332/360 [5:42:48<30:59, 66.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  92%|█████████▎| 333/360 [5:43:44<28:28, 63.26s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  93%|█████████▎| 334/360 [5:45:07<30:03, 69.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  93%|█████████▎| 335/360 [5:46:38<31:36, 75.84s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  93%|█████████▎| 336/360 [5:47:45<29:17, 73.23s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  94%|█████████▎| 337/360 [5:48:55<27:39, 72.17s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  94%|█████████▍| 338/360 [5:50:07<26:29, 72.23s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  94%|█████████▍| 339/360 [5:51:37<27:05, 77.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  94%|█████████▍| 340/360 [5:53:02<26:33, 79.69s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  95%|█████████▍| 341/360 [5:54:31<26:05, 82.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  95%|█████████▌| 342/360 [5:55:35<23:04, 76.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  95%|█████████▌| 343/360 [5:56:27<19:42, 69.58s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  96%|█████████▌| 344/360 [5:57:48<19:26, 72.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  96%|█████████▌| 345/360 [5:58:41<16:42, 66.82s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  96%|█████████▌| 346/360 [5:59:43<15:15, 65.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  96%|█████████▋| 347/360 [6:01:11<15:41, 72.39s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  97%|█████████▋| 348/360 [6:02:03<13:12, 66.06s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  97%|█████████▋| 349/360 [6:03:18<12:37, 68.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  97%|█████████▋| 350/360 [6:04:42<12:13, 73.37s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  98%|█████████▊| 351/360 [6:05:24<09:36, 64.09s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  98%|█████████▊| 352/360 [6:06:09<07:47, 58.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  98%|█████████▊| 353/360 [6:07:05<06:43, 57.61s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  98%|█████████▊| 354/360 [6:08:09<05:57, 59.52s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  99%|█████████▊| 355/360 [6:09:23<05:19, 63.89s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  99%|█████████▉| 356/360 [6:10:20<04:06, 61.73s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  99%|█████████▉| 357/360 [6:11:10<02:54, 58.25s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images:  99%|█████████▉| 358/360 [6:12:35<02:12, 66.38s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images: 100%|█████████▉| 359/360 [6:14:26<01:19, 79.57s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images: 100%|██████████| 360/360 [6:16:01<00:00, 84.36s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Processing images: 100%|██████████| 360/360 [6:16:01<00:00, 62.67s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATION SUMMARY FOR QWEN2.5-VL\n",
      "============================================================\n",
      "📊 IMAGE PROCESSING SUMMARY:\n",
      "   Total images attempted: 360\n",
      "   Successfully processed: 309 (85.8%)\n",
      "   Failed images: 51 (14.2%)\n",
      "\n",
      "📝 QUESTION PROCESSING SUMMARY:\n",
      "   Total questions attempted: 1080\n",
      "   Successfully processed: 927 (85.8%)\n",
      "   Failed questions: 153 (14.2%)\n",
      "   Results saved: 927\n",
      "\n",
      "✅ SUCCESSFUL IMAGES (309):\n",
      "   • image01 (Type: REAL, Questions: 3/3, Time: 66.8s)\n",
      "   • image02 (Type: REAL, Questions: 3/3, Time: 82.8s)\n",
      "   • image03 (Type: REAL, Questions: 3/3, Time: 71.5s)\n",
      "   • image04 (Type: REAL, Questions: 3/3, Time: 84.5s)\n",
      "   • image05 (Type: REAL, Questions: 3/3, Time: 85.8s)\n",
      "   • image06 (Type: REAL, Questions: 3/3, Time: 111.7s)\n",
      "   • image07 (Type: REAL, Questions: 3/3, Time: 44.4s)\n",
      "   • image08 (Type: REAL, Questions: 3/3, Time: 31.9s)\n",
      "   • image09 (Type: REAL, Questions: 3/3, Time: 78.3s)\n",
      "   • image10 (Type: REAL, Questions: 3/3, Time: 110.7s)\n",
      "   • image11 (Type: REAL, Questions: 3/3, Time: 49.9s)\n",
      "   • image12 (Type: REAL, Questions: 3/3, Time: 110.3s)\n",
      "   • image13 (Type: REAL, Questions: 3/3, Time: 63.5s)\n",
      "   • image14 (Type: REAL, Questions: 3/3, Time: 29.2s)\n",
      "   • image15 (Type: REAL, Questions: 3/3, Time: 88.6s)\n",
      "   • image16 (Type: REAL, Questions: 3/3, Time: 94.2s)\n",
      "   • image17 (Type: REAL, Questions: 3/3, Time: 56.7s)\n",
      "   • image18 (Type: REAL, Questions: 3/3, Time: 65.9s)\n",
      "   • image19 (Type: REAL, Questions: 3/3, Time: 86.8s)\n",
      "   • image20 (Type: REAL, Questions: 3/3, Time: 58.0s)\n",
      "   • image21 (Type: REAL, Questions: 3/3, Time: 90.6s)\n",
      "   • image24 (Type: REAL, Questions: 3/3, Time: 77.3s)\n",
      "   • image25 (Type: REAL, Questions: 3/3, Time: 55.1s)\n",
      "   • image26 (Type: REAL, Questions: 3/3, Time: 93.1s)\n",
      "   • image27 (Type: REAL, Questions: 3/3, Time: 48.7s)\n",
      "   • image28 (Type: REAL, Questions: 3/3, Time: 65.7s)\n",
      "   • image29 (Type: REAL, Questions: 3/3, Time: 91.8s)\n",
      "   • image30 (Type: REAL, Questions: 3/3, Time: 65.9s)\n",
      "   • image31 (Type: REAL, Questions: 3/3, Time: 83.6s)\n",
      "   • image32 (Type: REAL, Questions: 3/3, Time: 79.0s)\n",
      "   • image33 (Type: REAL, Questions: 3/3, Time: 53.1s)\n",
      "   • image34 (Type: REAL, Questions: 3/3, Time: 63.3s)\n",
      "   • image35 (Type: REAL, Questions: 3/3, Time: 167.0s)\n",
      "   • image36 (Type: REAL, Questions: 3/3, Time: 135.5s)\n",
      "   • image37 (Type: REAL, Questions: 3/3, Time: 80.9s)\n",
      "   • image38 (Type: REAL, Questions: 3/3, Time: 58.8s)\n",
      "   • image39 (Type: REAL, Questions: 3/3, Time: 64.5s)\n",
      "   • image40 (Type: REAL, Questions: 3/3, Time: 85.6s)\n",
      "   • image41 (Type: REAL, Questions: 3/3, Time: 44.0s)\n",
      "   • image42 (Type: REAL, Questions: 3/3, Time: 42.3s)\n",
      "   • image43 (Type: REAL, Questions: 3/3, Time: 56.0s)\n",
      "   • image44 (Type: REAL, Questions: 3/3, Time: 73.0s)\n",
      "   • image45 (Type: REAL, Questions: 3/3, Time: 118.6s)\n",
      "   • image46 (Type: REAL, Questions: 3/3, Time: 126.3s)\n",
      "   • image47 (Type: REAL, Questions: 3/3, Time: 57.1s)\n",
      "   • image48 (Type: REAL, Questions: 3/3, Time: 47.5s)\n",
      "   • image49 (Type: REAL, Questions: 3/3, Time: 67.4s)\n",
      "   • image50 (Type: REAL, Questions: 3/3, Time: 71.2s)\n",
      "   • image51 (Type: REAL, Questions: 3/3, Time: 113.8s)\n",
      "   • image52 (Type: REAL, Questions: 3/3, Time: 52.5s)\n",
      "   • image53 (Type: REAL, Questions: 3/3, Time: 68.4s)\n",
      "   • image55 (Type: REAL, Questions: 3/3, Time: 53.2s)\n",
      "   • image57 (Type: REAL, Questions: 3/3, Time: 105.9s)\n",
      "   • image58 (Type: REAL, Questions: 3/3, Time: 60.4s)\n",
      "   • image59 (Type: REAL, Questions: 3/3, Time: 56.2s)\n",
      "   • image60 (Type: REAL, Questions: 3/3, Time: 102.8s)\n",
      "   • image61 (Type: REAL, Questions: 3/3, Time: 76.4s)\n",
      "   • image62 (Type: REAL, Questions: 3/3, Time: 45.8s)\n",
      "   • image63 (Type: REAL, Questions: 3/3, Time: 88.0s)\n",
      "   • image65 (Type: REAL, Questions: 3/3, Time: 64.4s)\n",
      "   • image66 (Type: REAL, Questions: 3/3, Time: 47.7s)\n",
      "   • image67 (Type: REAL, Questions: 3/3, Time: 76.8s)\n",
      "   • image68 (Type: REAL, Questions: 3/3, Time: 78.4s)\n",
      "   • image69 (Type: REAL, Questions: 3/3, Time: 101.3s)\n",
      "   • image70 (Type: REAL, Questions: 3/3, Time: 60.4s)\n",
      "   • image72 (Type: REAL, Questions: 3/3, Time: 60.4s)\n",
      "   • image73 (Type: REAL, Questions: 3/3, Time: 40.6s)\n",
      "   • image74 (Type: REAL, Questions: 3/3, Time: 69.3s)\n",
      "   • image76 (Type: REAL, Questions: 3/3, Time: 62.4s)\n",
      "   • image77 (Type: REAL, Questions: 3/3, Time: 38.8s)\n",
      "   • image78 (Type: REAL, Questions: 3/3, Time: 103.8s)\n",
      "   • image79 (Type: REAL, Questions: 3/3, Time: 114.4s)\n",
      "   • image80 (Type: REAL, Questions: 3/3, Time: 58.1s)\n",
      "   • image81 (Type: REAL, Questions: 3/3, Time: 42.2s)\n",
      "   • image82 (Type: REAL, Questions: 3/3, Time: 66.7s)\n",
      "   • image83 (Type: REAL, Questions: 3/3, Time: 58.3s)\n",
      "   • image86 (Type: REAL, Questions: 3/3, Time: 46.7s)\n",
      "   • image87 (Type: REAL, Questions: 3/3, Time: 82.1s)\n",
      "   • image88 (Type: REAL, Questions: 3/3, Time: 73.5s)\n",
      "   • image91 (Type: REAL, Questions: 3/3, Time: 106.5s)\n",
      "   • image93 (Type: REAL, Questions: 3/3, Time: 58.5s)\n",
      "   • image94 (Type: REAL, Questions: 3/3, Time: 64.0s)\n",
      "   • image95 (Type: REAL, Questions: 3/3, Time: 54.9s)\n",
      "   • image96 (Type: REAL, Questions: 3/3, Time: 86.7s)\n",
      "   • image98 (Type: REAL, Questions: 3/3, Time: 74.7s)\n",
      "   • image100 (Type: REAL, Questions: 3/3, Time: 96.7s)\n",
      "   • image101 (Type: REAL, Questions: 3/3, Time: 110.6s)\n",
      "   • image102 (Type: REAL, Questions: 3/3, Time: 77.4s)\n",
      "   • image103 (Type: REAL, Questions: 3/3, Time: 85.6s)\n",
      "   • image104 (Type: REAL, Questions: 3/3, Time: 74.8s)\n",
      "   • image105 (Type: REAL, Questions: 3/3, Time: 87.6s)\n",
      "   • image106 (Type: REAL, Questions: 3/3, Time: 100.0s)\n",
      "   • image107 (Type: REAL, Questions: 3/3, Time: 83.8s)\n",
      "   • image112 (Type: REAL, Questions: 3/3, Time: 106.7s)\n",
      "   • image113 (Type: REAL, Questions: 3/3, Time: 133.0s)\n",
      "   • image114 (Type: REAL, Questions: 3/3, Time: 119.5s)\n",
      "   • image115 (Type: REAL, Questions: 3/3, Time: 74.7s)\n",
      "   • image117 (Type: REAL, Questions: 3/3, Time: 68.6s)\n",
      "   • image118 (Type: REAL, Questions: 3/3, Time: 119.9s)\n",
      "   • image119 (Type: REAL, Questions: 3/3, Time: 75.7s)\n",
      "   • image120 (Type: REAL, Questions: 3/3, Time: 117.8s)\n",
      "   • image01 (Type: ANIMATED, Questions: 3/3, Time: 67.9s)\n",
      "   • image02 (Type: ANIMATED, Questions: 3/3, Time: 62.5s)\n",
      "   • image03 (Type: ANIMATED, Questions: 3/3, Time: 42.8s)\n",
      "   • image04 (Type: ANIMATED, Questions: 3/3, Time: 74.8s)\n",
      "   • image05 (Type: ANIMATED, Questions: 3/3, Time: 69.4s)\n",
      "   • image06 (Type: ANIMATED, Questions: 3/3, Time: 55.4s)\n",
      "   • image07 (Type: ANIMATED, Questions: 3/3, Time: 39.3s)\n",
      "   • image08 (Type: ANIMATED, Questions: 3/3, Time: 55.8s)\n",
      "   • image09 (Type: ANIMATED, Questions: 3/3, Time: 90.5s)\n",
      "   • image11 (Type: ANIMATED, Questions: 3/3, Time: 97.7s)\n",
      "   • image12 (Type: ANIMATED, Questions: 3/3, Time: 147.6s)\n",
      "   • image14 (Type: ANIMATED, Questions: 3/3, Time: 74.2s)\n",
      "   • image17 (Type: ANIMATED, Questions: 3/3, Time: 75.6s)\n",
      "   • image18 (Type: ANIMATED, Questions: 3/3, Time: 61.7s)\n",
      "   • image19 (Type: ANIMATED, Questions: 3/3, Time: 111.4s)\n",
      "   • image20 (Type: ANIMATED, Questions: 3/3, Time: 69.3s)\n",
      "   • image22 (Type: ANIMATED, Questions: 3/3, Time: 93.6s)\n",
      "   • image24 (Type: ANIMATED, Questions: 3/3, Time: 52.7s)\n",
      "   • image25 (Type: ANIMATED, Questions: 3/3, Time: 57.0s)\n",
      "   • image26 (Type: ANIMATED, Questions: 3/3, Time: 84.2s)\n",
      "   • image27 (Type: ANIMATED, Questions: 3/3, Time: 78.8s)\n",
      "   • image28 (Type: ANIMATED, Questions: 3/3, Time: 114.6s)\n",
      "   • image29 (Type: ANIMATED, Questions: 3/3, Time: 62.2s)\n",
      "   • image30 (Type: ANIMATED, Questions: 3/3, Time: 43.3s)\n",
      "   • image31 (Type: ANIMATED, Questions: 3/3, Time: 80.6s)\n",
      "   • image32 (Type: ANIMATED, Questions: 3/3, Time: 29.6s)\n",
      "   • image33 (Type: ANIMATED, Questions: 3/3, Time: 35.2s)\n",
      "   • image34 (Type: ANIMATED, Questions: 3/3, Time: 40.7s)\n",
      "   • image35 (Type: ANIMATED, Questions: 3/3, Time: 71.5s)\n",
      "   • image36 (Type: ANIMATED, Questions: 3/3, Time: 62.4s)\n",
      "   • image37 (Type: ANIMATED, Questions: 3/3, Time: 67.6s)\n",
      "   • image38 (Type: ANIMATED, Questions: 3/3, Time: 77.7s)\n",
      "   • image39 (Type: ANIMATED, Questions: 3/3, Time: 72.8s)\n",
      "   • image40 (Type: ANIMATED, Questions: 3/3, Time: 50.9s)\n",
      "   • image50 (Type: ANIMATED, Questions: 3/3, Time: 95.3s)\n",
      "   • image54 (Type: ANIMATED, Questions: 3/3, Time: 63.6s)\n",
      "   • image55 (Type: ANIMATED, Questions: 3/3, Time: 37.3s)\n",
      "   • image56 (Type: ANIMATED, Questions: 3/3, Time: 32.0s)\n",
      "   • image57 (Type: ANIMATED, Questions: 3/3, Time: 32.1s)\n",
      "   • image60 (Type: ANIMATED, Questions: 3/3, Time: 42.8s)\n",
      "   • image61 (Type: ANIMATED, Questions: 3/3, Time: 33.7s)\n",
      "   • image62 (Type: ANIMATED, Questions: 3/3, Time: 96.7s)\n",
      "   • image63 (Type: ANIMATED, Questions: 3/3, Time: 108.3s)\n",
      "   • image64 (Type: ANIMATED, Questions: 3/3, Time: 44.3s)\n",
      "   • image65 (Type: ANIMATED, Questions: 3/3, Time: 59.2s)\n",
      "   • image66 (Type: ANIMATED, Questions: 3/3, Time: 43.8s)\n",
      "   • image67 (Type: ANIMATED, Questions: 3/3, Time: 50.9s)\n",
      "   • image68 (Type: ANIMATED, Questions: 3/3, Time: 66.5s)\n",
      "   • image69 (Type: ANIMATED, Questions: 3/3, Time: 75.1s)\n",
      "   • image70 (Type: ANIMATED, Questions: 3/3, Time: 41.9s)\n",
      "   • image71 (Type: ANIMATED, Questions: 3/3, Time: 45.2s)\n",
      "   • image72 (Type: ANIMATED, Questions: 3/3, Time: 82.7s)\n",
      "   • image73 (Type: ANIMATED, Questions: 3/3, Time: 73.3s)\n",
      "   • image74 (Type: ANIMATED, Questions: 3/3, Time: 50.0s)\n",
      "   • image75 (Type: ANIMATED, Questions: 3/3, Time: 57.2s)\n",
      "   • image76 (Type: ANIMATED, Questions: 3/3, Time: 90.7s)\n",
      "   • image77 (Type: ANIMATED, Questions: 3/3, Time: 98.6s)\n",
      "   • image78 (Type: ANIMATED, Questions: 3/3, Time: 69.2s)\n",
      "   • image79 (Type: ANIMATED, Questions: 3/3, Time: 55.0s)\n",
      "   • image80 (Type: ANIMATED, Questions: 3/3, Time: 106.5s)\n",
      "   • image81 (Type: ANIMATED, Questions: 3/3, Time: 78.7s)\n",
      "   • image82 (Type: ANIMATED, Questions: 3/3, Time: 55.0s)\n",
      "   • image83 (Type: ANIMATED, Questions: 3/3, Time: 51.2s)\n",
      "   • image85 (Type: ANIMATED, Questions: 3/3, Time: 36.1s)\n",
      "   • image86 (Type: ANIMATED, Questions: 3/3, Time: 57.0s)\n",
      "   • image87 (Type: ANIMATED, Questions: 3/3, Time: 55.9s)\n",
      "   • image88 (Type: ANIMATED, Questions: 3/3, Time: 52.1s)\n",
      "   • image89 (Type: ANIMATED, Questions: 3/3, Time: 49.7s)\n",
      "   • image90 (Type: ANIMATED, Questions: 3/3, Time: 32.2s)\n",
      "   • image91 (Type: ANIMATED, Questions: 3/3, Time: 59.2s)\n",
      "   • image92 (Type: ANIMATED, Questions: 3/3, Time: 71.7s)\n",
      "   • image93 (Type: ANIMATED, Questions: 3/3, Time: 78.9s)\n",
      "   • image94 (Type: ANIMATED, Questions: 3/3, Time: 55.6s)\n",
      "   • image95 (Type: ANIMATED, Questions: 3/3, Time: 39.4s)\n",
      "   • image96 (Type: ANIMATED, Questions: 3/3, Time: 54.3s)\n",
      "   • image97 (Type: ANIMATED, Questions: 3/3, Time: 46.2s)\n",
      "   • image98 (Type: ANIMATED, Questions: 3/3, Time: 44.5s)\n",
      "   • image99 (Type: ANIMATED, Questions: 3/3, Time: 66.0s)\n",
      "   • image100 (Type: ANIMATED, Questions: 3/3, Time: 46.5s)\n",
      "   • image101 (Type: ANIMATED, Questions: 3/3, Time: 36.1s)\n",
      "   • image102 (Type: ANIMATED, Questions: 3/3, Time: 81.6s)\n",
      "   • image103 (Type: ANIMATED, Questions: 3/3, Time: 93.9s)\n",
      "   • image104 (Type: ANIMATED, Questions: 3/3, Time: 51.8s)\n",
      "   • image105 (Type: ANIMATED, Questions: 3/3, Time: 57.3s)\n",
      "   • image106 (Type: ANIMATED, Questions: 3/3, Time: 98.1s)\n",
      "   • image107 (Type: ANIMATED, Questions: 3/3, Time: 42.6s)\n",
      "   • image108 (Type: ANIMATED, Questions: 3/3, Time: 26.3s)\n",
      "   • image109 (Type: ANIMATED, Questions: 3/3, Time: 61.4s)\n",
      "   • image01 (Type: AI_GENERATED, Questions: 3/3, Time: 59.6s)\n",
      "   • image02 (Type: AI_GENERATED, Questions: 3/3, Time: 59.0s)\n",
      "   • image03 (Type: AI_GENERATED, Questions: 3/3, Time: 69.8s)\n",
      "   • image04 (Type: AI_GENERATED, Questions: 3/3, Time: 83.5s)\n",
      "   • image05 (Type: AI_GENERATED, Questions: 3/3, Time: 77.4s)\n",
      "   • image06 (Type: AI_GENERATED, Questions: 3/3, Time: 72.0s)\n",
      "   • image07 (Type: AI_GENERATED, Questions: 3/3, Time: 56.3s)\n",
      "   • image08 (Type: AI_GENERATED, Questions: 3/3, Time: 113.7s)\n",
      "   • image09 (Type: AI_GENERATED, Questions: 3/3, Time: 55.9s)\n",
      "   • image10 (Type: AI_GENERATED, Questions: 3/3, Time: 86.7s)\n",
      "   • image11 (Type: AI_GENERATED, Questions: 3/3, Time: 100.8s)\n",
      "   • image12 (Type: AI_GENERATED, Questions: 3/3, Time: 44.3s)\n",
      "   • image13 (Type: AI_GENERATED, Questions: 3/3, Time: 56.2s)\n",
      "   • image14 (Type: AI_GENERATED, Questions: 3/3, Time: 48.1s)\n",
      "   • image15 (Type: AI_GENERATED, Questions: 3/3, Time: 90.0s)\n",
      "   • image16 (Type: AI_GENERATED, Questions: 3/3, Time: 72.0s)\n",
      "   • image17 (Type: AI_GENERATED, Questions: 3/3, Time: 52.2s)\n",
      "   • image18 (Type: AI_GENERATED, Questions: 3/3, Time: 126.2s)\n",
      "   • image19 (Type: AI_GENERATED, Questions: 3/3, Time: 78.8s)\n",
      "   • image20 (Type: AI_GENERATED, Questions: 3/3, Time: 60.6s)\n",
      "   • image21 (Type: AI_GENERATED, Questions: 3/3, Time: 40.9s)\n",
      "   • image22 (Type: AI_GENERATED, Questions: 3/3, Time: 73.3s)\n",
      "   • image23 (Type: AI_GENERATED, Questions: 3/3, Time: 70.2s)\n",
      "   • image24 (Type: AI_GENERATED, Questions: 3/3, Time: 73.4s)\n",
      "   • image25 (Type: AI_GENERATED, Questions: 3/3, Time: 49.8s)\n",
      "   • image26 (Type: AI_GENERATED, Questions: 3/3, Time: 68.9s)\n",
      "   • image27 (Type: AI_GENERATED, Questions: 3/3, Time: 58.2s)\n",
      "   • image28 (Type: AI_GENERATED, Questions: 3/3, Time: 48.0s)\n",
      "   • image29 (Type: AI_GENERATED, Questions: 3/3, Time: 36.1s)\n",
      "   • image30 (Type: AI_GENERATED, Questions: 3/3, Time: 63.0s)\n",
      "   • image31 (Type: AI_GENERATED, Questions: 3/3, Time: 72.1s)\n",
      "   • image32 (Type: AI_GENERATED, Questions: 3/3, Time: 52.2s)\n",
      "   • image33 (Type: AI_GENERATED, Questions: 3/3, Time: 66.2s)\n",
      "   • image34 (Type: AI_GENERATED, Questions: 3/3, Time: 98.5s)\n",
      "   • image35 (Type: AI_GENERATED, Questions: 3/3, Time: 72.5s)\n",
      "   • image36 (Type: AI_GENERATED, Questions: 3/3, Time: 73.4s)\n",
      "   • image37 (Type: AI_GENERATED, Questions: 3/3, Time: 44.5s)\n",
      "   • image38 (Type: AI_GENERATED, Questions: 3/3, Time: 59.0s)\n",
      "   • image39 (Type: AI_GENERATED, Questions: 3/3, Time: 53.0s)\n",
      "   • image40 (Type: AI_GENERATED, Questions: 3/3, Time: 62.4s)\n",
      "   • image41 (Type: AI_GENERATED, Questions: 3/3, Time: 62.6s)\n",
      "   • image42 (Type: AI_GENERATED, Questions: 3/3, Time: 73.4s)\n",
      "   • image43 (Type: AI_GENERATED, Questions: 3/3, Time: 70.9s)\n",
      "   • image44 (Type: AI_GENERATED, Questions: 3/3, Time: 79.9s)\n",
      "   • image45 (Type: AI_GENERATED, Questions: 3/3, Time: 55.3s)\n",
      "   • image46 (Type: AI_GENERATED, Questions: 3/3, Time: 113.1s)\n",
      "   • image47 (Type: AI_GENERATED, Questions: 3/3, Time: 62.5s)\n",
      "   • image48 (Type: AI_GENERATED, Questions: 3/3, Time: 86.0s)\n",
      "   • image49 (Type: AI_GENERATED, Questions: 3/3, Time: 79.6s)\n",
      "   • image50 (Type: AI_GENERATED, Questions: 3/3, Time: 107.5s)\n",
      "   • image51 (Type: AI_GENERATED, Questions: 3/3, Time: 101.1s)\n",
      "   • image52 (Type: AI_GENERATED, Questions: 3/3, Time: 129.7s)\n",
      "   • image53 (Type: AI_GENERATED, Questions: 3/3, Time: 99.5s)\n",
      "   • image54 (Type: AI_GENERATED, Questions: 3/3, Time: 143.7s)\n",
      "   • image55 (Type: AI_GENERATED, Questions: 3/3, Time: 88.6s)\n",
      "   • image56 (Type: AI_GENERATED, Questions: 3/3, Time: 52.6s)\n",
      "   • image57 (Type: AI_GENERATED, Questions: 3/3, Time: 63.7s)\n",
      "   • image58 (Type: AI_GENERATED, Questions: 3/3, Time: 136.0s)\n",
      "   • image59 (Type: AI_GENERATED, Questions: 3/3, Time: 66.7s)\n",
      "   • image60 (Type: AI_GENERATED, Questions: 3/3, Time: 80.8s)\n",
      "   • image61 (Type: AI_GENERATED, Questions: 3/3, Time: 100.5s)\n",
      "   • image62 (Type: AI_GENERATED, Questions: 3/3, Time: 136.5s)\n",
      "   • image63 (Type: AI_GENERATED, Questions: 3/3, Time: 92.1s)\n",
      "   • image64 (Type: AI_GENERATED, Questions: 3/3, Time: 68.8s)\n",
      "   • image65 (Type: AI_GENERATED, Questions: 3/3, Time: 65.4s)\n",
      "   • image66 (Type: AI_GENERATED, Questions: 3/3, Time: 83.7s)\n",
      "   • image67 (Type: AI_GENERATED, Questions: 3/3, Time: 78.2s)\n",
      "   • image68 (Type: AI_GENERATED, Questions: 3/3, Time: 72.5s)\n",
      "   • image69 (Type: AI_GENERATED, Questions: 3/3, Time: 101.9s)\n",
      "   • image70 (Type: AI_GENERATED, Questions: 3/3, Time: 126.8s)\n",
      "   • image71 (Type: AI_GENERATED, Questions: 3/3, Time: 40.9s)\n",
      "   • image72 (Type: AI_GENERATED, Questions: 3/3, Time: 46.3s)\n",
      "   • image73 (Type: AI_GENERATED, Questions: 3/3, Time: 104.0s)\n",
      "   • image74 (Type: AI_GENERATED, Questions: 3/3, Time: 48.0s)\n",
      "   • image75 (Type: AI_GENERATED, Questions: 3/3, Time: 44.5s)\n",
      "   • image76 (Type: AI_GENERATED, Questions: 3/3, Time: 39.1s)\n",
      "   • image77 (Type: AI_GENERATED, Questions: 3/3, Time: 114.1s)\n",
      "   • image78 (Type: AI_GENERATED, Questions: 3/3, Time: 82.1s)\n",
      "   • image79 (Type: AI_GENERATED, Questions: 3/3, Time: 67.2s)\n",
      "   • image80 (Type: AI_GENERATED, Questions: 3/3, Time: 103.2s)\n",
      "   • image81 (Type: AI_GENERATED, Questions: 3/3, Time: 64.4s)\n",
      "   • image82 (Type: AI_GENERATED, Questions: 3/3, Time: 60.4s)\n",
      "   • image83 (Type: AI_GENERATED, Questions: 3/3, Time: 74.7s)\n",
      "   • image84 (Type: AI_GENERATED, Questions: 3/3, Time: 91.1s)\n",
      "   • image85 (Type: AI_GENERATED, Questions: 3/3, Time: 36.5s)\n",
      "   • image86 (Type: AI_GENERATED, Questions: 3/3, Time: 67.2s)\n",
      "   • image87 (Type: AI_GENERATED, Questions: 3/3, Time: 69.0s)\n",
      "   • image88 (Type: AI_GENERATED, Questions: 3/3, Time: 51.5s)\n",
      "   • image89 (Type: AI_GENERATED, Questions: 3/3, Time: 45.9s)\n",
      "   • image90 (Type: AI_GENERATED, Questions: 3/3, Time: 78.5s)\n",
      "   • image91 (Type: AI_GENERATED, Questions: 3/3, Time: 70.5s)\n",
      "   • image92 (Type: AI_GENERATED, Questions: 3/3, Time: 67.8s)\n",
      "   • image93 (Type: AI_GENERATED, Questions: 3/3, Time: 55.9s)\n",
      "   • image94 (Type: AI_GENERATED, Questions: 3/3, Time: 83.6s)\n",
      "   • image95 (Type: AI_GENERATED, Questions: 3/3, Time: 90.9s)\n",
      "   • image96 (Type: AI_GENERATED, Questions: 3/3, Time: 67.0s)\n",
      "   • image97 (Type: AI_GENERATED, Questions: 3/3, Time: 69.7s)\n",
      "   • image98 (Type: AI_GENERATED, Questions: 3/3, Time: 72.3s)\n",
      "   • image99 (Type: AI_GENERATED, Questions: 3/3, Time: 89.5s)\n",
      "   • image100 (Type: AI_GENERATED, Questions: 3/3, Time: 84.9s)\n",
      "   • image101 (Type: AI_GENERATED, Questions: 3/3, Time: 88.8s)\n",
      "   • image102 (Type: AI_GENERATED, Questions: 3/3, Time: 64.0s)\n",
      "   • image103 (Type: AI_GENERATED, Questions: 3/3, Time: 52.5s)\n",
      "   • image104 (Type: AI_GENERATED, Questions: 3/3, Time: 80.5s)\n",
      "   • image105 (Type: AI_GENERATED, Questions: 3/3, Time: 52.7s)\n",
      "   • image106 (Type: AI_GENERATED, Questions: 3/3, Time: 62.0s)\n",
      "   • image107 (Type: AI_GENERATED, Questions: 3/3, Time: 88.7s)\n",
      "   • image108 (Type: AI_GENERATED, Questions: 3/3, Time: 51.3s)\n",
      "   • image109 (Type: AI_GENERATED, Questions: 3/3, Time: 75.5s)\n",
      "   • image110 (Type: AI_GENERATED, Questions: 3/3, Time: 83.8s)\n",
      "   • image111 (Type: AI_GENERATED, Questions: 3/3, Time: 42.4s)\n",
      "   • image112 (Type: AI_GENERATED, Questions: 3/3, Time: 45.1s)\n",
      "   • image113 (Type: AI_GENERATED, Questions: 3/3, Time: 55.8s)\n",
      "   • image114 (Type: AI_GENERATED, Questions: 3/3, Time: 63.9s)\n",
      "   • image115 (Type: AI_GENERATED, Questions: 3/3, Time: 74.1s)\n",
      "   • image116 (Type: AI_GENERATED, Questions: 3/3, Time: 56.7s)\n",
      "   • image117 (Type: AI_GENERATED, Questions: 3/3, Time: 50.1s)\n",
      "   • image118 (Type: AI_GENERATED, Questions: 3/3, Time: 85.2s)\n",
      "   • image119 (Type: AI_GENERATED, Questions: 3/3, Time: 110.4s)\n",
      "   • image120 (Type: AI_GENERATED, Questions: 3/3, Time: 95.5s)\n",
      "\n",
      "❌ FAILED/PROBLEMATIC IMAGES (51):\n",
      "   • image22 (Type: REAL) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image23 (Type: REAL) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image54 (Type: REAL) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image56 (Type: REAL) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image64 (Type: REAL) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image71 (Type: REAL) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image75 (Type: REAL) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image84 (Type: REAL) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image85 (Type: REAL) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image89 (Type: REAL) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image90 (Type: REAL) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image92 (Type: REAL) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image97 (Type: REAL) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image99 (Type: REAL) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image108 (Type: REAL) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image109 (Type: REAL) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image110 (Type: REAL) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image111 (Type: REAL) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image116 (Type: REAL) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image10 (Type: ANIMATED) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image13 (Type: ANIMATED) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image15 (Type: ANIMATED) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image16 (Type: ANIMATED) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image21 (Type: ANIMATED) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image23 (Type: ANIMATED) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image41 (Type: ANIMATED) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image42 (Type: ANIMATED) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image43 (Type: ANIMATED) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image44 (Type: ANIMATED) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image45 (Type: ANIMATED) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image46 (Type: ANIMATED) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image47 (Type: ANIMATED) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image48 (Type: ANIMATED) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image49 (Type: ANIMATED) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image51 (Type: ANIMATED) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image52 (Type: ANIMATED) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image53 (Type: ANIMATED) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image58 (Type: ANIMATED) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image59 (Type: ANIMATED) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image84 (Type: ANIMATED) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image110 (Type: ANIMATED) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image111 (Type: ANIMATED) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image112 (Type: ANIMATED) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image113 (Type: ANIMATED) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image114 (Type: ANIMATED) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image115 (Type: ANIMATED) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image116 (Type: ANIMATED) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image117 (Type: ANIMATED) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image118 (Type: ANIMATED) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image119 (Type: ANIMATED) - PARTIAL: 3/3 failed (0.0% success)\n",
      "   • image120 (Type: ANIMATED) - PARTIAL: 3/3 failed (0.0% success)\n",
      "\n",
      "📈 FAILURE ANALYSIS BY IMAGE TYPE:\n",
      "   • REAL: 19 failed images\n",
      "     - image22 (partial_failure)\n",
      "     - image23 (partial_failure)\n",
      "     - image54 (partial_failure)\n",
      "     - image56 (partial_failure)\n",
      "     - image64 (partial_failure)\n",
      "     - image71 (partial_failure)\n",
      "     - image75 (partial_failure)\n",
      "     - image84 (partial_failure)\n",
      "     - image85 (partial_failure)\n",
      "     - image89 (partial_failure)\n",
      "     - image90 (partial_failure)\n",
      "     - image92 (partial_failure)\n",
      "     - image97 (partial_failure)\n",
      "     - image99 (partial_failure)\n",
      "     - image108 (partial_failure)\n",
      "     - image109 (partial_failure)\n",
      "     - image110 (partial_failure)\n",
      "     - image111 (partial_failure)\n",
      "     - image116 (partial_failure)\n",
      "   • ANIMATED: 32 failed images\n",
      "     - image10 (partial_failure)\n",
      "     - image13 (partial_failure)\n",
      "     - image15 (partial_failure)\n",
      "     - image16 (partial_failure)\n",
      "     - image21 (partial_failure)\n",
      "     - image23 (partial_failure)\n",
      "     - image41 (partial_failure)\n",
      "     - image42 (partial_failure)\n",
      "     - image43 (partial_failure)\n",
      "     - image44 (partial_failure)\n",
      "     - image45 (partial_failure)\n",
      "     - image46 (partial_failure)\n",
      "     - image47 (partial_failure)\n",
      "     - image48 (partial_failure)\n",
      "     - image49 (partial_failure)\n",
      "     - image51 (partial_failure)\n",
      "     - image52 (partial_failure)\n",
      "     - image53 (partial_failure)\n",
      "     - image58 (partial_failure)\n",
      "     - image59 (partial_failure)\n",
      "     - image84 (partial_failure)\n",
      "     - image110 (partial_failure)\n",
      "     - image111 (partial_failure)\n",
      "     - image112 (partial_failure)\n",
      "     - image113 (partial_failure)\n",
      "     - image114 (partial_failure)\n",
      "     - image115 (partial_failure)\n",
      "     - image116 (partial_failure)\n",
      "     - image117 (partial_failure)\n",
      "     - image118 (partial_failure)\n",
      "     - image119 (partial_failure)\n",
      "     - image120 (partial_failure)\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_Qwen2_5VL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac9336f",
   "metadata": {
    "papermill": {
     "duration": 0.03132,
     "end_time": "2025-07-28T20:21:14.655716",
     "exception": false,
     "start_time": "2025-07-28T20:21:14.624396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7163706,
     "sourceId": 11436696,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python ((op_bench)",
   "language": "python",
   "name": "op_bench"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 22661.036525,
   "end_time": "2025-07-28T20:21:18.317604",
   "environment_variables": {},
   "exception": null,
   "input_path": "/var/scratch/ave303/OP_bench/opa-benchmark-smolvlm2-qwen2-5-vl.ipynb",
   "output_path": "qwen2-5-vl_32b_output.ipynb",
   "parameters": {},
   "start_time": "2025-07-28T14:03:37.281079",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
