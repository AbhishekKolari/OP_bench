{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11436696,"sourceType":"datasetVersion","datasetId":7163706}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# VLM Benchmark for Object Property Abstraction\n\nThis notebook implements a benchmark for evaluating Vision Language Models (VLMs) on object property abstraction and visual question answering (VQA) tasks. The benchmark includes three types of questions:\n\n1. Direct Recognition\n2. Property Inference\n3. Counterfactual Reasoning\n\nAnd three types of images:\n- REAL\n- ANIMATED\n- AI GENERATED","metadata":{}},{"cell_type":"markdown","source":"## Setup and Imports\n\nFirst, let's import the necessary libraries and set up our environment.","metadata":{}},{"cell_type":"code","source":"# Install required packages\n!pip install transformers torch Pillow tqdm bitsandbytes accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T15:36:51.285737Z","iopub.execute_input":"2025-04-16T15:36:51.286006Z","iopub.status.idle":"2025-04-16T15:38:18.459667Z","shell.execute_reply.started":"2025-04-16T15:36:51.285989Z","shell.execute_reply":"2025-04-16T15:38:18.458975Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bitsandbytes-0.45.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"pip install num2words qwen-vl-utils #flash-attn --no-build-isolation ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T15:38:18.461371Z","iopub.execute_input":"2025-04-16T15:38:18.461601Z","iopub.status.idle":"2025-04-16T15:38:26.386328Z","shell.execute_reply.started":"2025-04-16T15:38:18.461571Z","shell.execute_reply":"2025-04-16T15:38:26.385393Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting num2words\n  Downloading num2words-0.5.14-py3-none-any.whl.metadata (13 kB)\nCollecting qwen-vl-utils\n  Downloading qwen_vl_utils-0.0.10-py3-none-any.whl.metadata (6.3 kB)\nCollecting docopt>=0.6.2 (from num2words)\n  Downloading docopt-0.6.2.tar.gz (25 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting av (from qwen-vl-utils)\n  Downloading av-14.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from qwen-vl-utils) (24.2)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from qwen-vl-utils) (11.1.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from qwen-vl-utils) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->qwen-vl-utils) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->qwen-vl-utils) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->qwen-vl-utils) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->qwen-vl-utils) (2025.1.31)\nDownloading num2words-0.5.14-py3-none-any.whl (163 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading qwen_vl_utils-0.0.10-py3-none-any.whl (6.7 kB)\nDownloading av-14.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.2/35.2 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: docopt\n  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=f9812a8dffb87c070b5c10deef78397b0ea9fab2fef0558db8acc2ebcd9d0b67\n  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\nSuccessfully built docopt\nInstalling collected packages: docopt, num2words, av, qwen-vl-utils\nSuccessfully installed av-14.3.0 docopt-0.6.2 num2words-0.5.14 qwen-vl-utils-0.0.10\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Import required libraries\nimport torch\nimport json\nfrom pathlib import Path\nfrom PIL import Image\nimport gc\nimport re\nfrom tqdm import tqdm\nfrom typing import List, Dict, Any\nfrom qwen_vl_utils import process_vision_info\n\n# Check if CUDA is available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T15:38:26.387509Z","iopub.execute_input":"2025-04-16T15:38:26.387820Z","iopub.status.idle":"2025-04-16T15:38:33.432871Z","shell.execute_reply.started":"2025-04-16T15:38:26.387784Z","shell.execute_reply":"2025-04-16T15:38:33.432227Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Benchmark Tester Class\n\nThis class handles the evaluation of models against our benchmark.","metadata":{}},{"cell_type":"code","source":"class BenchmarkTester:\n    def __init__(self, benchmark_path=\"/kaggle/input/opabenchmark/benchmark.json\", data_dir=\"/kaggle/input/opabenchmark/data\"):\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        with open(benchmark_path, 'r') as f:\n            self.benchmark = json.load(f)\n        self.data_dir = data_dir\n    \n    def format_question(self, question, model_name):\n        \"\"\"Format a question for the model.\"\"\"\n\n        if model_name==\"blip2\":\n            return f\"Question: {question['question']} Answer:\"\n        else:\n            return f\"Question: {question['question']} Answer with a number and list of objects. Answer:\"\n\n    def clean_answer(self, answer):\n        \"\"\"Clean the model output to extract just the number.\"\"\"\n        # Remove any text that's not a number\n        # import re\n        # numbers = re.findall(r'\\d+', answer)\n        # if numbers:\n        #     return numbers[0]  # Return the first number found\n        # return answer\n        \"\"\"Extract number and reasoning from the model's answer.\"\"\"\n        # Try to extract number and reasoning using regex\n        import re\n        pattern = r'(\\d+)\\s*\\[(.*?)\\]'\n        match = re.search(pattern, answer)\n        \n        if match:\n            number = match.group(1)\n            objects = [obj.strip() for obj in match.group(2).split(',')]\n            return {\n                \"count\": number,\n                \"reasoning\": objects\n            }\n        else:\n            # Fallback if format isn't matched\n            numbers = re.findall(r'\\d+', answer)\n            return {\n                \"count\": numbers[0] if numbers else \"0\",\n                \"reasoning\": []\n            }\n\n    def model_generation(self, model_name, model, inputs, processor):\n        \"\"\"Generate answer and decode.\"\"\"\n        outputs = None  # Initialize outputs to None\n        \n        if model_name==\"smolVLM2\":\n            outputs = model.generate(**inputs, do_sample=False, max_new_tokens=64)\n            answer = processor.batch_decode(\n                outputs,\n                skip_special_tokens=True,\n            )[0]\n        elif model_name==\"Qwen2.5-VL\":\n            outputs = model.generate(**inputs, max_new_tokens=128)\n            outputs = [\n                out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, outputs)\n            ]\n            answer = processor.batch_decode(\n                outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False\n            )[0]\n        else:\n            print(f\"Warning: Unknown model name '{model_name}' in model_generation.\")\n            answer = \"\"  # Return an empty string\n\n        return answer, outputs\n    \n    def evaluate_model(self, model_name, model, processor, save_path, start_idx=0, batch_size=5):\n        results = []\n        print(f\"\\nEvaluating {model_name}...\")\n        print(f\"Using device: {self.device}\")\n        \n        # Force garbage collection before starting\n        gc.collect()\n        torch.cuda.empty_cache()\n\n        try:\n            images = self.benchmark['benchmark']['images'][start_idx:start_idx + batch_size]\n            total_images = len(images)\n            \n            for idx, image_data in enumerate(tqdm(images, desc=\"Processing images\")):\n                try:\n                    print(f\"\\nProcessing image {idx+1}/{total_images}: {image_data['image_id']}\")\n                    image_path = Path(self.data_dir)/image_data['path']\n                    if not image_path.exists():\n                        print(f\"Warning: Image not found at {image_path}\")\n                        continue\n                    \n                    # Load and preprocess image\n                    image = Image.open(image_path).convert(\"RGB\")\n                    image_results = []  # Store results for current image\n                    \n                    for question in image_data['questions']:\n                        try:\n                            # prompt = self.format_question(question, model_name)\n                            print(f\"Question: {question['question']}\")\n\n                            messages = [\n                                {\n                                    \"role\": \"user\",\n                                    \"content\": [\n                                        {\"type\": \"image\", \"image\": image},\n                                        {\"type\": \"text\", \"text\": f\"{question['question']} Answer format: total number(numerical) objects(within square brackets)\"},\n                                    ]\n                                },\n                            ]\n                            \n                            # Clear cache before processing each question\n                            torch.cuda.empty_cache()\n                            \n                            # Process image and text\n                            # inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(self.device)\n                            if model_name==\"smolVLM2\":\n                                inputs = processor.apply_chat_template(\n                                    messages,\n                                    add_generation_prompt=True,\n                                    tokenize=True,\n                                    return_dict=True,\n                                    return_tensors=\"pt\",\n                                ).to(model.device, dtype=torch.bfloat16)\n                            else:\n                                \n                                text = processor.apply_chat_template(\n                                    messages, tokenize=False, add_generation_prompt=True\n                                )\n                                # image_inputs, video_inputs = process_vision_info(messages)\n                                inputs = processor(\n                                    text=text,\n                                    images=image,\n                                    videos=None,\n                                    padding=True,\n                                    return_tensors=\"pt\",\n                                ).to(\"cuda\")\n                            \n                            # Generate answer with better settings\n                            with torch.no_grad():\n                                answer, outputs = self.model_generation(model_name, model, inputs, processor)    #call for model.generate\n        \n                            cleaned_answer = self.clean_answer(answer)\n                            \n                            image_results.append({\n                                \"image_id\": image_data[\"image_id\"],\n                                \"image_type\": image_data[\"image_type\"],\n                                \"question_id\": question[\"id\"],\n                                \"question\": question[\"question\"],\n                                \"ground_truth\": question[\"answer\"],\n                                \"model_answer\": cleaned_answer[\"count\"],\n                                \"model_reasoning\": cleaned_answer[\"reasoning\"],\n                                \"raw_answer\": answer,  # Keep raw answer for debugging\n                                \"property_category\": question[\"property_category\"]\n                            })\n                            \n                            # Clear memory\n                            del outputs, inputs\n                            torch.cuda.empty_cache()\n                            \n                        except Exception as e:\n                            print(f\"Error processing question: {str(e)}\")\n                            continue\n                    \n                    # Add results from this image\n                    results.extend(image_results)\n                    \n                    # Save intermediate results only every 2 images or if it's the last image\n                    if (idx + 1) % 2 == 0 or idx == total_images - 1:\n                        with open(f\"{save_path}_checkpoint.json\", 'w') as f:\n                            json.dump(results, f, indent=4)\n                            \n                except Exception as e:\n                    print(f\"Error processing image {image_data['image_id']}: {str(e)}\")\n                    continue\n            \n            # Save final results\n            if results:\n                with open(save_path, 'w') as f:\n                    json.dump(results, f, indent=4)\n            \n        except Exception as e:\n            print(f\"An error occurred during evaluation: {str(e)}\")\n            if results:\n                with open(f\"{save_path}_error_state.json\", 'w') as f:\n                    json.dump(results, f, indent=4)\n        \n        return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T15:39:48.990820Z","iopub.execute_input":"2025-04-16T15:39:48.991324Z","iopub.status.idle":"2025-04-16T15:39:49.007596Z","shell.execute_reply.started":"2025-04-16T15:39:48.991295Z","shell.execute_reply":"2025-04-16T15:39:49.006851Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import os\nprint(os.listdir(\"/kaggle/input/opabenchmark/\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T15:39:52.811491Z","iopub.execute_input":"2025-04-16T15:39:52.811750Z","iopub.status.idle":"2025-04-16T15:39:52.816990Z","shell.execute_reply.started":"2025-04-16T15:39:52.811731Z","shell.execute_reply":"2025-04-16T15:39:52.816395Z"}},"outputs":[{"name":"stdout","text":"['data', 'benchmark.json']\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Test SmolVLM Model\n\nLet's evaluate the SmolVLM2-2.2B-Instruct model","metadata":{}},{"cell_type":"code","source":"def test_smolVLM2():\n    from transformers import AutoProcessor, AutoModelForImageTextToText\n\n    print(\"Loading smolVLM model...\")\n    \n    model = AutoModelForImageTextToText.from_pretrained(\n        \"HuggingFaceTB/SmolVLM2-2.2B-Instruct\",\n        torch_dtype=torch.bfloat16,\n        # _attn_implementation=\"flash_attention_2\"\n        low_cpu_mem_usage=True\n    ).to(\"cuda\")\n\n    processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM2-2.2B-Instruct\")\n\n    ## A bit slow without the flash_attention2 requires ampere gpu's. Better performance in some cases\n\n    # Optional: Enable memory efficient attention\n    if hasattr(model.config, 'use_memory_efficient_attention'):\n        model.config.use_memory_efficient_attention = True\n\n    tester = BenchmarkTester()\n    smolVLM_results = tester.evaluate_model(\n        \"smolVLM2\",\n        model, \n        processor, \n        \"smolVLM2_results.json\", \n        batch_size=25\n    )\n\n    # Clean up\n    del model, processor\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T15:39:55.641199Z","iopub.execute_input":"2025-04-16T15:39:55.641737Z","iopub.status.idle":"2025-04-16T15:39:55.648824Z","shell.execute_reply.started":"2025-04-16T15:39:55.641701Z","shell.execute_reply":"2025-04-16T15:39:55.647854Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Test Qwen2.5-VL\n\nLets evaluate the Qwen2.5-VL-7B-Instruct model","metadata":{}},{"cell_type":"code","source":"def test_Qwen2_5VL():\n    from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n    \n    # default: Load the model on the available device(s)\n    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n        \"Qwen/Qwen2.5-VL-3B-Instruct\", \n        load_in_8bit=True, # throws error when .to() is added\n        torch_dtype=torch.bfloat16, \n        device_map=\"auto\",\n        # attn_implementation=\"flash_attention_2\",\n        low_cpu_mem_usage=True\n    )\n    \n    # We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n    # model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n    #     \"Qwen/Qwen2.5-VL-7B-Instruct\",\n    #     torch_dtype=torch.bfloat16,\n    #     attn_implementation=\"flash_attention_2\",\n    #     device_map=\"auto\",\n    # )\n    \n    # default processer\n    processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n\n    ### Qwen2.5-VL-7B-Instruct --> goes out of CUDA memory\n    ### Qwen2.5-VL-3B-Instruct --> can handle only 2 images before going out of memory but decent performance\n\n    # Optional: Enable memory efficient attention\n    if hasattr(model.config, 'use_memory_efficient_attention'):\n        model.config.use_memory_efficient_attention = True\n\n    tester = BenchmarkTester()\n    Qwen2_5VL_results = tester.evaluate_model(\n        \"Qwen2.5-VL\",\n        model, \n        processor, \n        \"Qwen2.5-VL_results.json\", \n        batch_size=2\n    )\n\n    # Clean up\n    del model, processor\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:00:23.256114Z","iopub.execute_input":"2025-04-16T11:00:23.257309Z","iopub.status.idle":"2025-04-16T11:00:23.262963Z","shell.execute_reply.started":"2025-04-16T11:00:23.257277Z","shell.execute_reply":"2025-04-16T11:00:23.262243Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Run Evaluation\n\nNow we can run our evaluation. Let's start with the SmolVLM2 model:","metadata":{}},{"cell_type":"code","source":"test_smolVLM2()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T15:40:22.880715Z","iopub.execute_input":"2025-04-16T15:40:22.881198Z","iopub.status.idle":"2025-04-16T15:49:44.356509Z","shell.execute_reply.started":"2025-04-16T15:40:22.881174Z","shell.execute_reply":"2025-04-16T15:49:44.355800Z"}},"outputs":[{"name":"stderr","text":"2025-04-16 15:40:27.391930: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1744818027.661107      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1744818027.736238      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Loading smolVLM model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/3.64k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8fd228deff741809716e567576e4a6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/63.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77b0c61c631a42c3bd6bd177c8756b0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"110930a0769e445889ae24487d8bcb7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12948562bf90473d877f91d9577418d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/4.03G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"921b9550beaa4596b3b126908f59b0b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b590464426b74370963d13ec9391ce51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/136 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"943d26b02ba146c3b062791f57e7f1ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"processor_config.json:   0%|          | 0.00/67.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68c98c12b5534f9b8196ba6fac6bbf3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.json:   0%|          | 0.00/430 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2040f70de1ea4e37836af4dcdf1563d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/599 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"396bd5c925d24125a23f79ae7c82e171"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/28.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cd5ccd281894cc7af8018c4697b8d6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f68ebacaf70840f9b8a46a20fd082cb9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0590bffc2c34ea6b159f4e56b9f67b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.55M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7978d1d68b784cc4ae7062d6e50d8c42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/4.74k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa088b49605447378e3119bb0464d646"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/868 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5bf45cbc6404b5ab95390a1afdc0d7e"}},"metadata":{}},{"name":"stdout","text":"\nEvaluating smolVLM2...\nUsing device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Processing images:   0%|          | 0/25 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 1/25: image01\nQuestion: How many objects made of wood are present?\nQuestion: Count the number of breakable items?\nQuestion: If one of the metal objects were replaced by a wooden object, how many wooden objects would be there in the image?\n","output_type":"stream"},{"name":"stderr","text":"Processing images:   4%|▍         | 1/25 [00:18<07:14, 18.09s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 2/25: image02\nQuestion: How many mammals are present in the image?\nQuestion: Count the number of items that can store other items?\nQuestion: If one of the zebra were replaced by a tree, how many mammals would be present in the image?\n","output_type":"stream"},{"name":"stderr","text":"Processing images:   8%|▊         | 2/25 [00:34<06:33, 17.10s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 3/25: image03\nQuestion: How many objects made of rubber are present?\nQuestion: How many objects with the primary purpose of illumination can be seen?\nQuestion: If the person riding one of the bicycles were replaced by a pedestrian, how many objects that have handles would be present?\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  12%|█▏        | 3/25 [00:56<07:04, 19.31s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 4/25: image04\nQuestion: How many tools are visible in the image?\nQuestion: How many cutting tools are present in this image?\nQuestion: If the red handle were replaced by a wooden handle, how many colored artifacts would remain in the image?\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  16%|█▌        | 4/25 [01:13<06:24, 18.31s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 5/25: image05\nQuestion: How many furniture items are present that have legs?\nQuestion: Count the number of containers that cannot hold hot liquids?\nQuestion: If the room were transformed into an open workspace instead of a meeting room, how many privacy features would need to be removed?\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  20%|██        | 5/25 [01:30<05:59, 17.99s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 6/25: image06\nQuestion: How many reptiles are visible in this enclosure?\nQuestion: How many reptilian couples, at maximum, are present?\nQuestion: If all the small pebbles forming the mosaic floor were replaced with sand, how many natural elements would still be visible in the enclosure?\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  24%|██▍       | 6/25 [01:48<05:42, 18.01s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 7/25: image07\nQuestion: How many birds are visible in this image?\nQuestion: How many objects are present that can comfortably seat a human?\nQuestion: If the birds sitting together only on one railing were to fly away, how many birds would remain?\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  28%|██▊       | 7/25 [02:06<05:21, 17.87s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 8/25: image08\nQuestion: How many reptiles are visible in this image?\nQuestion: How many objects are present that act as support?\nQuestion: If one turtle slid off the log into the water, how many turtles would be in the water?\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  32%|███▏      | 8/25 [02:23<05:02, 17.81s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 9/25: image09\nQuestion: How many different types of vegetables are present in the image?\nQuestion: How many objects are used as containers?\nQuestion: If the bag of limes were removed and replaced with two additional avocados, how many fruits would be present in total on the table, considering avocados are fruits?\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  36%|███▌      | 9/25 [02:42<04:48, 18.03s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 10/25: image10\nQuestion: How many objects are present that are flexible?\nQuestion: Count the number of items that are battery powered?\nQuestion: If two phones with three camera lenses were replaced with phones having two camera lenses, how many phones with two camera lenses would be present?\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  40%|████      | 10/25 [03:02<04:38, 18.54s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 11/25: image01\nQuestion: How many mammals are present in total?\nQuestion: How many objects are visible that can store items?\nQuestion: If the bear were to be replaced by a tree, how many different types of mammals would be there at the zoo?\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  44%|████▍     | 11/25 [03:20<04:18, 18.44s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 12/25: image02\nQuestion: How many kitchen tools are visible in the image?\nQuestion: Count the number of items that require electricity to operate?\nQuestion: If blinds were installed for the windows above the sink, how many transparent objects would remain?\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  48%|████▊     | 12/25 [03:38<03:57, 18.29s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 13/25: image03\nQuestion: How many objects made of glass are present?\nQuestion: How many tools are visible that can be used for cutting?\nQuestion: If the worker was not wearing ear protection, how many protective items would remain?\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  52%|█████▏    | 13/25 [04:02<03:59, 19.99s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 14/25: image04\nQuestion: How many objects made of rubber are present?\nQuestion: Excluding the drawers, how many items in the workshop serve as containers for storage?\nQuestion: If an electric fan were placed on the workstation to provide ventilation, how many objects in the room would require electricity to operate?\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  56%|█████▌    | 14/25 [04:20<03:34, 19.48s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 15/25: image05\nQuestion: How many birds are visible in the image?\nQuestion: How many objects are present that act as support?\nQuestion: If the clouds were to completely cover the sky, blocking the sunlight, how many natural elements would still be visible?\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  60%|██████    | 15/25 [04:44<03:29, 20.92s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 16/25: image06\nQuestion: How many objects are present that have chimneys?\nQuestion: How many objects are visible that are means of transportation?\nQuestion: If the bus were replaced by a pedestrian, how many mammals would be present?\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  64%|██████▍   | 16/25 [05:08<03:16, 21.85s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 17/25: image07\nQuestion: How many objects made of glass are present?\nQuestion: Count the number of items that can be used to carry liquid?\nQuestion: If the waste to be disposed was color-coded to match the bins, how many objects are to be thrown in the bin on the right?\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  68%|██████▊   | 17/25 [05:32<02:59, 22.49s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 18/25: image08\nQuestion: How many objects are present that have legs?\nQuestion: How many items are visible that are openable?\nQuestion: If the bottle was removed from the table, how many objects are present on top of the table?\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  72%|███████▏  | 18/25 [05:51<02:30, 21.47s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 19/25: image09\nQuestion: How many objects made of wood are present?\nQuestion: How many kitchen items are visible that can be used for cutting?\nQuestion: If the two jars on the top shelf were removed, how many breakable items would be present in the image?\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  76%|███████▌  | 19/25 [06:10<02:04, 20.71s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 20/25: image10\nQuestion: How many objects made of plastic are visible?\nQuestion: How many items are visible that can record audio?\nQuestion: If the microphones were replaced with headsets for every character, how many objects in total would be present that are worn on the head?\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  80%|████████  | 20/25 [06:29<01:40, 20.02s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 21/25: image01\nQuestion: How many objects made of rubber are visible?\nQuestion: How many objects are visible that are means of transportation?\nQuestion: If the car in the driveway were to leave, how many objects primarily made of metal would be present?\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  84%|████████▍ | 21/25 [06:54<01:26, 21.50s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 22/25: image02\nQuestion: How many objects made of concrete are present?\nQuestion: How many objects are visible that can be used for lifting?\nQuestion: If the orange paint spilled all over one of the plexiglass sheets, how many objects would remain that are transparent?\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  88%|████████▊ | 22/25 [07:18<01:07, 22.45s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 23/25: image03\nQuestion: How many mammals are present in the image?\nQuestion: How many objects are visible that are used for both meat and wool production?\nQuestion: If the two sheep were replaced by a cow grazing in the same area, how many objects would be present in between the two fences?\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  92%|█████████▏| 23/25 [07:43<00:46, 23.08s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 24/25: image04\nQuestion: How many objects are visible that are made of paper?\nQuestion: How many objects are present that behave as storage spaces?\nQuestion: If the glasses were placed inside the ceramic container, and we use this container as a dividing line between the left and right sides of the bookshelf, how many objects would be on the right side?\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  96%|█████████▌| 24/25 [08:02<00:21, 21.80s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 25/25: image05\nQuestion: How many objects are visible that are made of porcelain?\nQuestion: How many decoration items are present in the image?\nQuestion: If the drinks were split evenly between the two humans, how many drinks would each human consume?\n","output_type":"stream"},{"name":"stderr","text":"Processing images: 100%|██████████| 25/25 [08:20<00:00, 20.04s/it]\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"test_Qwen2_5VL()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T11:00:27.561431Z","iopub.execute_input":"2025-04-16T11:00:27.561723Z","iopub.status.idle":"2025-04-16T11:01:07.212699Z","shell.execute_reply.started":"2025-04-16T11:00:27.561701Z","shell.execute_reply":"2025-04-16T11:01:07.211928Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e3ae7a1d55448d49f039d4c068ae0bb"}},"metadata":{}},{"name":"stdout","text":"\nEvaluating Qwen2.5-VL...\nUsing device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Processing images:   0%|          | 0/2 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 1/2: image01\nQuestion: How many objects made of wood are present?\nQuestion: Count the number of breakable items?\nQuestion: If one of the metal objects were replaced by a wooden object, how many wooden objects would be there in the image?\n","output_type":"stream"},{"name":"stderr","text":"Processing images:  50%|█████     | 1/2 [00:13<00:13, 13.72s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 2/2: image02\nQuestion: How many mammals are present in the image?\nQuestion: Count the number of items that can store other items?\nQuestion: If one of the zebra were replaced by a tree, how many mammals would be present in the image?\n","output_type":"stream"},{"name":"stderr","text":"Processing images: 100%|██████████| 2/2 [00:33<00:00, 16.99s/it]\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}