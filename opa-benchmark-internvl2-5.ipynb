{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11436696,"sourceType":"datasetVersion","datasetId":7163706}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# VLM Benchmark for Object Property Abstraction\n\nThis notebook implements a benchmark for evaluating Vision Language Models (VLMs) on object property abstraction and visual question answering (VQA) tasks. The benchmark includes three types of questions:\n\n1. Direct Recognition\n2. Property Inference\n3. Counterfactual Reasoning\n\nAnd three types of images:\n- REAL\n- ANIMATED\n- AI GENERATED","metadata":{}},{"cell_type":"markdown","source":"## Setup and Imports\n\nFirst, let's import the necessary libraries and set up our environment.","metadata":{}},{"cell_type":"code","source":"# Install required packages\n!pip install transformers torch Pillow tqdm bitsandbytes accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T16:33:28.791327Z","iopub.execute_input":"2025-04-16T16:33:28.791609Z","iopub.status.idle":"2025-04-16T16:34:56.639077Z","shell.execute_reply.started":"2025-04-16T16:33:28.791589Z","shell.execute_reply":"2025-04-16T16:34:56.638370Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bitsandbytes-0.45.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"pip install num2words qwen-vl-utils #flash-attn --no-build-isolation ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T16:34:56.640635Z","iopub.execute_input":"2025-04-16T16:34:56.640928Z","iopub.status.idle":"2025-04-16T16:35:09.382661Z","shell.execute_reply.started":"2025-04-16T16:34:56.640905Z","shell.execute_reply":"2025-04-16T16:35:09.381771Z"}},"outputs":[{"name":"stdout","text":"Collecting num2words\n  Downloading num2words-0.5.14-py3-none-any.whl.metadata (13 kB)\nCollecting qwen-vl-utils\n  Downloading qwen_vl_utils-0.0.10-py3-none-any.whl.metadata (6.3 kB)\nCollecting docopt>=0.6.2 (from num2words)\n  Downloading docopt-0.6.2.tar.gz (25 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting av (from qwen-vl-utils)\n  Downloading av-14.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from qwen-vl-utils) (24.2)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from qwen-vl-utils) (11.1.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from qwen-vl-utils) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->qwen-vl-utils) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->qwen-vl-utils) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->qwen-vl-utils) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->qwen-vl-utils) (2025.1.31)\nDownloading num2words-0.5.14-py3-none-any.whl (163 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading qwen_vl_utils-0.0.10-py3-none-any.whl (6.7 kB)\nDownloading av-14.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.2/35.2 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: docopt\n  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=e7b3ff072ee2f8a3b99084997d1cfea8e445540fb4b0311bcf4479a02ba6232c\n  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\nSuccessfully built docopt\nInstalling collected packages: docopt, num2words, av, qwen-vl-utils\nSuccessfully installed av-14.3.0 docopt-0.6.2 num2words-0.5.14 qwen-vl-utils-0.0.10\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Import required libraries\nimport torch\nimport json\nfrom pathlib import Path\nfrom PIL import Image\nimport gc\nimport re\nfrom tqdm import tqdm\nfrom typing import List, Dict, Any\n\n# Check if CUDA is available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T16:35:09.383873Z","iopub.execute_input":"2025-04-16T16:35:09.384193Z","iopub.status.idle":"2025-04-16T16:35:14.582841Z","shell.execute_reply.started":"2025-04-16T16:35:09.384160Z","shell.execute_reply":"2025-04-16T16:35:14.582060Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import numpy as np\nimport torchvision.transforms as T\nfrom PIL import Image\nfrom torchvision.transforms.functional import InterpolationMode","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T16:35:14.584637Z","iopub.execute_input":"2025-04-16T16:35:14.585019Z","iopub.status.idle":"2025-04-16T16:35:18.507959Z","shell.execute_reply.started":"2025-04-16T16:35:14.584999Z","shell.execute_reply":"2025-04-16T16:35:18.507161Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Benchmark Tester Class\n\nThis class handles the evaluation of models against our benchmark.","metadata":{}},{"cell_type":"code","source":"class BenchmarkTester:\n    def __init__(self, benchmark_path=\"/kaggle/input/opabenchmark/benchmark.json\", data_dir=\"/kaggle/input/opabenchmark/data\"):\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        with open(benchmark_path, 'r') as f:\n            self.benchmark = json.load(f)\n        self.data_dir = data_dir\n\n    def clean_answer(self, answer):\n        \"\"\"Extract number and reasoning from the model's answer.\"\"\"\n        # Try to extract number and reasoning using regex\n        import re\n        pattern = r'(\\d+)\\s*\\[(.*?)\\]'\n        match = re.search(pattern, answer)\n        \n        if match:\n            number = match.group(1)\n            objects = [obj.strip() for obj in match.group(2).split(',')]\n            return {\n                \"count\": number,\n                \"reasoning\": objects\n            }\n        else:\n            # Fallback if format isn't matched\n            numbers = re.findall(r'\\d+', answer)\n            return {\n                \"count\": numbers[0] if numbers else \"0\",\n                \"reasoning\": []\n            }\n\n    IMAGENET_MEAN = (0.485, 0.456, 0.406)\n    IMAGENET_STD = (0.229, 0.224, 0.225)\n\n    def build_transform(self, input_size):\n        MEAN, STD = self.IMAGENET_MEAN, self.IMAGENET_STD\n        transform = T.Compose([\n            T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n            T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n            T.ToTensor(),\n            T.Normalize(mean=MEAN, std=STD)\n        ])\n        return transform\n\n    def find_closest_aspect_ratio(self, aspect_ratio, target_ratios, width, height, image_size):\n        best_ratio_diff = float('inf')\n        best_ratio = (1, 1)\n        area = width * height\n        for ratio in target_ratios:\n            target_aspect_ratio = ratio[0] / ratio[1]\n            ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n            if ratio_diff < best_ratio_diff:\n                best_ratio_diff = ratio_diff\n                best_ratio = ratio\n            elif ratio_diff == best_ratio_diff:\n                if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n                    best_ratio = ratio\n        return best_ratio\n\n    def dynamic_preprocess(self, image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n        orig_width, orig_height = image.size\n        aspect_ratio = orig_width / orig_height\n    \n        # calculate the existing image aspect ratio\n        target_ratios = set(\n            (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n            i * j <= max_num and i * j >= min_num)\n        target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n    \n        # find the closest aspect ratio to the target\n        target_aspect_ratio = self.find_closest_aspect_ratio(\n            aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n\n        # calculate the target width and height\n        target_width = image_size * target_aspect_ratio[0]\n        target_height = image_size * target_aspect_ratio[1]\n        blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n    \n        # resize the image\n        resized_img = image.resize((target_width, target_height))\n        processed_images = []\n        for i in range(blocks):\n            box = (\n                (i % (target_width // image_size)) * image_size,\n                (i // (target_width // image_size)) * image_size,\n                ((i % (target_width // image_size)) + 1) * image_size,\n                ((i // (target_width // image_size)) + 1) * image_size\n            )\n            # split the image\n            split_img = resized_img.crop(box)\n            processed_images.append(split_img)\n        assert len(processed_images) == blocks\n        if use_thumbnail and len(processed_images) != 1:\n            thumbnail_img = image.resize((image_size, image_size))\n            processed_images.append(thumbnail_img)\n        return processed_images\n\n    def load_image(self, image_file, input_size=448, max_num=12):\n        image = Image.open(image_file).convert('RGB')\n        transform = self.build_transform(input_size=input_size)\n        images = self.dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n        pixel_values = [transform(image) for image in images]\n        pixel_values = torch.stack(pixel_values)\n        return pixel_values\n    \n    def evaluate_model(self, model_name, model, processor, save_path, start_idx=0, batch_size=5):\n        results = []\n        print(f\"\\nEvaluating {model_name}...\")\n        print(f\"Using device: {self.device}\")\n        \n        # Force garbage collection before starting\n        gc.collect()\n        torch.cuda.empty_cache()\n\n        try:\n            images = self.benchmark['benchmark']['images'][start_idx:start_idx + batch_size]\n            total_images = len(images)\n            \n            for idx, image_data in enumerate(tqdm(images, desc=\"Processing images\")):\n                try:\n                    print(f\"\\nProcessing image {idx+1}/{total_images}: {image_data['image_id']}\")\n                    image_path = Path(self.data_dir)/image_data['path']\n                    if not image_path.exists():\n                        print(f\"Warning: Image not found at {image_path}\")\n                        continue\n                    \n                    # Load and preprocess image\n                    # image = Image.open(image_path).convert(\"RGB\")\n                    image_results = []  # Store results for current image\n                    \n                    for question in image_data['questions']:\n                        try:\n                            print(f\"Question: {question['question']}\")\n                            \n                            # Clear cache before processing each question\n                            torch.cuda.empty_cache()\n\n                            # set the max number of tiles in `max_num`\n                            \n                            pixel_values = self.load_image(image_path, max_num=12).to(torch.bfloat16).cuda()\n                            generation_config = dict(max_new_tokens=1024, do_sample=True)\n                            \n                            prompt = f'<image>\\n {question[\"question\"]} Provide just the total count and the list of objects in the given format \\n Format: number [objects]'# Answer with the total number(numerical) followed by the objects within square brackets' #Answer format: total number(numerical) objects(within square brackets)'\n                            answer = model.chat(processor, pixel_values, prompt, generation_config)\n                            \n                            cleaned_answer = self.clean_answer(answer)\n                            \n                            image_results.append({\n                                \"image_id\": image_data[\"image_id\"],\n                                \"image_type\": image_data[\"image_type\"],\n                                \"question_id\": question[\"id\"],\n                                \"question\": question[\"question\"],\n                                \"ground_truth\": question[\"answer\"],\n                                \"model_answer\": cleaned_answer[\"count\"],\n                                \"model_reasoning\": cleaned_answer[\"reasoning\"],\n                                \"raw_answer\": answer,  # Keep raw answer for debugging\n                                \"property_category\": question[\"property_category\"]\n                            })\n                            \n                            # Clear memory\n                            # del outputs, inputs\n                            torch.cuda.empty_cache()\n                            \n                        except Exception as e:\n                            print(f\"Error processing question: {str(e)}\")\n                            continue\n                    \n                    # Add results from this image\n                    results.extend(image_results)\n                    \n                    # Save intermediate results only every 2 images or if it's the last image\n                    if (idx + 1) % 2 == 0 or idx == total_images - 1:\n                        with open(f\"{save_path}_checkpoint.json\", 'w') as f:\n                            json.dump(results, f, indent=4)\n                            \n                except Exception as e:\n                    print(f\"Error processing image {image_data['image_id']}: {str(e)}\")\n                    continue\n            \n            # Save final results\n            if results:\n                with open(save_path, 'w') as f:\n                    json.dump(results, f, indent=4)\n            \n        except Exception as e:\n            print(f\"An error occurred during evaluation: {str(e)}\")\n            if results:\n                with open(f\"{save_path}_error_state.json\", 'w') as f:\n                    json.dump(results, f, indent=4)\n        \n        return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T16:35:18.508812Z","iopub.execute_input":"2025-04-16T16:35:18.509103Z","iopub.status.idle":"2025-04-16T16:35:18.529152Z","shell.execute_reply.started":"2025-04-16T16:35:18.509085Z","shell.execute_reply":"2025-04-16T16:35:18.528576Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import os\nprint(os.listdir(\"/kaggle/input/opabenchmark/\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T16:35:18.530083Z","iopub.execute_input":"2025-04-16T16:35:18.530332Z","iopub.status.idle":"2025-04-16T16:35:18.570284Z","shell.execute_reply.started":"2025-04-16T16:35:18.530316Z","shell.execute_reply":"2025-04-16T16:35:18.569764Z"}},"outputs":[{"name":"stdout","text":"['data', 'benchmark.json']\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Test InternVL2.5\nLet's evaluate the InternVL2_5-4B-MPO model","metadata":{}},{"cell_type":"code","source":"def split_model(model_name):\n    import math\n    device_map = {}\n    world_size = torch.cuda.device_count()\n    num_layers = {\n        'InternVL2_5-1B': 24, 'InternVL2_5-2B': 24, 'InternVL2_5-4B': 36, 'InternVL2_5-8B': 32,\n        'InternVL2_5-26B': 48, 'InternVL2_5-38B': 64, 'InternVL2_5-78B': 80}[model_name]\n    # Since the first GPU will be used for ViT, treat it as half a GPU.\n    num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n    num_layers_per_gpu = [num_layers_per_gpu] * world_size\n    num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\n    layer_cnt = 0\n    for i, num_layer in enumerate(num_layers_per_gpu):\n        for j in range(num_layer):\n            device_map[f'language_model.model.layers.{layer_cnt}'] = i\n            layer_cnt += 1\n    device_map['vision_model'] = 0\n    device_map['mlp1'] = 0\n    device_map['language_model.model.tok_embeddings'] = 0\n    device_map['language_model.model.embed_tokens'] = 0\n    device_map['language_model.output'] = 0\n    device_map['language_model.model.norm'] = 0\n    device_map['language_model.model.rotary_emb'] = 0\n    device_map['language_model.lm_head'] = 0\n    device_map[f'language_model.model.layers.{num_layers - 1}'] = 0\n\n    return device_map","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T16:35:18.570950Z","iopub.execute_input":"2025-04-16T16:35:18.571183Z","iopub.status.idle":"2025-04-16T16:35:18.583880Z","shell.execute_reply.started":"2025-04-16T16:35:18.571165Z","shell.execute_reply":"2025-04-16T16:35:18.583168Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def test_InternVL2_5():\n    import torch\n    from transformers import AutoTokenizer, AutoModel\n\n    device_map = split_model('InternVL2_5-4B')\n    \n    model = AutoModel.from_pretrained(\n        \"OpenGVLab/InternVL2_5-4B-MPO\",\n        torch_dtype=torch.bfloat16,\n        load_in_8bit=True,\n        low_cpu_mem_usage=True,\n        # use_flash_attn=True,\n        trust_remote_code=True).eval()\n    \n    tokenizer = AutoTokenizer.from_pretrained(\"OpenGVLab/InternVL2_5-4B-MPO\", trust_remote_code=True, use_fast=False)\n\n    ## InternVL2.5-4B --> performs decently well. slight post processing required\n    \n    # Optional: Enable memory efficient attention\n    if hasattr(model.config, 'use_memory_efficient_attention'):\n        model.config.use_memory_efficient_attention = True\n\n    tester = BenchmarkTester()\n    InternVL2_5_results = tester.evaluate_model(\n        \"InternVL2.5\",\n        model, \n        tokenizer, \n        \"InternVL2.5_results.json\", \n        batch_size=25\n    )\n\n    # Clean up\n    del model, tokenizer\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T16:35:18.584762Z","iopub.execute_input":"2025-04-16T16:35:18.585011Z","iopub.status.idle":"2025-04-16T16:35:18.605814Z","shell.execute_reply.started":"2025-04-16T16:35:18.584985Z","shell.execute_reply":"2025-04-16T16:35:18.605125Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Run Evaluation\n\nNow we can run our evaluation. Let's start with the InternVL2.5 model:","metadata":{}},{"cell_type":"code","source":"test_InternVL2_5() #8.59 #9.07","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T16:35:26.953977Z","iopub.execute_input":"2025-04-16T16:35:26.954995Z","iopub.status.idle":"2025-04-16T16:43:32.673597Z","shell.execute_reply.started":"2025-04-16T16:35:26.954949Z","shell.execute_reply":"2025-04-16T16:43:32.672973Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/3.74k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ec56058ec774f4091e32ba2542cd424"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_internvl_chat.py:   0%|          | 0.00/4.04k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a991f96579ee4597900d68fb193c601e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_intern_vit.py:   0%|          | 0.00/5.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a608f663d2054fb7af39a5356a2c9551"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/OpenGVLab/InternVL2_5-4B-MPO:\n- configuration_intern_vit.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/OpenGVLab/InternVL2_5-4B-MPO:\n- configuration_internvl_chat.py\n- configuration_intern_vit.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_internvl_chat.py:   0%|          | 0.00/15.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b20804d551d347b1a8b2cd3508a20402"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modeling_intern_vit.py:   0%|          | 0.00/18.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9dbbed25dde14b84aa1515dcad421896"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/OpenGVLab/InternVL2_5-4B-MPO:\n- modeling_intern_vit.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"conversation.py:   0%|          | 0.00/15.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7814df9d07414d3da054478826202d13"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/OpenGVLab/InternVL2_5-4B-MPO:\n- conversation.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/OpenGVLab/InternVL2_5-4B-MPO:\n- modeling_internvl_chat.py\n- modeling_intern_vit.py\n- conversation.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n2025-04-16 16:35:42.509952: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1744821342.926251      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1744821343.057282      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\nThe `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"name":"stdout","text":"FlashAttention2 is not installed.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/71.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"496c97ab8f2e4c55919a4f26724eae31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21de1c77317f4cd99a5364cd24794ddd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.43G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0786d24789a8487388d699675d08d3da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2479d8f77aea498f901c41b67ba24226"}},"metadata":{}},{"name":"stderr","text":"Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"557d2e2b2e6b4327a93ed4737957ed43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/129 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03cbfbbb808d4853b19a8f9b51339082"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/9.02k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99fd0578d6c947cd842155310b933cbe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/3.38M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea930c9072f64a558ede66a7673a0c11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1586aa6d27740208c68ea6d04f80347"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/790 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95eb0333471e44b3a5c20af73a27b83f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/744 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c84fdb98e20f40138984c1e26f1fb578"}},"metadata":{}},{"name":"stdout","text":"\nEvaluating InternVL2.5...\nUsing device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Processing images:   0%|          | 0/25 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 1/25: image01\nQuestion: How many objects made of wood are present?\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\nSetting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: Count the number of breakable items?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: If one of the metal objects were replaced by a wooden object, how many wooden objects would be there in the image?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\nProcessing images:   4%|▍         | 1/25 [00:15<06:04, 15.18s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 2/25: image02\nQuestion: How many mammals are present in the image?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: Count the number of items that can store other items?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: If one of the zebra were replaced by a tree, how many mammals would be present in the image?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\nProcessing images:   8%|▊         | 2/25 [00:28<05:19, 13.89s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 3/25: image03\nQuestion: How many objects made of rubber are present?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: How many objects with the primary purpose of illumination can be seen?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: If the person riding one of the bicycles were replaced by a pedestrian, how many objects that have handles would be present?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\nProcessing images:  12%|█▏        | 3/25 [00:44<05:34, 15.18s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 4/25: image04\nQuestion: How many tools are visible in the image?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: How many cutting tools are present in this image?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: If the red handle were replaced by a wooden handle, how many colored artifacts would remain in the image?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\nProcessing images:  16%|█▌        | 4/25 [01:09<06:34, 18.80s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 5/25: image05\nQuestion: How many furniture items are present that have legs?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: Count the number of containers that cannot hold hot liquids?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: If the room were transformed into an open workspace instead of a meeting room, how many privacy features would need to be removed?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\nProcessing images:  20%|██        | 5/25 [01:22<05:35, 16.79s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 6/25: image06\nQuestion: How many reptiles are visible in this enclosure?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: How many reptilian couples, at maximum, are present?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: If all the small pebbles forming the mosaic floor were replaced with sand, how many natural elements would still be visible in the enclosure?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\nProcessing images:  24%|██▍       | 6/25 [01:35<04:53, 15.44s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 7/25: image07\nQuestion: How many birds are visible in this image?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: How many objects are present that can comfortably seat a human?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: If the birds sitting together only on one railing were to fly away, how many birds would remain?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\nProcessing images:  28%|██▊       | 7/25 [01:50<04:33, 15.22s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 8/25: image08\nQuestion: How many reptiles are visible in this image?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: How many objects are present that act as support?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: If one turtle slid off the log into the water, how many turtles would be in the water?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\nProcessing images:  32%|███▏      | 8/25 [02:01<03:59, 14.11s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 9/25: image09\nQuestion: How many different types of vegetables are present in the image?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: How many objects are used as containers?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: If the bag of limes were removed and replaced with two additional avocados, how many fruits would be present in total on the table, considering avocados are fruits?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\nProcessing images:  36%|███▌      | 9/25 [02:19<04:01, 15.10s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 10/25: image10\nQuestion: How many objects are present that are flexible?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: Count the number of items that are battery powered?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: If two phones with three camera lenses were replaced with phones having two camera lenses, how many phones with two camera lenses would be present?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\nProcessing images:  40%|████      | 10/25 [02:45<04:41, 18.74s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 11/25: image01\nQuestion: How many mammals are present in total?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: How many objects are visible that can store items?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: If the bear were to be replaced by a tree, how many different types of mammals would be there at the zoo?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\nProcessing images:  44%|████▍     | 11/25 [03:11<04:50, 20.72s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 12/25: image02\nQuestion: How many kitchen tools are visible in the image?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: Count the number of items that require electricity to operate?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: If blinds were installed for the windows above the sink, how many transparent objects would remain?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\nProcessing images:  48%|████▊     | 12/25 [03:18<03:37, 16.70s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 13/25: image03\nQuestion: How many objects made of glass are present?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: How many tools are visible that can be used for cutting?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: If the worker was not wearing ear protection, how many protective items would remain?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\nProcessing images:  52%|█████▏    | 13/25 [03:28<02:55, 14.59s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 14/25: image04\nQuestion: How many objects made of rubber are present?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: Excluding the drawers, how many items in the workshop serve as containers for storage?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: If an electric fan were placed on the workstation to provide ventilation, how many objects in the room would require electricity to operate?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\nProcessing images:  56%|█████▌    | 14/25 [03:40<02:32, 13.91s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 15/25: image05\nQuestion: How many birds are visible in the image?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: How many objects are present that act as support?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: If the clouds were to completely cover the sky, blocking the sunlight, how many natural elements would still be visible?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\nProcessing images:  60%|██████    | 15/25 [03:54<02:19, 13.95s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 16/25: image06\nQuestion: How many objects are present that have chimneys?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: How many objects are visible that are means of transportation?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: If the bus were replaced by a pedestrian, how many mammals would be present?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\nProcessing images:  64%|██████▍   | 16/25 [04:05<01:55, 12.88s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 17/25: image07\nQuestion: How many objects made of glass are present?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: Count the number of items that can be used to carry liquid?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: If the waste to be disposed was color-coded to match the bins, how many objects are to be thrown in the bin on the right?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\nProcessing images:  68%|██████▊   | 17/25 [04:14<01:34, 11.81s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 18/25: image08\nQuestion: How many objects are present that have legs?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: How many items are visible that are openable?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: If the bottle was removed from the table, how many objects are present on top of the table?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\nProcessing images:  72%|███████▏  | 18/25 [04:31<01:32, 13.27s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 19/25: image09\nQuestion: How many objects made of wood are present?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: How many kitchen items are visible that can be used for cutting?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: If the two jars on the top shelf were removed, how many breakable items would be present in the image?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\nProcessing images:  76%|███████▌  | 19/25 [04:56<01:40, 16.79s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 20/25: image10\nQuestion: How many objects made of plastic are visible?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: How many items are visible that can record audio?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: If the microphones were replaced with headsets for every character, how many objects in total would be present that are worn on the head?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\nProcessing images:  80%|████████  | 20/25 [05:03<01:10, 14.10s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 21/25: image01\nQuestion: How many objects made of rubber are visible?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: How many objects are visible that are means of transportation?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: If the car in the driveway were to leave, how many objects primarily made of metal would be present?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\nProcessing images:  84%|████████▍ | 21/25 [05:22<01:01, 15.32s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 22/25: image02\nQuestion: How many objects made of concrete are present?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: How many objects are visible that can be used for lifting?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: If the orange paint spilled all over one of the plexiglass sheets, how many objects would remain that are transparent?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\nProcessing images:  88%|████████▊ | 22/25 [05:40<00:48, 16.27s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 23/25: image03\nQuestion: How many mammals are present in the image?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: How many objects are visible that are used for both meat and wool production?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: If the two sheep were replaced by a cow grazing in the same area, how many objects would be present in between the two fences?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\nProcessing images:  92%|█████████▏| 23/25 [06:01<00:35, 17.54s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 24/25: image04\nQuestion: How many objects are visible that are made of paper?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: How many objects are present that behave as storage spaces?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: If the glasses were placed inside the ceramic container, and we use this container as a dividing line between the left and right sides of the bookshelf, how many objects would be on the right side?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\nProcessing images:  96%|█████████▌| 24/25 [06:16<00:16, 16.84s/it]","output_type":"stream"},{"name":"stdout","text":"\nProcessing image 25/25: image05\nQuestion: How many objects are visible that are made of porcelain?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: How many decoration items are present in the image?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Question: If the drinks were split evenly between the two humans, how many drinks would each human consume?\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\nProcessing images: 100%|██████████| 25/25 [06:43<00:00, 16.13s/it]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}