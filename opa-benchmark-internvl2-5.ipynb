{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLM Benchmark for Object Property Abstraction\n",
    "\n",
    "This notebook implements a benchmark for evaluating Vision Language Models (VLMs) on object property abstraction and visual question answering (VQA) tasks. The benchmark includes three types of questions:\n",
    "\n",
    "1. Direct Recognition\n",
    "2. Property Inference\n",
    "3. Counterfactual Reasoning\n",
    "\n",
    "And three types of images:\n",
    "- REAL\n",
    "- ANIMATED\n",
    "- AI GENERATED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T16:33:28.791609Z",
     "iopub.status.busy": "2025-04-16T16:33:28.791327Z",
     "iopub.status.idle": "2025-04-16T16:34:56.639077Z",
     "shell.execute_reply": "2025-04-16T16:34:56.638370Z",
     "shell.execute_reply.started": "2025-04-16T16:33:28.791589Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# %pip install transformers torch Pillow tqdm bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T16:34:56.640928Z",
     "iopub.status.busy": "2025-04-16T16:34:56.640635Z",
     "iopub.status.idle": "2025-04-16T16:35:09.382661Z",
     "shell.execute_reply": "2025-04-16T16:35:09.381771Z",
     "shell.execute_reply.started": "2025-04-16T16:34:56.640905Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%pip install flash-attn #--no-build-isolation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T16:35:09.384193Z",
     "iopub.status.busy": "2025-04-16T16:35:09.383873Z",
     "iopub.status.idle": "2025-04-16T16:35:14.582841Z",
     "shell.execute_reply": "2025-04-16T16:35:14.582060Z",
     "shell.execute_reply.started": "2025-04-16T16:35:09.384160Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import gc\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from typing import Dict\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T16:35:14.585019Z",
     "iopub.status.busy": "2025-04-16T16:35:14.584637Z",
     "iopub.status.idle": "2025-04-16T16:35:18.507959Z",
     "shell.execute_reply": "2025-04-16T16:35:18.507161Z",
     "shell.execute_reply.started": "2025-04-16T16:35:14.584999Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Tester Class\n",
    "\n",
    "This class handles the evaluation of models against our benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T16:35:18.509103Z",
     "iopub.status.busy": "2025-04-16T16:35:18.508812Z",
     "iopub.status.idle": "2025-04-16T16:35:18.529152Z",
     "shell.execute_reply": "2025-04-16T16:35:18.528576Z",
     "shell.execute_reply.started": "2025-04-16T16:35:18.509085Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BenchmarkTester:\n",
    "    def __init__(self, benchmark_path=\"/var/scratch/ave303/OP_bench/benchmark.json\", data_dir=\"/var/scratch/ave303/OP_bench/\"):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        with open(benchmark_path, 'r') as f:\n",
    "            self.benchmark = json.load(f)\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "    def clean_answer(self, answer):\n",
    "        \"\"\"Extract number and reasoning from the model's answer.\"\"\"\n",
    "        # Try to extract number and reasoning using regex\n",
    "        import re\n",
    "        pattern = r'(\\d+)\\s*\\[(.*?)\\]'\n",
    "        match = re.search(pattern, answer)\n",
    "        \n",
    "        if match:\n",
    "            number = match.group(1)\n",
    "            objects = [obj.strip() for obj in match.group(2).split(',')]\n",
    "            return {\n",
    "                \"count\": number,\n",
    "                \"reasoning\": objects\n",
    "            }\n",
    "        else:\n",
    "            # Fallback if format isn't matched\n",
    "            numbers = re.findall(r'\\d+', answer)\n",
    "            return {\n",
    "                \"count\": numbers[0] if numbers else \"0\",\n",
    "                \"reasoning\": []\n",
    "            }\n",
    "\n",
    "    # IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "    # IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "    IMAGENET_MEAN = (0.5, 0.5, 0.5)\n",
    "    IMAGENET_STD = (0.5, 0.5, 0.5)\n",
    "\n",
    "\n",
    "    def build_transform(self, input_size):\n",
    "        MEAN, STD = self.IMAGENET_MEAN, self.IMAGENET_STD\n",
    "        transform = T.Compose([\n",
    "            T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "            T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=MEAN, std=STD)\n",
    "        ])\n",
    "        return transform\n",
    "\n",
    "    def find_closest_aspect_ratio(self, aspect_ratio, target_ratios, width, height, image_size):\n",
    "        best_ratio_diff = float('inf')\n",
    "        best_ratio = (1, 1)\n",
    "        area = width * height\n",
    "        for ratio in target_ratios:\n",
    "            target_aspect_ratio = ratio[0] / ratio[1]\n",
    "            ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "            if ratio_diff < best_ratio_diff:\n",
    "                best_ratio_diff = ratio_diff\n",
    "                best_ratio = ratio\n",
    "            elif ratio_diff == best_ratio_diff:\n",
    "                if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                    best_ratio = ratio\n",
    "        return best_ratio\n",
    "\n",
    "    def dynamic_preprocess(self, image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "        orig_width, orig_height = image.size\n",
    "        aspect_ratio = orig_width / orig_height\n",
    "    \n",
    "        # calculate the existing image aspect ratio\n",
    "        target_ratios = set(\n",
    "            (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "            i * j <= max_num and i * j >= min_num)\n",
    "        target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "    \n",
    "        # find the closest aspect ratio to the target\n",
    "        target_aspect_ratio = self.find_closest_aspect_ratio(\n",
    "            aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "        # calculate the target width and height\n",
    "        target_width = image_size * target_aspect_ratio[0]\n",
    "        target_height = image_size * target_aspect_ratio[1]\n",
    "        blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "    \n",
    "        # resize the image\n",
    "        resized_img = image.resize((target_width, target_height))\n",
    "        processed_images = []\n",
    "        for i in range(blocks):\n",
    "            box = (\n",
    "                (i % (target_width // image_size)) * image_size,\n",
    "                (i // (target_width // image_size)) * image_size,\n",
    "                ((i % (target_width // image_size)) + 1) * image_size,\n",
    "                ((i // (target_width // image_size)) + 1) * image_size\n",
    "            )\n",
    "            # split the image\n",
    "            split_img = resized_img.crop(box)\n",
    "            processed_images.append(split_img)\n",
    "        assert len(processed_images) == blocks\n",
    "        if use_thumbnail and len(processed_images) != 1:\n",
    "            thumbnail_img = image.resize((image_size, image_size))\n",
    "            processed_images.append(thumbnail_img)\n",
    "        return processed_images\n",
    "\n",
    "    def load_image(self, image_file, input_size=448, max_num=12):     #internvl -> 448, 12 || ristretto -> 384, 10\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "        transform = self.build_transform(input_size=input_size)\n",
    "        images = self.dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "        pixel_values = [transform(image) for image in images]\n",
    "        pixel_values = torch.stack(pixel_values)\n",
    "        return pixel_values\n",
    "    \n",
    "    def evaluate_model(self, model_name, model, processor, save_path, start_idx=0, batch_size=5):\n",
    "        results = []\n",
    "        print(f\"\\nEvaluating {model_name}...\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Force garbage collection before starting\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        try:\n",
    "            images = self.benchmark['benchmark']['images'][start_idx:start_idx + batch_size]\n",
    "            total_images = len(images)\n",
    "            \n",
    "            for idx, image_data in enumerate(tqdm(images, desc=\"Processing images\")):\n",
    "                try:\n",
    "                    print(f\"\\nProcessing image {idx+1}/{total_images}: {image_data['image_id']}\")\n",
    "                    image_path = Path(self.data_dir)/image_data['path']\n",
    "                    if not image_path.exists():\n",
    "                        print(f\"Warning: Image not found at {image_path}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Load and preprocess image\n",
    "                    # image = Image.open(image_path).convert(\"RGB\")\n",
    "                    image_results = []  # Store results for current image\n",
    "                    \n",
    "                    for question in image_data['questions']:\n",
    "                        try:\n",
    "                            print(f\"Question: {question['question']}\")\n",
    "                            \n",
    "                            # Clear cache before processing each question\n",
    "                            torch.cuda.empty_cache()\n",
    "\n",
    "                            # set the max number of tiles in `max_num`\n",
    "                            \n",
    "                            pixel_values = self.load_image(image_path, max_num=12).to(torch.float16).cuda()\n",
    "                            generation_config = dict(max_new_tokens=1024, do_sample=True)\n",
    "                            # prompt = f'<image>\\n {question[\"question\"]} Provide the total count of the objects and then list the objects, separated by commas. \\n Format: <number> [<object1>, <object2>, <object3>, ...]'\n",
    "                            # Answer with the total number(numerical) followed by the objects within square brackets' #Answer format: total number(numerical) objects(within square brackets)'\n",
    "                            prompt = f'<image>\\n {question[\"question\"]} Your response MUST be in the following format and nothing else:\\n <NUMBER> [<OBJECT1>, <OBJECT2>, <OBJECT3>, ...]'\n",
    "                            # prompt = f'<image>\\n {question[\"question\"]} Answer format: total count  [list of objects]'\n",
    "                            answer = model.chat(processor, pixel_values, prompt, generation_config)\n",
    "                            \n",
    "                            cleaned_answer = self.clean_answer(answer)\n",
    "                            \n",
    "                            image_results.append({\n",
    "                                \"image_id\": image_data[\"image_id\"],\n",
    "                                \"image_type\": image_data[\"image_type\"],\n",
    "                                \"question_id\": question[\"id\"],\n",
    "                                \"question\": question[\"question\"],\n",
    "                                \"ground_truth\": question[\"answer\"],\n",
    "                                \"model_answer\": cleaned_answer[\"count\"],\n",
    "                                \"model_reasoning\": cleaned_answer[\"reasoning\"],\n",
    "                                \"raw_answer\": answer,  # Keep raw answer for debugging\n",
    "                                \"property_category\": question[\"property_category\"]\n",
    "                            })\n",
    "                            \n",
    "                            # Clear memory\n",
    "                            # del outputs, inputs\n",
    "                            torch.cuda.empty_cache()\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing question: {str(e)}\")\n",
    "                            continue\n",
    "                    \n",
    "                    # Add results from this image\n",
    "                    results.extend(image_results)\n",
    "                    \n",
    "                    # Save intermediate results only every 2 images or if it's the last image\n",
    "                    if (idx + 1) % 2 == 0 or idx == total_images - 1:\n",
    "                        with open(f\"{save_path}_checkpoint.json\", 'w') as f:\n",
    "                            json.dump(results, f, indent=4)\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing image {image_data['image_id']}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            # Save final results\n",
    "            if results:\n",
    "                with open(save_path, 'w') as f:\n",
    "                    json.dump(results, f, indent=4)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during evaluation: {str(e)}\")\n",
    "            if results:\n",
    "                with open(f\"{save_path}_error_state.json\", 'w') as f:\n",
    "                    json.dump(results, f, indent=4)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test InternVL2.5\n",
    "Let's evaluate the InternVL2_5-4B-MPO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T16:35:18.571183Z",
     "iopub.status.busy": "2025-04-16T16:35:18.570950Z",
     "iopub.status.idle": "2025-04-16T16:35:18.583880Z",
     "shell.execute_reply": "2025-04-16T16:35:18.583168Z",
     "shell.execute_reply.started": "2025-04-16T16:35:18.571165Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def split_model(model_name):\n",
    "#     import math\n",
    "#     device_map = {}\n",
    "#     world_size = torch.cuda.device_count()\n",
    "#     config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n",
    "#     num_layers = config.llm_config.num_hidden_layers\n",
    "#     # num_layers = {\n",
    "#     #     'InternVL2_5-1B': 24, 'InternVL2_5-2B': 24, 'InternVL2_5-4B': 36, 'InternVL2_5-8B': 32,\n",
    "#     #     'InternVL2_5-26B': 48, 'InternVL2_5-38B': 64, 'InternVL2_5-78B': 80}[model_name]\n",
    "#     # Since the first GPU will be used for ViT, treat it as half a GPU.\n",
    "#     num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n",
    "#     num_layers_per_gpu = [num_layers_per_gpu] * world_size\n",
    "#     num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\n",
    "#     layer_cnt = 0\n",
    "#     for i, num_layer in enumerate(num_layers_per_gpu):\n",
    "#         for j in range(num_layer):\n",
    "#             device_map[f'language_model.model.layers.{layer_cnt}'] = i\n",
    "#             layer_cnt += 1\n",
    "#     device_map['vision_model'] = 0\n",
    "#     device_map['mlp1'] = 0\n",
    "#     device_map['language_model.model.tok_embeddings'] = 0\n",
    "#     device_map['language_model.model.embed_tokens'] = 0\n",
    "#     device_map['language_model.output'] = 0\n",
    "#     device_map['language_model.model.norm'] = 0\n",
    "#     device_map['language_model.model.rotary_emb'] = 0\n",
    "#     device_map['language_model.lm_head'] = 0\n",
    "#     device_map[f'language_model.model.layers.{num_layers - 1}'] = 0\n",
    "\n",
    "#     return device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T16:35:18.585011Z",
     "iopub.status.busy": "2025-04-16T16:35:18.584762Z",
     "iopub.status.idle": "2025-04-16T16:35:18.605814Z",
     "shell.execute_reply": "2025-04-16T16:35:18.605125Z",
     "shell.execute_reply.started": "2025-04-16T16:35:18.584985Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test_InternVL2_5():\n",
    "    import torch\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "    # device_map = split_model('InternVL2_5-4B')\n",
    "    \n",
    "    model = AutoModel.from_pretrained(\n",
    "        \"/var/scratch/ave303/models/internvl2.5-8b\",\n",
    "        torch_dtype=torch.float16,\n",
    "        # load_in_8bit=False,\n",
    "        low_cpu_mem_usage=True,\n",
    "        use_flash_attn=True,\n",
    "        trust_remote_code=True).to('cuda').eval()\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"/var/scratch/ave303/models/internvl2.5-8b\", trust_remote_code=True, use_fast=False)\n",
    "\n",
    "    ## InternVL2.5-4B --> performs decently well. slight post processing required\n",
    "    \n",
    "    # Optional: Enable memory efficient attention\n",
    "    if hasattr(model.config, 'use_memory_efficient_attention'):\n",
    "        model.config.use_memory_efficient_attention = True\n",
    "\n",
    "    tester = BenchmarkTester()\n",
    "    InternVL2_5_results = tester.evaluate_model(\n",
    "        \"InternVL2.5\",\n",
    "        model, \n",
    "        tokenizer, \n",
    "        \"InternVL2.5_results_8bMPO.json\", \n",
    "        batch_size=50\n",
    "    )\n",
    "\n",
    "    # Clean up\n",
    "    del model, tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_InternVL3():\n",
    "    import torch\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "    # device_map = split_model('InternVL3-8B')\n",
    "    \n",
    "    model = AutoModel.from_pretrained(\n",
    "        \"/var/scratch/ave303/models/internvl3-14b\",\n",
    "        torch_dtype=torch.float16,\n",
    "        load_in_8bit=False,\n",
    "        low_cpu_mem_usage=True,\n",
    "        use_flash_attn=True,\n",
    "        # device_map=device_map,\n",
    "        trust_remote_code=True).to('cuda').eval()\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"/var/scratch/ave303/models/internvl3-14b\", trust_remote_code=True, use_fast=False)\n",
    "\n",
    "    ## InternVL2.5-4B --> performs decently well. slight post processing required\n",
    "    \n",
    "    # Optional: Enable memory efficient attention\n",
    "    if hasattr(model.config, 'use_memory_efficient_attention'):\n",
    "        model.config.use_memory_efficient_attention = True\n",
    "\n",
    "    tester = BenchmarkTester()\n",
    "    InternVL3_results = tester.evaluate_model(\n",
    "        \"InternVL3\",\n",
    "        model, \n",
    "        tokenizer, \n",
    "        \"InternVL3_results_14b.json\", \n",
    "        batch_size=50\n",
    "    )\n",
    "\n",
    "    # Clean up\n",
    "    del model, tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_Ristretto():\n",
    "#     import torch\n",
    "#     from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "#     # device_map = split_model('InternVL3-8B')\n",
    "    \n",
    "#     model = AutoModel.from_pretrained(\n",
    "#         \"/var/scratch/ave303/models/ristretto-3b\",\n",
    "#         torch_dtype=torch.float16,\n",
    "#         low_cpu_mem_usage=True,\n",
    "#         # device_map=device_map,\n",
    "#         trust_remote_code=True).to('cuda').eval()\n",
    "    \n",
    "#     tokenizer = AutoTokenizer.from_pretrained(\"/var/scratch/ave303/models/ristretto-3b\", trust_remote_code=True, use_fast=False)\n",
    "\n",
    "#     ## InternVL2.5-4B --> performs decently well. slight post processing required\n",
    "    \n",
    "#     # Optional: Enable memory efficient attention\n",
    "#     if hasattr(model.config, 'use_memory_efficient_attention'):\n",
    "#         model.config.use_memory_efficient_attention = True\n",
    "\n",
    "#     tester = BenchmarkTester()\n",
    "#     Ristretto_results = tester.evaluate_model(\n",
    "#         \"Ristretto-3b\",\n",
    "#         model, \n",
    "#         tokenizer, \n",
    "#         \"Ristretto_3b_results.json\", \n",
    "#         batch_size=25\n",
    "#     )\n",
    "\n",
    "#     # Clean up\n",
    "#     del model, tokenizer\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation\n",
    "\n",
    "Now we can run our evaluation. Let's start with the InternVL2.5 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T16:35:26.954995Z",
     "iopub.status.busy": "2025-04-16T16:35:26.953977Z",
     "iopub.status.idle": "2025-04-16T16:43:32.673597Z",
     "shell.execute_reply": "2025-04-16T16:43:32.672973Z",
     "shell.execute_reply.started": "2025-04-16T16:35:26.954949Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# test_InternVL2_5() #8.59 #9.07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_InternVL3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_Ristretto()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7163706,
     "sourceId": 11436696,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
