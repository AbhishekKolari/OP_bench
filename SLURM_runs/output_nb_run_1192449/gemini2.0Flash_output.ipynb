{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdead5bb",
   "metadata": {
    "papermill": {
     "duration": 0.005263,
     "end_time": "2025-06-10T10:56:18.496724",
     "exception": false,
     "start_time": "2025-06-10T10:56:18.491461",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# VLM Benchmark for Object Property Abstraction\n",
    "\n",
    "This notebook implements a benchmark for evaluating Vision Language Models (VLMs) on object property abstraction and visual question answering (VQA) tasks. The benchmark includes three types of questions:\n",
    "\n",
    "1. Direct Recognition\n",
    "2. Property Inference\n",
    "3. Counterfactual Reasoning\n",
    "\n",
    "And three types of images:\n",
    "- REAL\n",
    "- ANIMATED\n",
    "- AI GENERATED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2054ebe9",
   "metadata": {
    "papermill": {
     "duration": 0.004187,
     "end_time": "2025-06-10T10:56:18.505622",
     "exception": false,
     "start_time": "2025-06-10T10:56:18.501435",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5157a60",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-10T10:56:18.514737Z",
     "iopub.status.busy": "2025-06-10T10:56:18.514456Z",
     "iopub.status.idle": "2025-06-10T10:56:18.518714Z",
     "shell.execute_reply": "2025-06-10T10:56:18.517984Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 0.010239,
     "end_time": "2025-06-10T10:56:18.519946",
     "exception": false,
     "start_time": "2025-06-10T10:56:18.509707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# %pip install transformers torch Pillow tqdm bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dccf8a7c",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-10T10:56:18.528887Z",
     "iopub.status.busy": "2025-06-10T10:56:18.528658Z",
     "iopub.status.idle": "2025-06-10T10:56:20.356007Z",
     "shell.execute_reply": "2025-06-10T10:56:20.355018Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 1.833402,
     "end_time": "2025-06-10T10:56:20.357407",
     "exception": false,
     "start_time": "2025-06-10T10:56:18.524005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install qwen-vl-utils flash-attn #--no-build-isolation\n",
    "%pip install -q -U google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d2356f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T10:56:20.368133Z",
     "iopub.status.busy": "2025-06-10T10:56:20.367855Z",
     "iopub.status.idle": "2025-06-10T10:56:25.390480Z",
     "shell.execute_reply": "2025-06-10T10:56:25.389337Z"
    },
    "papermill": {
     "duration": 5.029304,
     "end_time": "2025-06-10T10:56:25.391911",
     "exception": false,
     "start_time": "2025-06-10T10:56:20.362607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import gc\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Any\n",
    "from google import genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15391e87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T10:56:25.403612Z",
     "iopub.status.busy": "2025-06-10T10:56:25.402290Z",
     "iopub.status.idle": "2025-06-10T10:56:25.407947Z",
     "shell.execute_reply": "2025-06-10T10:56:25.407199Z"
    },
    "papermill": {
     "duration": 0.012113,
     "end_time": "2025-06-10T10:56:25.409126",
     "exception": false,
     "start_time": "2025-06-10T10:56:25.397013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get API key from environment\n",
    "api_key = os.getenv('GOOGLE_API_KEY')\n",
    "if not api_key:\n",
    "    raise ValueError(\"GOOGLE_API_KEY not found in environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ca2b9a",
   "metadata": {
    "papermill": {
     "duration": 0.004284,
     "end_time": "2025-06-10T10:56:25.417705",
     "exception": false,
     "start_time": "2025-06-10T10:56:25.413421",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Benchmark Tester Class\n",
    "\n",
    "This class handles the evaluation of models against our benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28935166",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T10:56:25.427195Z",
     "iopub.status.busy": "2025-06-10T10:56:25.426907Z",
     "iopub.status.idle": "2025-06-10T10:56:25.449484Z",
     "shell.execute_reply": "2025-06-10T10:56:25.448724Z"
    },
    "papermill": {
     "duration": 0.028707,
     "end_time": "2025-06-10T10:56:25.450545",
     "exception": false,
     "start_time": "2025-06-10T10:56:25.421838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BenchmarkTester:\n",
    "    def __init__(self, benchmark_path=\"/var/scratch/ave303/OP_bench/benchmark.json\", data_dir=\"/var/scratch/ave303/OP_bench/\"):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        with open(benchmark_path, 'r') as f:\n",
    "            self.benchmark = json.load(f)\n",
    "        self.data_dir = data_dir\n",
    "        self.api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "    def clean_answer(self, answer):\n",
    "        \"\"\"Clean the model output to extract just the number.\"\"\"\n",
    "        # Remove any text that's not a number\n",
    "        # import re\n",
    "        # numbers = re.findall(r'\\d+', answer)\n",
    "        # if numbers:\n",
    "        #     return numbers[0]  # Return the first number found\n",
    "        # return answer\n",
    "        \"\"\"Extract number and reasoning from the model's answer.\"\"\"\n",
    "        # Try to extract number and reasoning using regex\n",
    "        import re\n",
    "        pattern = r'(\\d+)\\s*\\[(.*?)\\]'\n",
    "        match = re.search(pattern, answer)\n",
    "        \n",
    "        if match:\n",
    "            number = match.group(1)\n",
    "            objects = [obj.strip() for obj in match.group(2).split(',')]\n",
    "            return {\n",
    "                \"count\": number,\n",
    "                \"reasoning\": objects\n",
    "            }\n",
    "        else:\n",
    "            # Fallback if format isn't matched\n",
    "            numbers = re.findall(r'\\d+', answer)\n",
    "            return {\n",
    "                \"count\": numbers[0] if numbers else \"0\",\n",
    "                \"reasoning\": []\n",
    "            }\n",
    "\n",
    "    # def model_generation(self, model_name, model, inputs, processor):\n",
    "    #     \"\"\"Generate answer and decode.\"\"\"\n",
    "    #     outputs = None  # Initialize outputs to None\n",
    "    #     input_len = inputs[\"input_ids\"].shape[-1]\n",
    "        \n",
    "    #     if model_name==\"Gemma3\":\n",
    "    #         outputs = model.generate(**inputs, max_new_tokens=200, do_sample=False)\n",
    "    #         outputs = outputs[0][input_len:]\n",
    "    #         answer = processor.decode(outputs, skip_special_tokens=True)\n",
    "    #         # outputs = model.generate(**inputs, do_sample=False, max_new_tokens=100)\n",
    "    #         # answer = processor.batch_decode(\n",
    "    #         #     outputs,\n",
    "    #         #     skip_special_tokens=True,\n",
    "    #         # )[0]\n",
    "    #     else:\n",
    "    #         print(f\"Warning: Unknown model name '{model_name}' in model_generation.\")\n",
    "    #         answer = \"\"  # Return an empty string\n",
    "\n",
    "    #     return answer, outputs\n",
    "    \n",
    "    def evaluate_model(self, model_name, save_path, start_idx=0, batch_size=5):\n",
    "        results = []\n",
    "        print(f\"\\nEvaluating {model_name}...\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        client = genai.Client(api_key=self.api_key)\n",
    "\n",
    "        # Force garbage collection before starting\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        try:\n",
    "            images = self.benchmark['benchmark']['images'][start_idx:start_idx + batch_size]\n",
    "            total_images = len(images)\n",
    "            \n",
    "            for idx, image_data in enumerate(tqdm(images, desc=\"Processing images\")):\n",
    "                try:\n",
    "                    print(f\"\\nProcessing image {idx+1}/{total_images}: {image_data['image_id']}\")\n",
    "                    image_path = Path(self.data_dir)/image_data['path']\n",
    "                    if not image_path.exists():\n",
    "                        print(f\"Warning: Image not found at {image_path}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Load and preprocess image\n",
    "                    # image = Image.open(image_path).convert(\"RGB\")\n",
    "                    \n",
    "                    image = client.files.upload(file=image_path)\n",
    "                    image_results = []  # Store results for current image\n",
    "                    \n",
    "                    for question in image_data['questions']:\n",
    "                        try:\n",
    "                            # prompt = self.format_question(question, model_name)\n",
    "                            print(f\"Question: {question['question']}\")\n",
    "\n",
    "                            # messages = [\n",
    "                            #     {\n",
    "                            #         \"role\": \"user\",\n",
    "                            #         \"content\": [\n",
    "                            #             {\"type\": \"image\", \"image\": image},\n",
    "                            #             # {\"type\": \"text\", \"text\": f\"{question['question']} Answer format: total number(numerical) objects(within square brackets)\"},\n",
    "                            #             # {\"type\": \"text\", \"text\": f\"{question['question']} Provide just the total count and the list of objects in the given format \\n Format: number [objects]\"},\n",
    "                            #             # {\"type\": \"text\", \"text\": f\"{question['question']} Answer Format: number [objects]\"},\n",
    "                            #             {\"type\": \"text\", \"text\": f\"{question[\"question\"]} Your response MUST be in the following format and nothing else:\\n <NUMBER> [<OBJECT1>, <OBJECT2>, <OBJECT3>, ...]\"}\n",
    "                            #         ]\n",
    "                            #     },\n",
    "                            # ]\n",
    "                            # messages = [\n",
    "                            #     {\n",
    "                            #         \"role\": \"system\",\n",
    "                            #         \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n",
    "                            #     },\n",
    "                            #     {\n",
    "                            #         \"role\": \"user\",\n",
    "                            #         \"content\": [\n",
    "                            #             {\"type\": \"image\", \"image\": image},\n",
    "                            #             {\"type\": \"text\", \"text\": f\"{question['question']} Your response MUST be in the following format and nothing else:\\n <NUMBER> [<OBJECT1>, <OBJECT2>, <OBJECT3>, ...]\"}\n",
    "                            #         ]\n",
    "                            #     }\n",
    "                            # ]\n",
    "\n",
    "                            response = client.models.generate_content(\n",
    "                                model=model_name,\n",
    "                                contents=[image, f\"{question['question']} Your response MUST be in the following format and nothing else:\\n <NUMBER> [<OBJECT1>, <OBJECT2>, <OBJECT3>, ...]\"],\n",
    "                            )\n",
    "                            # print(response.text)\n",
    "                            \n",
    "                            # Clear cache before processing each question\n",
    "                            torch.cuda.empty_cache()\n",
    "                            \n",
    "                            # Process image and text\n",
    "                            # inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(self.device)\n",
    "                            # if model_name==\"smolVLM2\":\n",
    "                            # inputs = processor.apply_chat_template(\n",
    "                            #     messages,\n",
    "                            #     add_generation_prompt=True,\n",
    "                            #     tokenize=True,\n",
    "                            #     return_dict=True,\n",
    "                            #     return_tensors=\"pt\",\n",
    "                            # ).to(model.device, dtype=torch.bfloat16)\n",
    "                           \n",
    "                            # with torch.no_grad():\n",
    "                            #     answer, outputs = self.model_generation(model_name, model, inputs, processor)    #call for model.generate\n",
    "        \n",
    "                            cleaned_answer = self.clean_answer(response.text)\n",
    "                            \n",
    "                            image_results.append({\n",
    "                                \"image_id\": image_data[\"image_id\"],\n",
    "                                \"image_type\": image_data[\"image_type\"],\n",
    "                                \"question_id\": question[\"id\"],\n",
    "                                \"question\": question[\"question\"],\n",
    "                                \"ground_truth\": question[\"answer\"],\n",
    "                                \"model_answer\": cleaned_answer[\"count\"],\n",
    "                                \"model_reasoning\": cleaned_answer[\"reasoning\"],\n",
    "                                \"raw_answer\": response.text,  # Keep raw answer for debugging\n",
    "                                \"property_category\": question[\"property_category\"]\n",
    "                            })\n",
    "                            \n",
    "                            # # Clear memory\n",
    "                            # del outputs, inputs\n",
    "                            # torch.cuda.empty_cache()\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing question: {str(e)}\")\n",
    "                            continue\n",
    "                    \n",
    "                    # Add results from this image\n",
    "                    results.extend(image_results)\n",
    "                    \n",
    "                    # Save intermediate results only every 2 images or if it's the last image\n",
    "                    if (idx + 1) % 2 == 0 or idx == total_images - 1:\n",
    "                        with open(f\"{save_path}_checkpoint.json\", 'w') as f:\n",
    "                            json.dump(results, f, indent=4)\n",
    "\n",
    "                    client.files.delete(name=image.name)\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing image {image_data['image_id']}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            # Save final results\n",
    "            if results:\n",
    "                with open(save_path, 'w') as f:\n",
    "                    json.dump(results, f, indent=4)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during evaluation: {str(e)}\")\n",
    "            if results:\n",
    "                with open(f\"{save_path}_error_state.json\", 'w') as f:\n",
    "                    json.dump(results, f, indent=4)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af57ce28",
   "metadata": {
    "papermill": {
     "duration": 0.004051,
     "end_time": "2025-06-10T10:56:25.458763",
     "exception": false,
     "start_time": "2025-06-10T10:56:25.454712",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test Gemma3\n",
    "\n",
    "Let's evaluate the Gemma-3-27B-it model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6da013cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T10:56:25.468347Z",
     "iopub.status.busy": "2025-06-10T10:56:25.468097Z",
     "iopub.status.idle": "2025-06-10T10:56:25.472301Z",
     "shell.execute_reply": "2025-06-10T10:56:25.471582Z"
    },
    "papermill": {
     "duration": 0.010461,
     "end_time": "2025-06-10T10:56:25.473459",
     "exception": false,
     "start_time": "2025-06-10T10:56:25.462998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def test_Gemma3():\n",
    "#     from transformers import AutoProcessor, Gemma3ForConditionalGeneration\n",
    "\n",
    "#     print(\"Loading Gemma3 model...\")\n",
    "\n",
    "#     model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "#         \"/var/scratch/ave303/models/gemma-3-27b-it\",\n",
    "#         torch_dtype=torch.bfloat16,\n",
    "#         # attn_implementation=\"flash_attention_2\",\n",
    "#         device_map=\"auto\",\n",
    "#         low_cpu_mem_usage=True,\n",
    "#         trust_remote_code=True\n",
    "#     ).eval()\n",
    "    \n",
    "#     processor = AutoProcessor.from_pretrained(\"/var/scratch/ave303/models/gemma-3-27b-it\")\n",
    "\n",
    "#     tester = BenchmarkTester()\n",
    "#     Gemma3_results = tester.evaluate_model(\n",
    "#         \"Gemma3\",\n",
    "#         model,\n",
    "#         processor,\n",
    "#         \"Gemma3_27b_results.json\",\n",
    "#         batch_size=50\n",
    "#     )\n",
    "    \n",
    "#     # Clean up\n",
    "#     del model, processor\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494c1b67",
   "metadata": {
    "papermill": {
     "duration": 0.00412,
     "end_time": "2025-06-10T10:56:25.481852",
     "exception": false,
     "start_time": "2025-06-10T10:56:25.477732",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test Gemini 2.0 Flash\n",
    "\n",
    "Let's evaluate Gemini-2.0-flash model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b2f9bbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T10:56:25.491694Z",
     "iopub.status.busy": "2025-06-10T10:56:25.491227Z",
     "iopub.status.idle": "2025-06-10T10:56:25.495209Z",
     "shell.execute_reply": "2025-06-10T10:56:25.494479Z"
    },
    "papermill": {
     "duration": 0.010213,
     "end_time": "2025-06-10T10:56:25.496381",
     "exception": false,
     "start_time": "2025-06-10T10:56:25.486168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_Gemini2_0Flash():\n",
    "    tester = BenchmarkTester()\n",
    "    Gemini2_0Flash_results = tester.evaluate_model(\n",
    "        \"gemini-2.0-flash\",\n",
    "        \"Gemini2_0Flash_results.json\",\n",
    "        start_idx=6,\n",
    "        batch_size=5\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00f4a8a",
   "metadata": {
    "papermill": {
     "duration": 0.004174,
     "end_time": "2025-06-10T10:56:25.504950",
     "exception": false,
     "start_time": "2025-06-10T10:56:25.500776",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Run Evaluation\n",
    "\n",
    "Now we can run our evaluation. Let's start with the SmolVLM2 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "986c501a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T10:56:25.514224Z",
     "iopub.status.busy": "2025-06-10T10:56:25.513933Z",
     "iopub.status.idle": "2025-06-10T10:56:25.517456Z",
     "shell.execute_reply": "2025-06-10T10:56:25.516744Z"
    },
    "papermill": {
     "duration": 0.009505,
     "end_time": "2025-06-10T10:56:25.518619",
     "exception": false,
     "start_time": "2025-06-10T10:56:25.509114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_Gemma3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42818981",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T10:56:25.528057Z",
     "iopub.status.busy": "2025-06-10T10:56:25.527826Z",
     "iopub.status.idle": "2025-06-10T10:57:00.547668Z",
     "shell.execute_reply": "2025-06-10T10:57:00.546874Z"
    },
    "papermill": {
     "duration": 35.026069,
     "end_time": "2025-06-10T10:57:00.549033",
     "exception": false,
     "start_time": "2025-06-10T10:56:25.522964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating gemini-2.0-flash...\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing images:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image 1/5: image07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many birds are visible in this image?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many objects are present that can comfortably seat a human?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: If the birds sitting together only on one railing were to fly away, how many birds would remain?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing images:  20%|██        | 1/5 [00:07<00:28,  7.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image 2/5: image08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many reptiles are visible in this image?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many objects are present that act as support?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: If one turtle slid off the log into the water, how many turtles would be in the water?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing images:  40%|████      | 2/5 [00:14<00:21,  7.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image 3/5: image09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many different types of vegetables are present in the image?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many objects are used as containers?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: If the bag of limes were removed and replaced with two additional avocados, how many fruits would be present in total on the table, considering avocados are fruits?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing images:  60%|██████    | 3/5 [00:21<00:14,  7.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image 4/5: image10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many objects are present that are flexible?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Count the number of items that are battery powered?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: If two phones with three camera lenses were replaced with phones having two camera lenses, how many phones with two camera lenses would be present?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing images:  80%|████████  | 4/5 [00:27<00:06,  6.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing image 5/5: image11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many objects made of glass are present on the table?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many objects are present at the table that can be used for sitting?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: If the tables in the center are removed, how many objects are visible that have legs?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing images: 100%|██████████| 5/5 [00:34<00:00,  6.96s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processing images: 100%|██████████| 5/5 [00:34<00:00,  6.94s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_Gemini2_0Flash()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7163706,
     "sourceId": 11436696,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python ((op_bench)",
   "language": "python",
   "name": "op_bench"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 45.203018,
   "end_time": "2025-06-10T10:57:02.075634",
   "environment_variables": {},
   "exception": null,
   "input_path": "/var/scratch/ave303/OP_bench/opa-benchmark-gemini2.0Flash.ipynb",
   "output_path": "gemini2.0Flash_output.ipynb",
   "parameters": {},
   "start_time": "2025-06-10T10:56:16.872616",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}